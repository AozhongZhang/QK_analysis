{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2681a139-429e-4c94-b9d9-de6a4a98fe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3964\n",
      "torch.Size([32, 65406, 128])\n",
      "torch.Size([32, 3964, 3964])\n"
     ]
    }
   ],
   "source": [
    "# Get the output of Q/K for 4k and 64K before rope and after rope\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n",
    "from types import MethodType\n",
    "import json\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "with open(\"/home/azzhang/streaming-llm/output/wikitext2_prompts_llama3.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts = json.load(f)\n",
    "target_length_64k = \"64k\"\n",
    "target_length_4k = \"4k\"\n",
    "\n",
    "prompt_64k = prompts[target_length_64k]\n",
    "prompt_4k = prompts[target_length_4k]\n",
    "inputs_64k = tokenizer(prompt_64k, return_tensors=\"pt\").to(model.device)\n",
    "inputs_4k = tokenizer(prompt_4k, return_tensors=\"pt\").to(model.device)\n",
    "seq_len_64 = inputs_64k[\"input_ids\"].shape[1]\n",
    "seq_len_4 = inputs_4k[\"input_ids\"].shape[1]\n",
    "\n",
    "cache_4k = {}\n",
    "cache_64k = {}\n",
    "target_layer = 0\n",
    "\n",
    "def patched_forward(self, hidden_states, position_embeddings=None, *args, **kwargs):\n",
    "    q = self.q_proj(hidden_states)\n",
    "    k = self.k_proj(hidden_states)\n",
    "    v = self.v_proj(hidden_states)\n",
    "\n",
    "    bsz, seqlen, dim = q.shape\n",
    "    head_dim = self.head_dim\n",
    "    \n",
    "    num_heads_q = self.config.num_attention_heads\n",
    "    num_heads_kv = self.config.num_key_value_heads\n",
    "    \n",
    "    if seqlen == seq_len_4:\n",
    "        print(seq_len_4)\n",
    "        q = q.view(bsz, seqlen, num_heads_q, head_dim).transpose(1, 2)\n",
    "        k = k.view(bsz, seqlen, num_heads_kv, head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Q、K before rope\n",
    "        cache_4k[\"q_raw\"] = q.detach().cpu()\n",
    "        cache_4k[\"k_raw\"] = k.detach().cpu()\n",
    "        \n",
    "        cos, sin = position_embeddings\n",
    "\n",
    "        q_rope, k_rope = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        cache_4k[\"q_rope\"] = q_rope.detach().cpu()\n",
    "        cache_4k[\"k_rope\"] = k_rope.detach().cpu()\n",
    "    else:\n",
    "        # print(seq_len_64)\n",
    "        q = q.view(bsz, seqlen, num_heads_q, head_dim).transpose(1, 2)\n",
    "        k = k.view(bsz, seqlen, num_heads_kv, head_dim).transpose(1, 2)\n",
    "\n",
    "        # Q、K before rope\n",
    "        cache_64k[\"q_raw\"] = q.detach().cpu()\n",
    "        cache_64k[\"k_raw\"] = k.detach().cpu()\n",
    "        \n",
    "        cos, sin = position_embeddings\n",
    "\n",
    "        q_rope, k_rope = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        cache_64k[\"q_rope\"] = q_rope.detach().cpu()\n",
    "        cache_64k[\"k_rope\"] = k_rope.detach().cpu()\n",
    "\n",
    "    return self._orig_forward(hidden_states, position_embeddings, *args, **kwargs)\n",
    "\n",
    "# insert patch\n",
    "attn_layer = model.model.layers[target_layer].self_attn\n",
    "attn_layer._orig_forward = attn_layer.forward\n",
    "attn_layer.forward = MethodType(patched_forward, attn_layer)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs_4k)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs_64k)\n",
    "\n",
    "Q_4k = cache_4k[\"q_raw\"].squeeze(0)  # shape: (num_heads, seq_len, head_dim) for one layer (32, 4k, 1024//32)\n",
    "K_4k = cache_4k[\"k_raw\"].squeeze(0) # (8, 4k, 1024//8)\n",
    "Q_4k_rope = cache_4k[\"q_rope\"].squeeze(0)  # shape: (num_heads, seq_len, head_dim)\n",
    "K_4k_rope = cache_4k[\"k_rope\"].squeeze(0)\n",
    "\n",
    "Q_64k = cache_64k[\"q_raw\"].squeeze(0)  # shape: (num_heads, seq_len, head_dim)\n",
    "K_64k = cache_64k[\"k_raw\"].squeeze(0)\n",
    "Q_64k_rope = cache_64k[\"q_rope\"].squeeze(0)  # shape: (num_heads, seq_len, head_dim)\n",
    "K_64k_rope = cache_64k[\"k_rope\"].squeeze(0)\n",
    "\n",
    "print(Q_64k_rope.shape)\n",
    "K_4k_rope = K_4k_rope[:, None, :, :].expand(8, 4, seq_len_4, 128)\n",
    "K_4k_rope = K_4k_rope.reshape(32, seq_len_4, 128)\n",
    "QK_production = torch.matmul(Q_4k_rope, K_4k_rope.transpose(1, 2))\n",
    "print(QK_production.shape)\n",
    "# target_head = 0\n",
    "\n",
    "# K_head_4k_before, K_head_64k_before = K_4k[target_head].float(), K_64k[target_head].float()\n",
    "# K_head_4k_rope, K_head_64k_rope = K_4k_rope[target_head].float(), K_64k_rope[target_head].float()\n",
    "# Q_4k_before_list, Q_64k_before_list, Q_4k_rope_list, Q_64k_rope_list = [], [], [], []\n",
    "\n",
    "# for i in range(4):\n",
    "#     Q_4k_before_list.append(Q_4k[4*target_head+i].float())\n",
    "#     Q_64k_before_list.append(Q_64k[4*target_head+i].float())\n",
    "#     Q_4k_rope_list.append(Q_4k_rope[4*target_head+i].float())\n",
    "#     Q_64k_rope_list.append(Q_64k_rope[4*target_head+i].float())\n",
    "\n",
    "# print(K_head_4k_rope.shape)\n",
    "# print(len(Q_4k_before_list))\n",
    "# print(Q_4k_before_list[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fms-mo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

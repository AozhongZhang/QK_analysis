{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681a139-429e-4c94-b9d9-de6a4a98fe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3964\n",
      "torch.Size([3964, 128])\n",
      "4\n",
      "torch.Size([3964, 128])\n",
      "tensor([[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.5391,  0.6875,  0.7891,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.4160, -0.0583,  0.2412,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        ...,\n",
      "        [-0.8516, -0.9609, -0.5508,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.8984, -0.4551, -0.9492,  ...,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.1187,  0.3359, -0.9414,  ...,  1.0000,  1.0000,  1.0000]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# Get the output of Q/K for 4k and 64K before rope and after rope\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n",
    "from types import MethodType\n",
    "import json\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "with open(\"/home/azzhang/streaming-llm/output/wikitext2_prompts_llama3.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts = json.load(f)\n",
    "target_length_64k = \"64k\"\n",
    "target_length_4k = \"4k\"\n",
    "\n",
    "prompt_64k = prompts[target_length_64k]\n",
    "prompt_4k = prompts[target_length_4k]\n",
    "inputs_64k = tokenizer(prompt_64k, return_tensors=\"pt\").to(model.device)\n",
    "inputs_4k = tokenizer(prompt_4k, return_tensors=\"pt\").to(model.device)\n",
    "seq_len_64 = inputs_64k[\"input_ids\"].shape[1]\n",
    "seq_len_4 = inputs_4k[\"input_ids\"].shape[1]\n",
    "\n",
    "cache_4k = {}\n",
    "cache_64k = {}\n",
    "target_layer = 0\n",
    "\n",
    "def patched_forward(self, hidden_states, position_embeddings=None, *args, **kwargs):\n",
    "    q = self.q_proj(hidden_states)\n",
    "    k = self.k_proj(hidden_states)\n",
    "    v = self.v_proj(hidden_states)\n",
    "\n",
    "    bsz, seqlen, dim = q.shape\n",
    "    head_dim = self.head_dim\n",
    "    \n",
    "    num_heads_q = self.config.num_attention_heads\n",
    "    num_heads_kv = self.config.num_key_value_heads\n",
    "    \n",
    "    if seqlen == seq_len_4:\n",
    "        print(seq_len_4)\n",
    "        q = q.view(bsz, seqlen, num_heads_q, head_dim).transpose(1, 2)\n",
    "        k = k.view(bsz, seqlen, num_heads_kv, head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Q、K before rope\n",
    "        cache_4k[\"q_raw\"] = q.detach().cpu()\n",
    "        cache_4k[\"k_raw\"] = k.detach().cpu()\n",
    "        \n",
    "        cos, sin = position_embeddings\n",
    "\n",
    "        q_rope, k_rope = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        cache_4k[\"q_rope\"] = q_rope.detach().cpu()\n",
    "        cache_4k[\"k_rope\"] = k_rope.detach().cpu()\n",
    "    else:\n",
    "        # print(seq_len_64)\n",
    "        q = q.view(bsz, seqlen, num_heads_q, head_dim).transpose(1, 2)\n",
    "        k = k.view(bsz, seqlen, num_heads_kv, head_dim).transpose(1, 2)\n",
    "\n",
    "        # Q、K before rope\n",
    "        cache_64k[\"q_raw\"] = q.detach().cpu()\n",
    "        cache_64k[\"k_raw\"] = k.detach().cpu()\n",
    "        \n",
    "        cos, sin = position_embeddings\n",
    "\n",
    "        q_rope, k_rope = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        cache_64k[\"q_rope\"] = q_rope.detach().cpu()\n",
    "        cache_64k[\"k_rope\"] = k_rope.detach().cpu()\n",
    "\n",
    "    return self._orig_forward(hidden_states, position_embeddings, *args, **kwargs)\n",
    "\n",
    "# insert patch\n",
    "attn_layer = model.model.layers[target_layer].self_attn\n",
    "attn_layer._orig_forward = attn_layer.forward\n",
    "attn_layer.forward = MethodType(patched_forward, attn_layer)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs_4k)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs_64k)\n",
    "\n",
    "Q_4k = cache_4k[\"q_raw\"].squeeze(0)  # shape: (num_heads, seq_len, head_dim) for one layer (32, 4k, 1024//32)\n",
    "K_4k = cache_4k[\"k_raw\"].squeeze(0) # (8, 4k, 1024//8)\n",
    "Q_4k_rope = cache_4k[\"q_rope\"].squeeze(0)  # shape: (num_heads, seq_len, head_dim)\n",
    "K_4k_rope = cache_4k[\"k_rope\"].squeeze(0)\n",
    "\n",
    "Q_64k = cache_64k[\"q_raw\"].squeeze(0)  # shape: (num_heads, seq_len, head_dim)\n",
    "K_64k = cache_64k[\"k_raw\"].squeeze(0)\n",
    "Q_64k_rope = cache_64k[\"q_rope\"].squeeze(0)  # shape: (num_heads, seq_len, head_dim)\n",
    "K_64k_rope = cache_64k[\"k_rope\"].squeeze(0)\n",
    "\n",
    "target_head = 0\n",
    "\n",
    "K_head_4k_before, K_head_64k_before = K_4k[target_head].float(), K_64k[target_head].float()\n",
    "K_head_4k_rope, K_head_64k_rope = K_4k_rope[target_head].float(), K_64k_rope[target_head].float()\n",
    "Q_4k_before_list, Q_64k_before_list, Q_4k_rope_list, Q_64k_rope_list = [], [], [], []\n",
    "\n",
    "for i in range(4):\n",
    "    Q_4k_before_list.append(Q_4k[4*target_head+i].float())\n",
    "    Q_64k_before_list.append(Q_64k[4*target_head+i].float())\n",
    "    Q_4k_rope_list.append(Q_4k_rope[4*target_head+i].float())\n",
    "    Q_64k_rope_list.append(Q_64k_rope[4*target_head+i].float())\n",
    "\n",
    "print(K_head_4k_rope.shape)\n",
    "print(len(Q_4k_before_list))\n",
    "print(Q_4k_before_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d034a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3964, 256])\n"
     ]
    }
   ],
   "source": [
    "def build_rope_matrix(seq_len: int, head_dim: int, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    build cos/sin eig matrix of RoPE: shape [2*seq_len, head_dim//2]\n",
    "    \"\"\"\n",
    "    assert head_dim % 2 == 0, \"head_dim must be even\"\n",
    "    dim = head_dim // 2\n",
    "    position = torch.arange(seq_len).unsqueeze(1).to(dtype)  # [seq_len, 1]\n",
    "    dim_indices = torch.arange(dim).unsqueeze(0).to(dtype)   # [1, dim]\n",
    "    freq = 100000 ** (-2 * dim_indices / head_dim)            # [1, dim]\n",
    "    angles = position * freq                                 # [seq_len, dim]\n",
    "    cos_part = torch.cos(angles)\n",
    "    sin_part = torch.sin(angles)\n",
    "    rope_matrix = torch.cat([cos_part, sin_part], dim=1)     # [seq_len, 2*dim]\n",
    "    return rope_matrix  # shape: [L, 2D]\n",
    "\n",
    "\n",
    "def get_rope_null_space(cloud: torch.Tensor, tol: float = 1e-4):\n",
    "    \"\"\"\n",
    "    Extract the base of nulll space.\n",
    "    \"\"\"\n",
    "    \n",
    "    seq_len, full_dim = cloud.shape\n",
    "    cov = cloud.T @ cloud / seq_len  # [2*dim, 2*dim]\n",
    "    eigvals, eigvecs = torch.linalg.eigh(cov)  # eigvecs: [2*dim, 2*dim]\n",
    "    eigvals_sqrt = eigvals.sqrt()\n",
    "    null_mask = eigvals_sqrt < tol\n",
    "    null_rank = null_mask.sum().item()\n",
    "    null_basis = eigvecs[:, null_mask].T  # shape: [null_rank, 2*dim]\n",
    "\n",
    "    return {\n",
    "        \"eigvals\": eigvals_sqrt,\n",
    "        \"null_rank\": null_rank,\n",
    "        \"total_rank\": full_dim,\n",
    "        \"ratio\": null_rank / full_dim,\n",
    "        \"null_basis\": null_basis  \n",
    "    }\n",
    "\n",
    "\n",
    "rope_4k = build_rope_matrix(seq_len=seq_len_4, head_dim=128)\n",
    "rope_64k = build_rope_matrix(seq_len=seq_len_64, head_dim=128)\n",
    "null_basis_4k = get_rope_null_space(rope_4k)[\"null_basis\"]  # shape: [r, d]\n",
    "null_basis_64k = get_rope_null_space(rope_64k)[\"null_basis\"]  # shape: [r, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cfdb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3964, 128])\n"
     ]
    }
   ],
   "source": [
    "# Project to null space\n",
    "def apply_null_projection(q_list, k_tensor, null_basis):\n",
    "    B = null_basis.T\n",
    "    P = B @ B.T\n",
    "    q_proj_list = [q @ P for q in q_list]\n",
    "    k_proj = k_tensor @ P\n",
    "    return q_proj_list, k_proj\n",
    "\n",
    "Q_proj_4k_before_list, K_proj_4k_before = apply_null_projection(Q_4k_before_list, K_head_4k_before, null_basis_4k)\n",
    "Q_proj_4k_rope_list, K_proj_4k_rope = apply_null_projection(Q_4k_rope_list, K_head_4k_rope, null_basis_4k)\n",
    "Q_proj_64k_list, K_proj_64k = apply_null_projection(Q_64k_before_list, K_head_64k_before, null_basis_64k)\n",
    "Q_proj_64k_rope_list, K_proj_64k_rope = apply_null_projection(Q_64k_rope_list, K_head_64k_rope, null_basis_4k)\n",
    "\n",
    "print(K_proj_4k_before.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b27238f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K null space ratio: tensor(0.0172)\n",
      "tensor([0.0000e+00, 5.9341e-16, 7.2622e-16, 2.7578e-16, 1.5009e-15, 7.2654e-17,\n",
      "        3.0101e-15, 2.0377e-15, 6.1580e-17, 1.0541e-15, 7.1247e-15, 2.1657e-14,\n",
      "        1.3627e-16, 1.8053e-15, 6.2129e-15, 1.8232e-14, 3.4423e-14, 1.5561e-14,\n",
      "        1.2528e-13, 1.6727e-13, 6.2550e-13, 8.8453e-14, 1.2905e-12, 4.1495e-13,\n",
      "        1.2280e-11, 1.6710e-10, 1.8557e-09, 4.4164e-08, 1.6287e-06, 2.9889e-05,\n",
      "        6.5884e-05, 6.8869e-04, 4.6110e-04, 2.8782e-04, 9.4966e-04, 5.1015e-04,\n",
      "        3.6210e-04, 1.5103e-03, 4.1202e-05, 1.1422e-03, 4.0343e-04, 4.5132e-05,\n",
      "        6.7421e-04, 1.1828e-03, 1.5680e-03, 5.3019e-04, 3.5367e-04, 7.7921e-04,\n",
      "        1.4466e-04, 6.1196e-05, 2.4771e-04, 8.6770e-04, 3.0380e-04, 1.0692e-03,\n",
      "        1.1446e-04, 2.7890e-04, 4.0228e-04, 2.6823e-04, 4.0093e-04, 1.0691e-03,\n",
      "        4.7966e-04, 3.2873e-04, 2.2035e-04, 3.7001e-04, 2.6244e-16, 9.1444e-17,\n",
      "        4.5153e-16, 1.1485e-15, 9.9567e-16, 4.5744e-15, 7.0398e-15, 9.8230e-16,\n",
      "        1.5145e-14, 2.0119e-15, 3.1199e-14, 1.9025e-14, 6.4451e-15, 5.5256e-15,\n",
      "        5.2634e-15, 1.7224e-13, 2.0044e-13, 5.3565e-14, 5.9352e-15, 5.2539e-14,\n",
      "        2.1612e-13, 1.5563e-13, 2.3332e-13, 6.2015e-13, 2.0323e-11, 8.6122e-11,\n",
      "        3.0247e-09, 8.4059e-08, 1.2169e-06, 1.0833e-05, 2.3871e-04, 2.4614e-04,\n",
      "        2.9146e-04, 3.7572e-04, 5.2676e-04, 2.9751e-04, 7.4291e-04, 1.7694e-04,\n",
      "        2.2250e-04, 9.1898e-04, 5.7197e-04, 3.4506e-05, 1.2483e-03, 2.5802e-04,\n",
      "        9.9963e-04, 1.5871e-03, 3.0476e-03, 1.0673e-03, 3.8476e-03, 3.5760e-04,\n",
      "        1.0406e-03, 2.3191e-02, 1.5058e-02, 9.3368e-04, 3.1660e-03, 9.5229e-02,\n",
      "        7.7560e-02, 7.1022e-02, 6.0050e-02, 4.9880e-02, 2.8208e-02, 2.0834e-01,\n",
      "        4.4733e-02, 5.9188e-02])\n"
     ]
    }
   ],
   "source": [
    "def energy_ratio(x, x_proj):\n",
    "    return (x_proj.norm(dim=-1) ** 2).sum() / (x.norm(dim=-1) ** 2).sum()\n",
    "\n",
    "print(\"K null space ratio:\", energy_ratio(K_head_4k_rope, K_proj_4k_rope))\n",
    "\n",
    "# for i, q_proj in enumerate(q_proj_null_list):\n",
    "#     print(f\"Q{i} null space ratio:\", energy_ratio(q_rope_list[i], q_proj))\n",
    "\n",
    "def null_energy_per_token(x, null_basis):\n",
    "    \"\"\"\n",
    "    x: [seq_len, 2*dim] - RoPEed feature\n",
    "    null_basis: [r, 2*dim]\n",
    "    return: [seq_len], the energy of each projected token\n",
    "    \"\"\"\n",
    "    P = null_basis.T @ null_basis  # [2*dim, 2*dim]\n",
    "    x_proj = x @ P\n",
    "    energy = x_proj.norm(dim=-1) ** 2\n",
    "    return energy\n",
    "\n",
    "def count_null_tokens(x, null_basis, threshold=0.3):\n",
    "    \"\"\"\n",
    "    返回落入 null space 严重的 token 比例\n",
    "    \"\"\"\n",
    "    P = null_basis.T @ null_basis\n",
    "    x_proj = x @ P\n",
    "    total_energy = (x.norm(dim=-1) ** 2)\n",
    "    null_energy = (x_proj.norm(dim=-1) ** 2)\n",
    "    ratio = null_energy / (total_energy + 1e-8)\n",
    "\n",
    "    # 返回比例（多少 token 比例 > threshold）\n",
    "    return (ratio > threshold).float().mean().item()\n",
    "\n",
    "def null_basis_activation(x, null_basis):\n",
    "    x_proj = x @ (null_basis.T @ null_basis)  # [seq_len, 2*dim]\n",
    "    return x_proj.var(dim=0)  # [2*dim]\n",
    "\n",
    "def null_direction_strength(x, null_basis):\n",
    "    \"\"\"\n",
    "    投影到 null_basis 后的 norm 方差，表示各方向被用的程度\n",
    "    \"\"\"\n",
    "    coeff = x @ null_basis.T  # [seq_len, r]\n",
    "    return coeff.var(dim=0) \n",
    "\n",
    "\n",
    "print(null_basis_activation(K_head_4k_rope, null_basis_4k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f83e992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0871209, 1.1279013, 1.1306926, 1.1371487]\n",
      "[1.0340893, 1.0736986, 1.0501916, 1.0837443]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "def analyze_cluster_separation(x_proj, num_clusters=4):\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    labels = kmeans.fit_predict(x_proj)\n",
    "    score = silhouette_score(x_proj, labels)\n",
    "    return score\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def qk_separation(q_list, k_tensor, method=\"cosine\"):\n",
    "    \"\"\"\n",
    "    compute avg distance between each Q head and K head\n",
    "    return: List: the distance of each Q head and K head\n",
    "    \"\"\"\n",
    "    sep_scores = []\n",
    "    for q in q_list:\n",
    "        dist = pairwise_distances(q.numpy(), k_tensor.numpy(), metric=method)\n",
    "        avg_dist = dist.mean()\n",
    "        sep_scores.append(avg_dist)\n",
    "    return sep_scores  \n",
    "\n",
    "def qq_cluster_separation(q_list, n_clusters=4):\n",
    "    \"\"\"\n",
    "    Concat Q list to KMeans, label: head_id\n",
    "    compute silhouette score to present the distance of each Q head\n",
    "    \"\"\"\n",
    "    X = torch.cat(q_list, dim=0).numpy()  # [4*seq_len, head_dim]\n",
    "    y = np.concatenate([[i]*len(q) for i, q in enumerate(q_list)])  # 0,0,...1,1,...\n",
    "    return silhouette_score(X, y)\n",
    "\n",
    "# result_4k_rope = qk_separation(Q_4k_rope_list, K_head_4k_rope)\n",
    "# print(result_4k_rope)\n",
    "# result_64k_rope = qk_separation(Q_64k_rope_list, K_head_64k_rope)\n",
    "# # result_4k_proj_before = qk_separation(Q_proj_4k_before_list, K_proj_4k_before)\n",
    "# # result_64k_proj_rope = qk_separation(Q_proj_64k_rope_list, K_proj_64k_rope)\n",
    "# print(result_64k_rope)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fms-mo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

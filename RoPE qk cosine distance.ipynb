{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2681a139-429e-4c94-b9d9-de6a4a98fe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azzhang/miniforge3/envs/fms-mo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/azzhang/miniforge3/envs/fms-mo/lib/python3.10/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get the output of Q/K for 4k and 64K before rope and after rope\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n",
    "from types import MethodType\n",
    "import json\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "with open(\"/home/azzhang/streaming-llm/output/wikitext2_prompts_llama3.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "target_length_4k = \"64k\"\n",
    "\n",
    "prompt_4k = prompts[target_length_4k]\n",
    "inputs_4k = tokenizer(prompt_4k, return_tensors=\"pt\").to(model.device)\n",
    "seq_len_4 = inputs_4k[\"input_ids\"].shape[1]\n",
    "\n",
    "cache_4k = {}\n",
    "target_layers = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]\n",
    "\n",
    "def make_patched_forward(layer_idx):\n",
    "    def patched_forward(self, hidden_states, position_embeddings=None, *args, **kwargs):\n",
    "        q = self.q_proj(hidden_states)\n",
    "        k = self.k_proj(hidden_states)\n",
    "        v = self.v_proj(hidden_states)\n",
    "\n",
    "        bsz, seqlen, dim = q.shape\n",
    "        head_dim = self.head_dim\n",
    "        num_heads_q = self.config.num_attention_heads\n",
    "        num_heads_kv = self.config.num_key_value_heads\n",
    "\n",
    "        q = q.view(bsz, seqlen, num_heads_q, head_dim).transpose(1, 2)\n",
    "        k = k.view(bsz, seqlen, num_heads_kv, head_dim).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        q_rope, k_rope = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        \n",
    "        cache_4k[layer_idx] = {\n",
    "                \"q_raw\": q.detach().cpu(),\n",
    "                \"k_raw\": k.detach().cpu(),\n",
    "                \"q_rope\": q_rope.detach().cpu(),\n",
    "                \"k_rope\": k_rope.detach().cpu(),\n",
    "            }\n",
    "\n",
    "        return self._orig_forward(hidden_states, position_embeddings, *args, **kwargs)\n",
    "    \n",
    "    return patched_forward\n",
    "\n",
    "for layer_idx in target_layers:\n",
    "    attn_layer = model.model.layers[layer_idx].self_attn\n",
    "    attn_layer._orig_forward = attn_layer.forward  \n",
    "    attn_layer.forward = MethodType(make_patched_forward(layer_idx), attn_layer)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs_4k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c56d034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cosine_similarity_mean(Q, K):\n",
    "    # Normalize along last dimension\n",
    "    Q_norm = F.normalize(Q, dim=-1)\n",
    "    K_norm = F.normalize(K, dim=-1)\n",
    "\n",
    "    # Cosine similarity matrices\n",
    "    return (Q_norm @ K_norm.T).mean()\n",
    "\n",
    "def cosine_distance_difference(Q, K):\n",
    "    # Normalize along last dimension\n",
    "    Q_norm = F.normalize(Q, dim=-1)\n",
    "    K_norm = F.normalize(K, dim=-1)\n",
    "\n",
    "    # Cosine similarity matrices\n",
    "    S_qk = Q_norm @ K_norm.T\n",
    "    S_qq = Q_norm @ Q_norm.T\n",
    "    S_kk = K_norm @ K_norm.T\n",
    "\n",
    "    # Difference = average of self-similarity - cross similarity\n",
    "    diff = (S_qq + S_kk) / 2 - S_qk\n",
    "    return diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078905f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference \n",
    "cos_sim_before_rope_all_layer = []\n",
    "cos_sim_after_rope_all_layer = []\n",
    "for layer_idx in target_layers:\n",
    "    Q_before_rope = cache_4k[layer_idx][\"q_raw\"].squeeze(0) # [num_head, sen_len, head_dim]\n",
    "    K_before_rope = cache_4k[layer_idx][\"k_raw\"].squeeze(0) # [num_head, sen_len, head_dim]\n",
    "    Q_after_rope = cache_4k[layer_idx][\"q_rope\"].squeeze(0)\n",
    "    K_after_rope = cache_4k[layer_idx][\"k_rope\"].squeeze(0)\n",
    "\n",
    "    layer_difference_before_rope = []\n",
    "    layer_difference_after_rope = []\n",
    "\n",
    "    for target_head in range(8):\n",
    "        \n",
    "        for i in range(4):\n",
    "            dif_before_rope = cosine_distance_difference(Q_before_rope[4*target_head+i].float(), K_before_rope[target_head].float())\n",
    "            layer_difference_before_rope.append(dif_before_rope)\n",
    "            # print(dif_before_rope)\n",
    "            dif_after_rope = cosine_distance_difference(Q_after_rope[4*target_head+i].float(), K_after_rope[target_head].float())\n",
    "            layer_difference_after_rope.append(dif_after_rope)\n",
    "\n",
    "    cos_sim_before_rope_all_layer.append(sum(layer_difference_before_rope)/len(layer_difference_before_rope))\n",
    "    cos_sim_after_rope_all_layer.append(sum(layer_difference_after_rope)/len(layer_difference_after_rope))\n",
    "print(len(cos_sim_before_rope_all_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86fc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9461)\n",
      "tensor(0.3977)\n"
     ]
    }
   ],
   "source": [
    "print(sum(cos_sim_before_rope_all_layer)/len(cos_sim_before_rope_all_layer))\n",
    "print(sum(cos_sim_after_rope_all_layer)/len(cos_sim_after_rope_all_layer))\n",
    "# before rope 1k: 0.8588 2k: 0.8600 4k: 0.8598 8k: 0.8581 16k: 0.9033 32k: 0.9461\n",
    "# after rope 1k: 0.7787 2k: 0.7519 4k: 0.7062 8k: 0.6155 16k: 0.4823 32k: 0.3977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908bfbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(target_layers, cos_sim_before_rope_all_layer, label=\"before rope\")\n",
    "# plt.plot(target_layers, cos_sim_after_rope_all_layer, label=\"after rope\")\n",
    "# plt.axhline(y=0, color='gray', linestyle='--', linewidth=1)\n",
    "# plt.xticks(target_layers)\n",
    "# plt.xlabel(\"Layer Index\")\n",
    "# plt.ylabel(\"Mean of Difference of Cosine Similarity\")\n",
    "# plt.title(\"Difference of Cosine Similarity Across Layers with 2k tokens\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92849cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q/K similarity\n",
    "# cos_sim_before_rope_all_layer = []\n",
    "# cos_sim_after_rope_all_layer = []\n",
    "# for layer_idx in target_layers:\n",
    "#     Q_before_rope = cache_4k[layer_idx][\"q_raw\"].squeeze(0) # [num_head, sen_len, head_dim]\n",
    "#     K_before_rope = cache_4k[layer_idx][\"k_raw\"].squeeze(0) # [num_head, sen_len, head_dim]\n",
    "#     Q_after_rope = cache_4k[layer_idx][\"q_rope\"].squeeze(0)\n",
    "#     K_after_rope = cache_4k[layer_idx][\"k_rope\"].squeeze(0)\n",
    "\n",
    "#     cos_sim_before_rope = []\n",
    "#     cos_sim_after_rope = []\n",
    "\n",
    "#     for target_head in range(8):\n",
    "        \n",
    "#         for i in range(4):\n",
    "#             sim_before_rope = cosine_similarity_mean(Q_before_rope[4*target_head+i].float(), K_before_rope[target_head].float())\n",
    "#             cos_sim_before_rope.append(sim_before_rope)\n",
    "#             sim_after_rope = cosine_similarity_mean(Q_after_rope[4*target_head+i].float(), K_after_rope[target_head].float())\n",
    "#             cos_sim_after_rope.append(sim_after_rope)\n",
    "\n",
    "#     cos_sim_before_rope_all_layer.append(sum(cos_sim_before_rope)/len(cos_sim_before_rope))\n",
    "#     cos_sim_after_rope_all_layer.append(sum(cos_sim_after_rope)/len(cos_sim_after_rope))\n",
    "\n",
    "# Q/Q similarity\n",
    "\n",
    "# cos_sim_before_rope_all_layer = []\n",
    "# cos_sim_after_rope_all_layer = []\n",
    "# for layer_idx in target_layers:\n",
    "\n",
    "#     Q_before_rope = cache_4k[layer_idx][\"q_raw\"].squeeze(0) # [num_head, sen_len, head_dim]\n",
    "#     Q_after_rope = cache_4k[layer_idx][\"q_rope\"].squeeze(0)\n",
    "\n",
    "#     cos_sim_before_rope = []\n",
    "#     cos_sim_after_rope = []\n",
    "\n",
    "#     for target_head in range(32):\n",
    "#         sim_before_rope = cosine_similarity_mean(Q_before_rope[target_head].float(), Q_before_rope[target_head].float())\n",
    "#         cos_sim_before_rope.append(sim_before_rope)\n",
    "#         sim_after_rope = cosine_similarity_mean(Q_after_rope[target_head].float(), Q_after_rope[target_head].float())\n",
    "#         cos_sim_after_rope.append(sim_after_rope)\n",
    "#     cos_sim_before_rope_all_layer.append(sum(cos_sim_before_rope)/len(cos_sim_before_rope))\n",
    "#     cos_sim_after_rope_all_layer.append(sum(cos_sim_after_rope)/len(cos_sim_after_rope))\n",
    "\n",
    "# K/K similarity\n",
    "\n",
    "# cos_sim_before_rope_all_layer = []\n",
    "# cos_sim_after_rope_all_layer = []\n",
    "# for layer_idx in target_layers:\n",
    "\n",
    "#     K_before_rope = cache_4k[layer_idx][\"k_raw\"].squeeze(0) # [num_head, sen_len, head_dim]\n",
    "#     K_after_rope = cache_4k[layer_idx][\"k_rope\"].squeeze(0)\n",
    "\n",
    "#     cos_sim_before_rope = []\n",
    "#     cos_sim_after_rope = []\n",
    "\n",
    "#     for target_head in range(8):\n",
    "#         sim_before_rope = cosine_similarity_mean(K_before_rope[target_head].float(), K_before_rope[target_head].float())\n",
    "#         cos_sim_before_rope.append(sim_before_rope)\n",
    "#         sim_after_rope = cosine_similarity_mean(K_after_rope[target_head].float(), K_after_rope[target_head].float())\n",
    "#         cos_sim_after_rope.append(sim_after_rope)\n",
    "#     cos_sim_before_rope_all_layer.append(sum(cos_sim_before_rope)/len(cos_sim_before_rope))\n",
    "#     cos_sim_after_rope_all_layer.append(sum(cos_sim_after_rope)/len(cos_sim_after_rope))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4807d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(target_layers, cos_sim_before_rope_all_layer, label=\"before rope\")\n",
    "# plt.plot(target_layers, cos_sim_after_rope_all_layer, label=\"after rope\")\n",
    "# plt.axhline(y=0, color='gray', linestyle='--', linewidth=1)\n",
    "# plt.xticks(target_layers)\n",
    "# plt.xlabel(\"Layer Index\")\n",
    "# plt.ylabel(\"Mean Cosine Similarity\")\n",
    "# plt.title(\"Q/K Cosine Similarity Across Layers with 2k tokens\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5268585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the ratio keep it\n",
    "import torch\n",
    "\n",
    "def compute_cluster_ratio(Q: torch.Tensor, K: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Q: [seq_len, head_dim]\n",
    "        K: [seq_len, head_dim]\n",
    "    Returns:\n",
    "        ratio: scalar float\n",
    "    \"\"\"\n",
    "    assert Q.shape == K.shape and Q.dim() == 2, \"Expect [seq_len, head_dim] Q/K\"\n",
    "\n",
    "    # Compute cluster centers\n",
    "    mu_Q = Q.mean(dim=0)  # [head_dim]\n",
    "    mu_K = K.mean(dim=0)\n",
    "    # Compute average intra-cluster distance\n",
    "    intra_q = (Q - mu_Q).norm(dim=1).mean()\n",
    "    intra_k = (K - mu_K).norm(dim=1).mean()\n",
    "    intra_avg = 0.5 * (intra_q + intra_k)\n",
    "    # Compute inter-cluster distance\n",
    "    inter = (mu_Q - mu_K).norm()\n",
    "    # Avoid division by zero\n",
    "    if inter.item() < 1e-6:\n",
    "        return float('inf')  # or some large number / log\n",
    "    ratio = (intra_avg / inter).item()\n",
    "    return ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a37857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio\n",
    "ratio_before_rope_all_layer = []\n",
    "ratio_after_rope_all_layer = []\n",
    "for layer_idx in target_layers:\n",
    "    Q_before_rope = cache_4k[layer_idx][\"q_raw\"].squeeze(0) # [num_head, sen_len, head_dim]\n",
    "    K_before_rope = cache_4k[layer_idx][\"k_raw\"].squeeze(0) # [num_head, sen_len, head_dim]\n",
    "    Q_after_rope = cache_4k[layer_idx][\"q_rope\"].squeeze(0)\n",
    "    K_after_rope = cache_4k[layer_idx][\"k_rope\"].squeeze(0)\n",
    "\n",
    "    layer_ratio_before_rope = []\n",
    "    layer_ratio_after_rope = []\n",
    "\n",
    "    for target_head in range(8):\n",
    "        \n",
    "        for i in range(4):\n",
    "            dif_before_rope = compute_cluster_ratio(Q_before_rope[4*target_head+i].float(), K_before_rope[target_head].float())\n",
    "            layer_ratio_before_rope.append(dif_before_rope)\n",
    "            # print(dif_before_rope)\n",
    "            dif_after_rope = compute_cluster_ratio(Q_after_rope[4*target_head+i].float(), K_after_rope[target_head].float())\n",
    "            layer_ratio_after_rope.append(dif_after_rope)\n",
    "\n",
    "    ratio_before_rope_all_layer.append(sum(layer_ratio_before_rope)/len(layer_ratio_before_rope))\n",
    "    ratio_after_rope_all_layer.append(sum(layer_ratio_after_rope)/len(layer_ratio_after_rope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0314c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(target_layers, ratio_before_rope_all_layer, label=\"before rope\")\n",
    "plt.plot(target_layers, ratio_after_rope_all_layer, label=\"after rope\")\n",
    "plt.axhline(y=0, color='gray', linestyle='--', linewidth=1)\n",
    "plt.xticks(target_layers)\n",
    "plt.xlabel(\"Layer Index\")\n",
    "plt.ylabel(\"Mean of Ratio\")\n",
    "plt.title(\"Ratio Across Layers with 4k tokens\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fms-mo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "1k": "import numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\"\"\"\nExplanation: Simple MNIST convnet\nAuthor: fchollet<br>\nDate created: 2015/06/19<br>\nLast modified: 2020/04/21<br>\nDescription: A simple convnet that achieves ~99% test accuracy on MNIST.\nSetup\nEnd of explanation\n\"\"\"\n\n\n# Model / data parameters\nnum_classes = 10\ninput_shape = (28, 28, 1)\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") / 255\nx_test = x_test.astype(\"float32\") / 255\n# Make sure images have shape (28, 28, 1)\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\nprint(\"x_train shape:\", x_train.shape)\nprint(x_train.shape[0], \"train samples\")\nprint(x_test.shape[0], \"test samples\")\n\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\"\"\"\nExplanation: Prepare the data\nEnd of explanation\n\"\"\"\n\n\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Flatten(),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation=\"softmax\"),\n    ]\n)\n\nmodel.summary()\n\n\"\"\"\nExplanation: Build the model\nEnd of explanation\n\"\"\"\n\n\nbatch_size = 128\nepochs = 15\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n\n\"\"\"\nExplanation: Train the model\nEnd of explanation\n\"\"\"\n\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])\n\n\"\"\"\nExplanation: Evaluate the trained model\nEnd of explanation\n\"\"\"\n\n#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nExplanation: Copyright 2021 The TensorFlow Authors.\nEnd of explanation\n\"\"\"\n\n\nimport sys\nif 'google.colab' in sys.modules:\n  !pip install --upgrade pip\n\n\"\"\"\nExplanation: TFX Keras コンポーネントのチュートリアル\nTensorFlow Extended",
  "2k": "import numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\"\"\"\nExplanation: Simple MNIST convnet\nAuthor: fchollet<br>\nDate created: 2015/06/19<br>\nLast modified: 2020/04/21<br>\nDescription: A simple convnet that achieves ~99% test accuracy on MNIST.\nSetup\nEnd of explanation\n\"\"\"\n\n\n# Model / data parameters\nnum_classes = 10\ninput_shape = (28, 28, 1)\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") / 255\nx_test = x_test.astype(\"float32\") / 255\n# Make sure images have shape (28, 28, 1)\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\nprint(\"x_train shape:\", x_train.shape)\nprint(x_train.shape[0], \"train samples\")\nprint(x_test.shape[0], \"test samples\")\n\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\"\"\"\nExplanation: Prepare the data\nEnd of explanation\n\"\"\"\n\n\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Flatten(),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation=\"softmax\"),\n    ]\n)\n\nmodel.summary()\n\n\"\"\"\nExplanation: Build the model\nEnd of explanation\n\"\"\"\n\n\nbatch_size = 128\nepochs = 15\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n\n\"\"\"\nExplanation: Train the model\nEnd of explanation\n\"\"\"\n\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])\n\n\"\"\"\nExplanation: Evaluate the trained model\nEnd of explanation\n\"\"\"\n\n#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nExplanation: Copyright 2021 The TensorFlow Authors.\nEnd of explanation\n\"\"\"\n\n\nimport sys\nif 'google.colab' in sys.modules:\n  !pip install --upgrade pip\n\n\"\"\"\nExplanation: TFX Keras コンポーネントのチュートリアル\nTensorFlow Extended (TFX) の各コンポーネントの紹介\n注：この例は、Jupyter スタイルのノートブックで今すぐ実行できます。セットアップは必要ありません。「Google Colab で実行」をクリックするだけです\n<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n<td><a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/tfx/components_keras\"> <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a></td>\n<td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a></td>\n<td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n<td><a target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n</table></div>\n\nこの Colab ベースのチュートリアルでは、TensorFlow Extended (TFX) のそれぞれの組み込みコンポーネントをインタラクティブに説明します。\nここではデータの取り込みからモデルのプッシュ、サービングまで、エンド ツー エンドの機械学習パイプラインのすべてのステップを見ていきます。\n完了したら、このノートブックのコンテンツを TFX パイプライン ソース コードとして自動的にエクスポートできます。これは、Apache Airflow および Apache Beam とオーケストレーションできます。\n注意: このノートブックは、TFX パイプラインでのネイティブ Keras モデルの使用を示しています。TFX は TensorFlow 2 バージョンの Keras のみをサポートします。\n背景情報\nこのノートブックは、Jupyter/Colab 環境で TFX を使用する方法を示しています。 ここでは、インタラクティブなノートブックでシカゴのタクシーの例を見ていきます。\nTFX パイプラインの構造に慣れるのには、インタラクティブなノートブックで作業するのが便利です。独自のパイプラインを軽量の開発環境として開発する場合にも役立ちますが、インタラクティブ ノートブックのオーケストレーション",
  "4k": "import numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\"\"\"\nExplanation: Simple MNIST convnet\nAuthor: fchollet<br>\nDate created: 2015/06/19<br>\nLast modified: 2020/04/21<br>\nDescription: A simple convnet that achieves ~99% test accuracy on MNIST.\nSetup\nEnd of explanation\n\"\"\"\n\n\n# Model / data parameters\nnum_classes = 10\ninput_shape = (28, 28, 1)\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") / 255\nx_test = x_test.astype(\"float32\") / 255\n# Make sure images have shape (28, 28, 1)\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\nprint(\"x_train shape:\", x_train.shape)\nprint(x_train.shape[0], \"train samples\")\nprint(x_test.shape[0], \"test samples\")\n\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\"\"\"\nExplanation: Prepare the data\nEnd of explanation\n\"\"\"\n\n\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Flatten(),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation=\"softmax\"),\n    ]\n)\n\nmodel.summary()\n\n\"\"\"\nExplanation: Build the model\nEnd of explanation\n\"\"\"\n\n\nbatch_size = 128\nepochs = 15\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n\n\"\"\"\nExplanation: Train the model\nEnd of explanation\n\"\"\"\n\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])\n\n\"\"\"\nExplanation: Evaluate the trained model\nEnd of explanation\n\"\"\"\n\n#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nExplanation: Copyright 2021 The TensorFlow Authors.\nEnd of explanation\n\"\"\"\n\n\nimport sys\nif 'google.colab' in sys.modules:\n  !pip install --upgrade pip\n\n\"\"\"\nExplanation: TFX Keras コンポーネントのチュートリアル\nTensorFlow Extended (TFX) の各コンポーネントの紹介\n注：この例は、Jupyter スタイルのノートブックで今すぐ実行できます。セットアップは必要ありません。「Google Colab で実行」をクリックするだけです\n<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n<td><a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/tfx/components_keras\"> <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a></td>\n<td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a></td>\n<td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n<td><a target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n</table></div>\n\nこの Colab ベースのチュートリアルでは、TensorFlow Extended (TFX) のそれぞれの組み込みコンポーネントをインタラクティブに説明します。\nここではデータの取り込みからモデルのプッシュ、サービングまで、エンド ツー エンドの機械学習パイプラインのすべてのステップを見ていきます。\n完了したら、このノートブックのコンテンツを TFX パイプライン ソース コードとして自動的にエクスポートできます。これは、Apache Airflow および Apache Beam とオーケストレーションできます。\n注意: このノートブックは、TFX パイプラインでのネイティブ Keras モデルの使用を示しています。TFX は TensorFlow 2 バージョンの Keras のみをサポートします。\n背景情報\nこのノートブックは、Jupyter/Colab 環境で TFX を使用する方法を示しています。 ここでは、インタラクティブなノートブックでシカゴのタクシーの例を見ていきます。\nTFX パイプラインの構造に慣れるのには、インタラクティブなノートブックで作業するのが便利です。独自のパイプラインを軽量の開発環境として開発する場合にも役立ちますが、インタラクティブ ノートブックのオーケストレーションとメタデータ アーティファクトへのアクセス方法には違いがあるので注意してください。\nオーケストレーション\nTFX の実稼働デプロイメントでは、Apache Airflow、Kubeflow Pipelines、Apache Beam などのオーケストレーターを使用して、TFX コンポーネントの事前定義済みパイプライン グラフをオーケストレーションします。インタラクティブなノートブックでは、ノートブック自体がオーケストレーターであり、ノートブック セルを実行するときにそれぞれの TFX コンポーネントを実行します。\nメタデータ\nTFX の実稼働デプロイメントでは、ML Metadata（MLMD）API を介してメタデータにアクセスします。MLMD は、メタデータ プロパティを MySQL や SQLite などのデータベースに格納し、メタデータ ペイロードをファイル システムなどの永続ストアに保存します。インタラクティブなノートブックでは、プロパティとペイロードの両方が、Jupyter ノートブックまたは Colab サーバーの /tmp ディレクトリにあるエフェメラル SQLite データベースに保存されます。\nセットアップ\nまず、必要なパッケージをインストールしてインポートし、パスを設定して、データをダウンロードします。\nPip のアップグレード\nローカルで実行する場合にシステム Pipをアップグレードしないようにするには、Colab で実行していることを確認してください。もちろん、ローカルシステムは個別にアップグレードできます。\nEnd of explanation\n\"\"\"\n\n\n!pip install -U tfx\n\n\"\"\"\nExplanation: TFX をインストールする\n注：Google Colab では、パッケージが更新されるため、このセルを初めて実行するときに、ランタイムを再起動する必要があります（[ランタイム]&gt; [ランタイムの再起動...]）。\nEnd of explanation\n\"\"\"\n\n\nimport os\nimport pprint\nimport tempfile\nimport urllib\n\nimport absl\nimport tensorflow as tf\nimport tensorflow_model_analysis as tfma\ntf.get_logger().propagate = False\npp = pprint.PrettyPrinter()\n\nfrom tfx import v1 as tfx\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n\n%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip\n\n\"\"\"\nExplanation: ランタイムを再起動しましたか？\nGoogle Colab を使用している場合は、上記のセルを初めて実行するときにランタイムを再起動する必要があります（[ランタイム]&gt; [ランタイムの再起動...]）。 これは、Colab がパッケージを読み込むために必要ですです。\nパッケージをインポートする\n標準の TFX コンポーネント クラスを含む必要なパッケージをインポートします。\nEnd of explanation\n\"\"\"\n\n\nprint('TensorFlow version: {}'.format(tf.__version__))\nprint('TFX version: {}'.format(tfx.__version__))\n\n\"\"\"\nExplanation: ライブラリのバージョンを確認します。\nEnd of explanation\n\"\"\"\n\n\n# This is the root directory for your TFX pip package installation.\n_tfx_root = tfx.__path__[0]\n\n# This is the directory containing the TFX Chicago Taxi Pipeline example.\n_taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')\n\n# This is the path where your model will be pushed for serving.\n_serving_model_dir = os.path.join(\n    tempfile.mkdtemp(), 'serving_model/taxi_simple')\n\n# Set up logging.\nabsl.logging.set_verbosity(absl.logging.INFO)\n\n\"\"\"\nExplanation: パイプライン パスを設定\nEnd of explanation\n\"\"\"\n\n\n_data_root = tempfile.mkdtemp(prefix='tfx-data')\nDATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv'\n_data_filepath = os.path.join(_data_root, \"data.csv\")\nurllib.request.urlretrieve(DATA_PATH, _data_filepath)\n\n\"\"\"\nExplanation: サンプルデータのダウンロード\nTFX パイプラインで使用するサンプル データセットをダウンロードします。\n使用しているデータセットは、シカゴ市がリリースした タクシートリップデータセットです。 このデータセットの列は次のとおりです。\n<table>\n<tr>\n<td>pickup_community_area</td>\n<td>fare</td>\n<td>trip_start_month</td>\n</tr>\n<tr>\n<td>trip_start_hour</td>\n<td>trip_start_day</td>\n<td>trip_start_timestamp</td>\n</tr>\n<tr>\n<td>pickup_latitude</td>\n<td>pickup_longitude</td>\n<td>dropoff_latitude</td>\n</tr>\n<tr>\n<td>dropoff_longitude</td>\n<td>trip_miles</td>\n<td>pickup_census_tract</td>\n</tr>\n<tr>\n<td>dropoff_census_tract</td>\n<td>payment_type</td>\n<td>company</td>\n</tr>\n<tr>\n<td>trip_seconds</td>\n<td>dropoff_community_area</td>\n<td>tips</td>\n</tr>\n</table>\n\nこのデータセットを使用して、タクシー乗車のtipsを予測するモデルを構築します。\nEnd of explanation\n\"\"\"\n\n\n!head {_data_filepath}\n\n\"\"\"\nExplanation: CSV ファイルを見てみましょう。\nEnd of explanation\n\"\"\"\n\n\n# Here, we create an InteractiveContext using default parameters. This will\n# use a temporary directory with an ephemeral ML Metadata database instance.\n# To use your own pipeline root or database, the optional properties\n# `pipeline_root` and `metadata_connection_config` may be passed to\n# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n# notebook.\ncontext = InteractiveContext()\n\n\"\"\"\nExplanation: ",
  "8k": "import numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\"\"\"\nExplanation: Simple MNIST convnet\nAuthor: fchollet<br>\nDate created: 2015/06/19<br>\nLast modified: 2020/04/21<br>\nDescription: A simple convnet that achieves ~99% test accuracy on MNIST.\nSetup\nEnd of explanation\n\"\"\"\n\n\n# Model / data parameters\nnum_classes = 10\ninput_shape = (28, 28, 1)\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") / 255\nx_test = x_test.astype(\"float32\") / 255\n# Make sure images have shape (28, 28, 1)\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\nprint(\"x_train shape:\", x_train.shape)\nprint(x_train.shape[0], \"train samples\")\nprint(x_test.shape[0], \"test samples\")\n\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\"\"\"\nExplanation: Prepare the data\nEnd of explanation\n\"\"\"\n\n\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Flatten(),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation=\"softmax\"),\n    ]\n)\n\nmodel.summary()\n\n\"\"\"\nExplanation: Build the model\nEnd of explanation\n\"\"\"\n\n\nbatch_size = 128\nepochs = 15\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n\n\"\"\"\nExplanation: Train the model\nEnd of explanation\n\"\"\"\n\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])\n\n\"\"\"\nExplanation: Evaluate the trained model\nEnd of explanation\n\"\"\"\n\n#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nExplanation: Copyright 2021 The TensorFlow Authors.\nEnd of explanation\n\"\"\"\n\n\nimport sys\nif 'google.colab' in sys.modules:\n  !pip install --upgrade pip\n\n\"\"\"\nExplanation: TFX Keras コンポーネントのチュートリアル\nTensorFlow Extended (TFX) の各コンポーネントの紹介\n注：この例は、Jupyter スタイルのノートブックで今すぐ実行できます。セットアップは必要ありません。「Google Colab で実行」をクリックするだけです\n<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n<td><a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/tfx/components_keras\"> <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a></td>\n<td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a></td>\n<td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n<td><a target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n</table></div>\n\nこの Colab ベースのチュートリアルでは、TensorFlow Extended (TFX) のそれぞれの組み込みコンポーネントをインタラクティブに説明します。\nここではデータの取り込みからモデルのプッシュ、サービングまで、エンド ツー エンドの機械学習パイプラインのすべてのステップを見ていきます。\n完了したら、このノートブックのコンテンツを TFX パイプライン ソース コードとして自動的にエクスポートできます。これは、Apache Airflow および Apache Beam とオーケストレーションできます。\n注意: このノートブックは、TFX パイプラインでのネイティブ Keras モデルの使用を示しています。TFX は TensorFlow 2 バージョンの Keras のみをサポートします。\n背景情報\nこのノートブックは、Jupyter/Colab 環境で TFX を使用する方法を示しています。 ここでは、インタラクティブなノートブックでシカゴのタクシーの例を見ていきます。\nTFX パイプラインの構造に慣れるのには、インタラクティブなノートブックで作業するのが便利です。独自のパイプラインを軽量の開発環境として開発する場合にも役立ちますが、インタラクティブ ノートブックのオーケストレーションとメタデータ アーティファクトへのアクセス方法には違いがあるので注意してください。\nオーケストレーション\nTFX の実稼働デプロイメントでは、Apache Airflow、Kubeflow Pipelines、Apache Beam などのオーケストレーターを使用して、TFX コンポーネントの事前定義済みパイプライン グラフをオーケストレーションします。インタラクティブなノートブックでは、ノートブック自体がオーケストレーターであり、ノートブック セルを実行するときにそれぞれの TFX コンポーネントを実行します。\nメタデータ\nTFX の実稼働デプロイメントでは、ML Metadata（MLMD）API を介してメタデータにアクセスします。MLMD は、メタデータ プロパティを MySQL や SQLite などのデータベースに格納し、メタデータ ペイロードをファイル システムなどの永続ストアに保存します。インタラクティブなノートブックでは、プロパティとペイロードの両方が、Jupyter ノートブックまたは Colab サーバーの /tmp ディレクトリにあるエフェメラル SQLite データベースに保存されます。\nセットアップ\nまず、必要なパッケージをインストールしてインポートし、パスを設定して、データをダウンロードします。\nPip のアップグレード\nローカルで実行する場合にシステム Pipをアップグレードしないようにするには、Colab で実行していることを確認してください。もちろん、ローカルシステムは個別にアップグレードできます。\nEnd of explanation\n\"\"\"\n\n\n!pip install -U tfx\n\n\"\"\"\nExplanation: TFX をインストールする\n注：Google Colab では、パッケージが更新されるため、このセルを初めて実行するときに、ランタイムを再起動する必要があります（[ランタイム]&gt; [ランタイムの再起動...]）。\nEnd of explanation\n\"\"\"\n\n\nimport os\nimport pprint\nimport tempfile\nimport urllib\n\nimport absl\nimport tensorflow as tf\nimport tensorflow_model_analysis as tfma\ntf.get_logger().propagate = False\npp = pprint.PrettyPrinter()\n\nfrom tfx import v1 as tfx\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n\n%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip\n\n\"\"\"\nExplanation: ランタイムを再起動しましたか？\nGoogle Colab を使用している場合は、上記のセルを初めて実行するときにランタイムを再起動する必要があります（[ランタイム]&gt; [ランタイムの再起動...]）。 これは、Colab がパッケージを読み込むために必要ですです。\nパッケージをインポートする\n標準の TFX コンポーネント クラスを含む必要なパッケージをインポートします。\nEnd of explanation\n\"\"\"\n\n\nprint('TensorFlow version: {}'.format(tf.__version__))\nprint('TFX version: {}'.format(tfx.__version__))\n\n\"\"\"\nExplanation: ライブラリのバージョンを確認します。\nEnd of explanation\n\"\"\"\n\n\n# This is the root directory for your TFX pip package installation.\n_tfx_root = tfx.__path__[0]\n\n# This is the directory containing the TFX Chicago Taxi Pipeline example.\n_taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')\n\n# This is the path where your model will be pushed for serving.\n_serving_model_dir = os.path.join(\n    tempfile.mkdtemp(), 'serving_model/taxi_simple')\n\n# Set up logging.\nabsl.logging.set_verbosity(absl.logging.INFO)\n\n\"\"\"\nExplanation: パイプライン パスを設定\nEnd of explanation\n\"\"\"\n\n\n_data_root = tempfile.mkdtemp(prefix='tfx-data')\nDATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv'\n_data_filepath = os.path.join(_data_root, \"data.csv\")\nurllib.request.urlretrieve(DATA_PATH, _data_filepath)\n\n\"\"\"\nExplanation: サンプルデータのダウンロード\nTFX パイプラインで使用するサンプル データセットをダウンロードします。\n使用しているデータセットは、シカゴ市がリリースした タクシートリップデータセットです。 このデータセットの列は次のとおりです。\n<table>\n<tr>\n<td>pickup_community_area</td>\n<td>fare</td>\n<td>trip_start_month</td>\n</tr>\n<tr>\n<td>trip_start_hour</td>\n<td>trip_start_day</td>\n<td>trip_start_timestamp</td>\n</tr>\n<tr>\n<td>pickup_latitude</td>\n<td>pickup_longitude</td>\n<td>dropoff_latitude</td>\n</tr>\n<tr>\n<td>dropoff_longitude</td>\n<td>trip_miles</td>\n<td>pickup_census_tract</td>\n</tr>\n<tr>\n<td>dropoff_census_tract</td>\n<td>payment_type</td>\n<td>company</td>\n</tr>\n<tr>\n<td>trip_seconds</td>\n<td>dropoff_community_area</td>\n<td>tips</td>\n</tr>\n</table>\n\nこのデータセットを使用して、タクシー乗車のtipsを予測するモデルを構築します。\nEnd of explanation\n\"\"\"\n\n\n!head {_data_filepath}\n\n\"\"\"\nExplanation: CSV ファイルを見てみましょう。\nEnd of explanation\n\"\"\"\n\n\n# Here, we create an InteractiveContext using default parameters. This will\n# use a temporary directory with an ephemeral ML Metadata database instance.\n# To use your own pipeline root or database, the optional properties\n# `pipeline_root` and `metadata_connection_config` may be passed to\n# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n# notebook.\ncontext = InteractiveContext()\n\n\"\"\"\nExplanation: 注：このWeb サイトは、シカゴ市の公式 Web サイト www.cityofchicago.org で公開されたデータを変更して使用するアプリケーションを提供します。シカゴ市は、この Web サイトで提供されるデータの内容、正確性、適時性、または完全性について一切の表明を行いません。この Web サイトで提供されるデータは、いつでも変更される可能性があります。かかる Web サイトで提供されるデータはユーザーの自己責任で利用されるものとします。\nInteractiveContext を作成する\n最後に、このノートブックで TFX コンポーネントをインタラクティブに実行できるようにする InteractiveContext を作成します。\nEnd of explanation\n\"\"\"\n\n\nexample_gen = tfx.components.CsvExampleGen(input_base=_data_root)\ncontext.run(example_gen, enable_cache=True)\n\n\"\"\"\nExplanation: TFX コンポーネントをインタラクティブに実行する\n次のセルでは、TFX コンポーネントを 1 つずつ作成し、それぞれを実行して、出力アーティファクトを視覚化します。\nExampleGen\nExampleGen コンポーネントは通常、TFX パイプラインの先頭にあり、以下を実行します。\n\nデータをトレーニング セットと評価セットに分割します (デフォルトでは、2/3 トレーニング + 1/3 評価)。\nデータを tf.Example 形式に変換します。 (詳細はこちら)\n他のコンポーネントがアクセスできるように、データを _tfx_root ディレクトリにコピーします。\n\nExampleGen は、データソースへのパスを入力として受け取ります。 ここでは、これはダウンロードした CSV を含む _data_root パスです。\n注意: このノートブックでは、コンポーネントを 1 つずつインスタンス化し、InteractiveContext.run() で実行しますが、実稼働環境では、すべてのコンポーネントを事前に Pipelineで指定して、オーケストレーターに渡します（TFX パイプライン ガイドの構築を参照してください）。\nキャッシュを有効にする\nノートブックで InteractiveContext を使用してパイプラインを作成している場合、個別のコンポーネントが出力をキャッシュするタイミングを制御することができます。コンポーネントが前に生成した出力アーティファクトを再利用する場合は、enable_cache を True に設定します。コードを変更するなどにより、コンポーネントの出力アーティファクトを再計算する場合は、enable_cache を False に設定します。\nEnd of explanation\n\"\"\"\n\n\nartifact = example_gen.outputs['examples'].get()[0]\nprint(artifact.split_names, artifact.uri)\n\n\"\"\"\nExplanation: ExampleGenの出力アーティファクトを調べてみましょう。このコンポーネントは、トレーニングサンプルと評価サンプルの 2 つのアーティファクトを生成します。\nEnd of explanation\n\"\"\"\n\n\n# Get the URI of the output artifact representing the training examples, which is a directory\ntrain_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')\n\n# Get the list of files in this directory (all compressed TFRecord files)\ntfrecord_filenames = [os.path.join(train_uri, name)\n                      for name in os.listdir(train_uri)]\n\n# Create a `TFRecordDataset` to read these files\ndataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n\n# Iterate over the first 3 records and decode them.\nfor tfrecord in dataset.take(3):\n  serialized_example = tfrecord.numpy()\n  example = tf.train.Example()\n  example.ParseFromString(serialized_example)\n  pp.pprint(example)\n\n\"\"\"\nExplanation: また、最初の 3 つのトレーニングサンプルも見てみます。\nEnd of explanation\n\"\"\"\n\n\nstatistics_gen = tfx.components.StatisticsGen(\n    examples=example_gen.outputs['examples'])\ncontext.run(statistics_gen, enable_cache=True)\n\n\"\"\"\nExplanation: ExampleGenがデータの取り込みを完了したので、次のステップ、データ分析に進みます。\nStatisticsGen\nStatisticsGenコンポーネントは、データ分析用のデータセットの統計を計算し、ダウンストリームのコンポーネントで使用します。これは、TensorFlow Data Validation ライブラリを使用します。\nStatisticsGenコンポーネントは、データ分析用のデータセットの統計を計算し、ダウンストリーム コンポーネントで使用します。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(statistics_gen.outputs['statistics'])\n\n\"\"\"\nExplanation: StatisticsGen の実行が完了すると、出力された統計を視覚化できます。 色々なプロットを試してみてください！\nEnd of explanation\n\"\"\"\n\n\nschema_gen = tfx.components.SchemaGen(\n    statistics=statistics_gen.outputs['statistics'],\n    infer_feature_shape=False)\ncontext.run(schema_gen, enable_cache=True)\n\n\"\"\"\nExplanation: SchemaGen\nSchemaGen コンポーネントは、データ統計に基づいてスキーマを生成します。（スキーマは、データセット内の特徴の予想される境界、タイプ、プロパティを定義します。）また、TensorFlow データ検証ライブラリも使用します。\n注意: 生成されたスキーマはベストエフォートのもので、データの基本的なプロパティだけを推論しようとします。確認し、必要に応じて修正する必要があります。\nSchemaGen は、StatisticsGen で生成した統計を入力として受け取り、デフォルトでトレーニング分割を参照します。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(schema_gen.outputs['schema'])\n\n\"\"\"\nExplanation: SchemaGen の実行が完了すると、生成されたスキーマをテーブルとして視覚化できます。\nEnd of explanation\n\"\"\"\n\n\nexample_validator = tfx.components.ExampleValidator(\n    statistics=statistics_gen.outputs['statistics'],\n    schema=schema_gen.outputs['schema'])\ncontext.run(example_validator, enable_cache=True)\n\n\"\"\"\nExplanation: データセットのそれぞれの特徴は、スキーマ テーブルのプロパティの横に行として表示されます。スキーマは、ドメインとして示される、カテゴリ特徴が取るすべての値もキャプチャします。\nスキーマの詳細については、SchemaGen のドキュメントをご覧ください。\nExampleValidator\nExampleValidator コンポーネントは、スキーマで定義された期待に基づいて、データの異常を検出します。また、TensorFlow Data Validation ライブラリも使用します。\nExampleValidator は、Statistics Gen{/code 1} からの統計と &lt;code data-md-type=\"codespan\"&gt;SchemaGen からのスキーマを入力として受け取ります。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(example_validator.outputs['anomalies'])\n\n\"\"\"\nExplanation: ExampleValidator の実行が完了すると、異常をテーブルとして視覚化できます。\nEnd of explanation\n\"\"\"\n\n\n_taxi_constants_module_file = 'taxi_constants.py'\n\n%%writefile {_taxi_constants_module_file}\n\nNUMERICAL_FEATURES = ['trip_miles', 'fare', 'trip_seconds']\n\nBUCKET_FEATURES = [\n    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n    'dropoff_longitude'\n]\n# Number of buckets used by tf.transform for encoding each feature.\nFEATURE_BUCKET_COUNT = 10\n\nCATEGORICAL_NUMERICAL_FEATURES = [\n    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n    'dropoff_community_area'\n]\n\nCATEGORICAL_STRING_FEATURES = [\n    'payment_type',\n    'company',\n]\n\n# Number of vocabulary terms used for encoding categorical features.\nVOCAB_SIZE = 1000\n\n# Count of out-of-vocab buckets in which unrecognized categorical are hashed.\nOOV_SIZE = 10\n\n# Keys\nLABEL_KEY = 'tips'\nFARE_KEY = 'fare'\n\ndef t_name(key):\n  \"\"\"\n  Rename the feature keys so that they don't clash with the raw keys when\n  running the Evaluator component.\n  Args:\n    key: The original feature key\n  Returns:\n    key with '_xf' appended\n  \"\"\"\n  return key + '_xf'\n\n\"\"\"\nExplanation: 異常テーブルでは、異常がないことがわかります。これは、分析した最初のデータセットで、スキーマはこれに合わせて調整されているため、異常がないことが予想されます。このスキーマを確認する必要があります。予期されないものは、データに異常があることを意味します。確認されたスキーマを使用して将来のデータを保護できます。ここで生成された異常は、モデルのパフォーマンスをデバッグし、データが時間の経過とともにどのように変化するかを理解し、データ エラーを特定するために使用できます。\n変換\nTransformコンポーネントは、トレーニングとサービングの両方で特徴量エンジニアリングを実行します。これは、 TensorFlow Transform ライブラリを使用します。\nTransformは、ExampleGenからのデータ、SchemaGenからのスキーマ、ユーザー定義の Transform コードを含むモジュールを入力として受け取ります。\n以下のユーザー定義の Transform コードの例を見てみましょう（TensorFlow Transform API の概要については、チュートリアルを参照してください）。まず、特徴量エンジニアリングのいくつかの定数を定義します。\n注意: %%writefile セル マジックは、セルの内容をディスク上の.pyファイルとして保存します。これにより、Transform コンポーネントはコードをモジュールとして読み込むことができます。\nEnd of explanation\n\"\"\"\n\n\n_taxi_transform_module_file = 'taxi_transform.py'\n\n%%writefile {_taxi_transform_module_file}\n\nimport tensorflow as tf\nimport tensorflow_transform as tft\n\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimport taxi_constants\nimport sys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  import importlib\n  importlib.reload(taxi_constants)\n\n_NUMERICAL_FEATURES = taxi_constants.NUMERICAL_FEATURES\n_BUCKET_FEATURES = taxi_constants.BUCKET_FEATURES\n_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\n_CATEGORICAL_NUMERICAL_FEATURES = taxi_constants.CATEGORICAL_NUMERICAL_FEATURES\n_CATEGORICAL_STRING_FEATURES = taxi_constants.CATEGORICAL_STRING_FEATURES\n_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\n_OOV_SIZE = taxi_constants.OOV_SIZE\n_FARE_KEY = taxi_constants.FARE_KEY\n_LABEL_KEY = taxi_constants.LABEL_KEY\n\n\ndef _make_one_hot(x, key):\n  \"\"\"Make a one-hot tensor to encode categorical features.\n  Args:\n    X: A dense tensor\n    key: A string key for the feature in the input\n  Returns:\n    A dense one-hot tensor as a float",
  "16k": "import numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\"\"\"\nExplanation: Simple MNIST convnet\nAuthor: fchollet<br>\nDate created: 2015/06/19<br>\nLast modified: 2020/04/21<br>\nDescription: A simple convnet that achieves ~99% test accuracy on MNIST.\nSetup\nEnd of explanation\n\"\"\"\n\n\n# Model / data parameters\nnum_classes = 10\ninput_shape = (28, 28, 1)\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") / 255\nx_test = x_test.astype(\"float32\") / 255\n# Make sure images have shape (28, 28, 1)\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\nprint(\"x_train shape:\", x_train.shape)\nprint(x_train.shape[0], \"train samples\")\nprint(x_test.shape[0], \"test samples\")\n\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\"\"\"\nExplanation: Prepare the data\nEnd of explanation\n\"\"\"\n\n\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Flatten(),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation=\"softmax\"),\n    ]\n)\n\nmodel.summary()\n\n\"\"\"\nExplanation: Build the model\nEnd of explanation\n\"\"\"\n\n\nbatch_size = 128\nepochs = 15\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n\n\"\"\"\nExplanation: Train the model\nEnd of explanation\n\"\"\"\n\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])\n\n\"\"\"\nExplanation: Evaluate the trained model\nEnd of explanation\n\"\"\"\n\n#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nExplanation: Copyright 2021 The TensorFlow Authors.\nEnd of explanation\n\"\"\"\n\n\nimport sys\nif 'google.colab' in sys.modules:\n  !pip install --upgrade pip\n\n\"\"\"\nExplanation: TFX Keras コンポーネントのチュートリアル\nTensorFlow Extended (TFX) の各コンポーネントの紹介\n注：この例は、Jupyter スタイルのノートブックで今すぐ実行できます。セットアップは必要ありません。「Google Colab で実行」をクリックするだけです\n<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n<td><a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/tfx/components_keras\"> <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a></td>\n<td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a></td>\n<td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n<td><a target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n</table></div>\n\nこの Colab ベースのチュートリアルでは、TensorFlow Extended (TFX) のそれぞれの組み込みコンポーネントをインタラクティブに説明します。\nここではデータの取り込みからモデルのプッシュ、サービングまで、エンド ツー エンドの機械学習パイプラインのすべてのステップを見ていきます。\n完了したら、このノートブックのコンテンツを TFX パイプライン ソース コードとして自動的にエクスポートできます。これは、Apache Airflow および Apache Beam とオーケストレーションできます。\n注意: このノートブックは、TFX パイプラインでのネイティブ Keras モデルの使用を示しています。TFX は TensorFlow 2 バージョンの Keras のみをサポートします。\n背景情報\nこのノートブックは、Jupyter/Colab 環境で TFX を使用する方法を示しています。 ここでは、インタラクティブなノートブックでシカゴのタクシーの例を見ていきます。\nTFX パイプラインの構造に慣れるのには、インタラクティブなノートブックで作業するのが便利です。独自のパイプラインを軽量の開発環境として開発する場合にも役立ちますが、インタラクティブ ノートブックのオーケストレーションとメタデータ アーティファクトへのアクセス方法には違いがあるので注意してください。\nオーケストレーション\nTFX の実稼働デプロイメントでは、Apache Airflow、Kubeflow Pipelines、Apache Beam などのオーケストレーターを使用して、TFX コンポーネントの事前定義済みパイプライン グラフをオーケストレーションします。インタラクティブなノートブックでは、ノートブック自体がオーケストレーターであり、ノートブック セルを実行するときにそれぞれの TFX コンポーネントを実行します。\nメタデータ\nTFX の実稼働デプロイメントでは、ML Metadata（MLMD）API を介してメタデータにアクセスします。MLMD は、メタデータ プロパティを MySQL や SQLite などのデータベースに格納し、メタデータ ペイロードをファイル システムなどの永続ストアに保存します。インタラクティブなノートブックでは、プロパティとペイロードの両方が、Jupyter ノートブックまたは Colab サーバーの /tmp ディレクトリにあるエフェメラル SQLite データベースに保存されます。\nセットアップ\nまず、必要なパッケージをインストールしてインポートし、パスを設定して、データをダウンロードします。\nPip のアップグレード\nローカルで実行する場合にシステム Pipをアップグレードしないようにするには、Colab で実行していることを確認してください。もちろん、ローカルシステムは個別にアップグレードできます。\nEnd of explanation\n\"\"\"\n\n\n!pip install -U tfx\n\n\"\"\"\nExplanation: TFX をインストールする\n注：Google Colab では、パッケージが更新されるため、このセルを初めて実行するときに、ランタイムを再起動する必要があります（[ランタイム]&gt; [ランタイムの再起動...]）。\nEnd of explanation\n\"\"\"\n\n\nimport os\nimport pprint\nimport tempfile\nimport urllib\n\nimport absl\nimport tensorflow as tf\nimport tensorflow_model_analysis as tfma\ntf.get_logger().propagate = False\npp = pprint.PrettyPrinter()\n\nfrom tfx import v1 as tfx\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n\n%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip\n\n\"\"\"\nExplanation: ランタイムを再起動しましたか？\nGoogle Colab を使用している場合は、上記のセルを初めて実行するときにランタイムを再起動する必要があります（[ランタイム]&gt; [ランタイムの再起動...]）。 これは、Colab がパッケージを読み込むために必要ですです。\nパッケージをインポートする\n標準の TFX コンポーネント クラスを含む必要なパッケージをインポートします。\nEnd of explanation\n\"\"\"\n\n\nprint('TensorFlow version: {}'.format(tf.__version__))\nprint('TFX version: {}'.format(tfx.__version__))\n\n\"\"\"\nExplanation: ライブラリのバージョンを確認します。\nEnd of explanation\n\"\"\"\n\n\n# This is the root directory for your TFX pip package installation.\n_tfx_root = tfx.__path__[0]\n\n# This is the directory containing the TFX Chicago Taxi Pipeline example.\n_taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')\n\n# This is the path where your model will be pushed for serving.\n_serving_model_dir = os.path.join(\n    tempfile.mkdtemp(), 'serving_model/taxi_simple')\n\n# Set up logging.\nabsl.logging.set_verbosity(absl.logging.INFO)\n\n\"\"\"\nExplanation: パイプライン パスを設定\nEnd of explanation\n\"\"\"\n\n\n_data_root = tempfile.mkdtemp(prefix='tfx-data')\nDATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv'\n_data_filepath = os.path.join(_data_root, \"data.csv\")\nurllib.request.urlretrieve(DATA_PATH, _data_filepath)\n\n\"\"\"\nExplanation: サンプルデータのダウンロード\nTFX パイプラインで使用するサンプル データセットをダウンロードします。\n使用しているデータセットは、シカゴ市がリリースした タクシートリップデータセットです。 このデータセットの列は次のとおりです。\n<table>\n<tr>\n<td>pickup_community_area</td>\n<td>fare</td>\n<td>trip_start_month</td>\n</tr>\n<tr>\n<td>trip_start_hour</td>\n<td>trip_start_day</td>\n<td>trip_start_timestamp</td>\n</tr>\n<tr>\n<td>pickup_latitude</td>\n<td>pickup_longitude</td>\n<td>dropoff_latitude</td>\n</tr>\n<tr>\n<td>dropoff_longitude</td>\n<td>trip_miles</td>\n<td>pickup_census_tract</td>\n</tr>\n<tr>\n<td>dropoff_census_tract</td>\n<td>payment_type</td>\n<td>company</td>\n</tr>\n<tr>\n<td>trip_seconds</td>\n<td>dropoff_community_area</td>\n<td>tips</td>\n</tr>\n</table>\n\nこのデータセットを使用して、タクシー乗車のtipsを予測するモデルを構築します。\nEnd of explanation\n\"\"\"\n\n\n!head {_data_filepath}\n\n\"\"\"\nExplanation: CSV ファイルを見てみましょう。\nEnd of explanation\n\"\"\"\n\n\n# Here, we create an InteractiveContext using default parameters. This will\n# use a temporary directory with an ephemeral ML Metadata database instance.\n# To use your own pipeline root or database, the optional properties\n# `pipeline_root` and `metadata_connection_config` may be passed to\n# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n# notebook.\ncontext = InteractiveContext()\n\n\"\"\"\nExplanation: 注：このWeb サイトは、シカゴ市の公式 Web サイト www.cityofchicago.org で公開されたデータを変更して使用するアプリケーションを提供します。シカゴ市は、この Web サイトで提供されるデータの内容、正確性、適時性、または完全性について一切の表明を行いません。この Web サイトで提供されるデータは、いつでも変更される可能性があります。かかる Web サイトで提供されるデータはユーザーの自己責任で利用されるものとします。\nInteractiveContext を作成する\n最後に、このノートブックで TFX コンポーネントをインタラクティブに実行できるようにする InteractiveContext を作成します。\nEnd of explanation\n\"\"\"\n\n\nexample_gen = tfx.components.CsvExampleGen(input_base=_data_root)\ncontext.run(example_gen, enable_cache=True)\n\n\"\"\"\nExplanation: TFX コンポーネントをインタラクティブに実行する\n次のセルでは、TFX コンポーネントを 1 つずつ作成し、それぞれを実行して、出力アーティファクトを視覚化します。\nExampleGen\nExampleGen コンポーネントは通常、TFX パイプラインの先頭にあり、以下を実行します。\n\nデータをトレーニング セットと評価セットに分割します (デフォルトでは、2/3 トレーニング + 1/3 評価)。\nデータを tf.Example 形式に変換します。 (詳細はこちら)\n他のコンポーネントがアクセスできるように、データを _tfx_root ディレクトリにコピーします。\n\nExampleGen は、データソースへのパスを入力として受け取ります。 ここでは、これはダウンロードした CSV を含む _data_root パスです。\n注意: このノートブックでは、コンポーネントを 1 つずつインスタンス化し、InteractiveContext.run() で実行しますが、実稼働環境では、すべてのコンポーネントを事前に Pipelineで指定して、オーケストレーターに渡します（TFX パイプライン ガイドの構築を参照してください）。\nキャッシュを有効にする\nノートブックで InteractiveContext を使用してパイプラインを作成している場合、個別のコンポーネントが出力をキャッシュするタイミングを制御することができます。コンポーネントが前に生成した出力アーティファクトを再利用する場合は、enable_cache を True に設定します。コードを変更するなどにより、コンポーネントの出力アーティファクトを再計算する場合は、enable_cache を False に設定します。\nEnd of explanation\n\"\"\"\n\n\nartifact = example_gen.outputs['examples'].get()[0]\nprint(artifact.split_names, artifact.uri)\n\n\"\"\"\nExplanation: ExampleGenの出力アーティファクトを調べてみましょう。このコンポーネントは、トレーニングサンプルと評価サンプルの 2 つのアーティファクトを生成します。\nEnd of explanation\n\"\"\"\n\n\n# Get the URI of the output artifact representing the training examples, which is a directory\ntrain_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')\n\n# Get the list of files in this directory (all compressed TFRecord files)\ntfrecord_filenames = [os.path.join(train_uri, name)\n                      for name in os.listdir(train_uri)]\n\n# Create a `TFRecordDataset` to read these files\ndataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n\n# Iterate over the first 3 records and decode them.\nfor tfrecord in dataset.take(3):\n  serialized_example = tfrecord.numpy()\n  example = tf.train.Example()\n  example.ParseFromString(serialized_example)\n  pp.pprint(example)\n\n\"\"\"\nExplanation: また、最初の 3 つのトレーニングサンプルも見てみます。\nEnd of explanation\n\"\"\"\n\n\nstatistics_gen = tfx.components.StatisticsGen(\n    examples=example_gen.outputs['examples'])\ncontext.run(statistics_gen, enable_cache=True)\n\n\"\"\"\nExplanation: ExampleGenがデータの取り込みを完了したので、次のステップ、データ分析に進みます。\nStatisticsGen\nStatisticsGenコンポーネントは、データ分析用のデータセットの統計を計算し、ダウンストリームのコンポーネントで使用します。これは、TensorFlow Data Validation ライブラリを使用します。\nStatisticsGenコンポーネントは、データ分析用のデータセットの統計を計算し、ダウンストリーム コンポーネントで使用します。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(statistics_gen.outputs['statistics'])\n\n\"\"\"\nExplanation: StatisticsGen の実行が完了すると、出力された統計を視覚化できます。 色々なプロットを試してみてください！\nEnd of explanation\n\"\"\"\n\n\nschema_gen = tfx.components.SchemaGen(\n    statistics=statistics_gen.outputs['statistics'],\n    infer_feature_shape=False)\ncontext.run(schema_gen, enable_cache=True)\n\n\"\"\"\nExplanation: SchemaGen\nSchemaGen コンポーネントは、データ統計に基づいてスキーマを生成します。（スキーマは、データセット内の特徴の予想される境界、タイプ、プロパティを定義します。）また、TensorFlow データ検証ライブラリも使用します。\n注意: 生成されたスキーマはベストエフォートのもので、データの基本的なプロパティだけを推論しようとします。確認し、必要に応じて修正する必要があります。\nSchemaGen は、StatisticsGen で生成した統計を入力として受け取り、デフォルトでトレーニング分割を参照します。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(schema_gen.outputs['schema'])\n\n\"\"\"\nExplanation: SchemaGen の実行が完了すると、生成されたスキーマをテーブルとして視覚化できます。\nEnd of explanation\n\"\"\"\n\n\nexample_validator = tfx.components.ExampleValidator(\n    statistics=statistics_gen.outputs['statistics'],\n    schema=schema_gen.outputs['schema'])\ncontext.run(example_validator, enable_cache=True)\n\n\"\"\"\nExplanation: データセットのそれぞれの特徴は、スキーマ テーブルのプロパティの横に行として表示されます。スキーマは、ドメインとして示される、カテゴリ特徴が取るすべての値もキャプチャします。\nスキーマの詳細については、SchemaGen のドキュメントをご覧ください。\nExampleValidator\nExampleValidator コンポーネントは、スキーマで定義された期待に基づいて、データの異常を検出します。また、TensorFlow Data Validation ライブラリも使用します。\nExampleValidator は、Statistics Gen{/code 1} からの統計と &lt;code data-md-type=\"codespan\"&gt;SchemaGen からのスキーマを入力として受け取ります。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(example_validator.outputs['anomalies'])\n\n\"\"\"\nExplanation: ExampleValidator の実行が完了すると、異常をテーブルとして視覚化できます。\nEnd of explanation\n\"\"\"\n\n\n_taxi_constants_module_file = 'taxi_constants.py'\n\n%%writefile {_taxi_constants_module_file}\n\nNUMERICAL_FEATURES = ['trip_miles', 'fare', 'trip_seconds']\n\nBUCKET_FEATURES = [\n    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n    'dropoff_longitude'\n]\n# Number of buckets used by tf.transform for encoding each feature.\nFEATURE_BUCKET_COUNT = 10\n\nCATEGORICAL_NUMERICAL_FEATURES = [\n    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n    'dropoff_community_area'\n]\n\nCATEGORICAL_STRING_FEATURES = [\n    'payment_type',\n    'company',\n]\n\n# Number of vocabulary terms used for encoding categorical features.\nVOCAB_SIZE = 1000\n\n# Count of out-of-vocab buckets in which unrecognized categorical are hashed.\nOOV_SIZE = 10\n\n# Keys\nLABEL_KEY = 'tips'\nFARE_KEY = 'fare'\n\ndef t_name(key):\n  \"\"\"\n  Rename the feature keys so that they don't clash with the raw keys when\n  running the Evaluator component.\n  Args:\n    key: The original feature key\n  Returns:\n    key with '_xf' appended\n  \"\"\"\n  return key + '_xf'\n\n\"\"\"\nExplanation: 異常テーブルでは、異常がないことがわかります。これは、分析した最初のデータセットで、スキーマはこれに合わせて調整されているため、異常がないことが予想されます。このスキーマを確認する必要があります。予期されないものは、データに異常があることを意味します。確認されたスキーマを使用して将来のデータを保護できます。ここで生成された異常は、モデルのパフォーマンスをデバッグし、データが時間の経過とともにどのように変化するかを理解し、データ エラーを特定するために使用できます。\n変換\nTransformコンポーネントは、トレーニングとサービングの両方で特徴量エンジニアリングを実行します。これは、 TensorFlow Transform ライブラリを使用します。\nTransformは、ExampleGenからのデータ、SchemaGenからのスキーマ、ユーザー定義の Transform コードを含むモジュールを入力として受け取ります。\n以下のユーザー定義の Transform コードの例を見てみましょう（TensorFlow Transform API の概要については、チュートリアルを参照してください）。まず、特徴量エンジニアリングのいくつかの定数を定義します。\n注意: %%writefile セル マジックは、セルの内容をディスク上の.pyファイルとして保存します。これにより、Transform コンポーネントはコードをモジュールとして読み込むことができます。\nEnd of explanation\n\"\"\"\n\n\n_taxi_transform_module_file = 'taxi_transform.py'\n\n%%writefile {_taxi_transform_module_file}\n\nimport tensorflow as tf\nimport tensorflow_transform as tft\n\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimport taxi_constants\nimport sys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  import importlib\n  importlib.reload(taxi_constants)\n\n_NUMERICAL_FEATURES = taxi_constants.NUMERICAL_FEATURES\n_BUCKET_FEATURES = taxi_constants.BUCKET_FEATURES\n_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\n_CATEGORICAL_NUMERICAL_FEATURES = taxi_constants.CATEGORICAL_NUMERICAL_FEATURES\n_CATEGORICAL_STRING_FEATURES = taxi_constants.CATEGORICAL_STRING_FEATURES\n_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\n_OOV_SIZE = taxi_constants.OOV_SIZE\n_FARE_KEY = taxi_constants.FARE_KEY\n_LABEL_KEY = taxi_constants.LABEL_KEY\n\n\ndef _make_one_hot(x, key):\n  \"\"\"Make a one-hot tensor to encode categorical features.\n  Args:\n    X: A dense tensor\n    key: A string key for the feature in the input\n  Returns:\n    A dense one-hot tensor as a float list\n  \"\"\"\n  integerized = tft.compute_and_apply_vocabulary(x,\n          top_k=_VOCAB_SIZE,\n          num_oov_buckets=_OOV_SIZE,\n          vocab_filename=key, name=key)\n  depth = (\n      tft.experimental.get_vocabulary_size_by_name(key) + _OOV_SIZE)\n  one_hot_encoded = tf.one_hot(\n      integerized,\n      depth=tf.cast(depth, tf.int32),\n      on_value=1.0,\n      off_value=0.0)\n  return tf.reshape(one_hot_encoded, [-1, depth])\n\n\ndef _fill_in_missing(x):\n  \"\"\"Replace missing values in a SparseTensor.\n  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n  Args:\n    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n      in the second dimension.\n  Returns:\n    A rank 1 tensor where missing values of `x` have been filled in.\n  \"\"\"\n  if not isinstance(x, tf.sparse.SparseTensor):\n    return x\n\n  default_value = '' if x.dtype == tf.string else 0\n  return tf.squeeze(\n      tf.sparse.to_dense(\n          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n          default_value),\n      axis=1)\n\n\ndef preprocessing_fn(inputs):\n  \"\"\"tf.transform's callback function for preprocessing inputs.\n  Args:\n    inputs: map from feature keys to raw not-yet-transformed features.\n  Returns:\n    Map from string feature key to transformed feature operations.\n  \"\"\"\n  outputs = {}\n  for key in _NUMERICAL_FEATURES:\n    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.\n    outputs[taxi_constants.t_name(key)] = tft.scale_to_z_score(\n        _fill_in_missing(inputs[key]), name=key)\n\n  for key in _BUCKET_FEATURES:\n    outputs[taxi_constants.t_name(key)] = tf.cast(tft.bucketize(\n            _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT, name=key),\n            dtype=tf.float32)\n\n  for key in _CATEGORICAL_STRING_FEATURES:\n    outputs[taxi_constants.t_name(key)] = _make_one_hot(_fill_in_missing(inputs[key]), key)\n\n  for key in _CATEGORICAL_NUMERICAL_FEATURES:\n    outputs[taxi_constants.t_name(key)] = _make_one_hot(tf.strings.strip(\n        tf.strings.as_string(_fill_in_missing(inputs[key]))), key)\n\n  # Was this passenger a big tipper?\n  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])\n  tips = _fill_in_missing(inputs[_LABEL_KEY])\n  outputs[_LABEL_KEY] = tf.where(\n      tf.math.is_nan(taxi_fare),\n      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\n      # Test if the tip was > 20% of the fare.\n      tf.cast(\n          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\n\n  return outputs\n\n\"\"\"\nExplanation: 次に、生データを入力として受け取り、モデルがトレーニングできる変換された特徴量を返す {code 0}preprocessing _fn を記述します。\nEnd of explanation\n\"\"\"\n\n\ntransform = tfx.components.Transform(\n    examples=example_gen.outputs['examples'],\n    schema=schema_gen.outputs['schema'],\n    module_file=os.path.abspath(_taxi_transform_module_file))\ncontext.run(transform, enable_cache=True)\n\n\"\"\"\nExplanation: 次に、この特徴量エンジニアリング コードを Transformコンポーネントに渡し、実行してデータを変換します。\nEnd of explanation\n\"\"\"\n\n\ntransform.outputs\n\n\"\"\"\nExplanation: Transformの出力アーティファクトを調べてみましょう。このコンポーネントは、2 種類の出力を生成します。\n\ntransform_graph は、前処理演算を実行できるグラフです (このグラフは、サービングモデルと評価モデルに含まれます)。\ntransformed_examplesは前処理されたトレーニングおよび評価データを表します。\nEnd of explanation\n\"\"\"\n\n\ntrain_uri = transform.outputs['transform_graph'].get()[0].uri\nos.listdir(train_uri)\n\n\"\"\"\nExplanation: transform_graph アーティファクトを見てみましょう。これは、3 つのサブディレクトリを含むディレクトリを指しています。\nEnd of explanation\n\"\"\"\n\n\n# Get the URI of the output artifact representing the transformed examples, which is a directory\ntrain_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n\n# Get the list of files in this directory (all compressed TFRecord files)\ntfrecord_filenames = [os.path.join(train_uri, name)\n                      for name in os.listdir(train_uri)]\n\n# Create a `TFRecordDataset` to read these files\ndataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n\n# Iterate over the first 3 records and decode them.\nfor tfrecord in dataset.take(3):\n  serialized_example = tfrecord.numpy()\n  example = tf.train.Example()\n  example.ParseFromString(serialized_example)\n  pp.pprint(example)\n\n\"\"\"\nExplanation: transformed_metadata サブディレクトリには、前処理されたデータのスキーマが含まれています。transform_fnサブディレクトリには、実際の前処理グラフが含まれています。metadataサブディレクトリには、元のデータのスキーマが含まれています。\nまた、最初の 3 つの変換された例も見てみます。\nEnd of explanation\n\"\"\"\n\n\n_taxi_trainer_module_file = 'taxi_trainer.py'\n\n%%writefile {_taxi_trainer_module_file}\n\nfrom typing import Dict, List, Text\n\nimport os\nimport glob\nfrom absl import logging\n\nimport datetime\nimport tensorflow as tf\nimport tensorflow_transform as tft\n\nfrom tfx import v1 as tfx\nfrom tfx_bsl.public import tfxio\nfrom tensorflow_transform import TFTransformOutput\n\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimport taxi_constants\nimport sys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  import importlib\n  importlib.reload(taxi_constants)\n\n_LABEL_KEY = taxi_constants.LABEL_KEY\n\n_BATCH_SIZE = 40\n\n\ndef _input_fn(file_pattern: List[Text],\n              data_accessor: tfx.components.DataAccessor,\n              tf_transform_output: tft.TFTransformOutput,\n              batch_size: int = 200) -> tf.data.Dataset:\n  \"\"\"Generates features and label for tuning/training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    tf_transform_output: A TFTransformOutput.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      tf_transform_output.transformed_metadata.schema)\n\ndef _get_tf_examples_serving_signature(model, tf_transform_output):\n  \"\"\"Returns a serving signature that accepts `tensorflow.Example`.\"\"\"\n\n  # We need to track the layers in the model in order to save it.\n  # TODO(b/162357359): Revise once the bug is resolved.\n  model.tft_layer_inference = tf_transform_output.transform_features_layer()\n\n  @tf.function(input_signature=[\n      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n  ])\n  def serve_tf_examples_fn(serialized_tf_example):\n    \"\"\"Returns the output to be used in the serving signature.\"\"\"\n    raw_feature_spec = tf_transform_output.raw_feature_spec()\n    # Remove label feature since these will not be present at serving time.\n    raw_feature_spec.pop(_LABEL_KEY)\n    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n    transformed_features = model.tft_layer_inference(raw_features)\n    logging.info('serve_transformed_features = %s', transformed_features)\n\n    outputs = model(transformed_features)\n    # TODO(b/154085620): Convert the predicted labels from the model using a\n    # reverse-lookup (opposite of transform.py).\n    return {'outputs': outputs}\n\n  return serve_tf_examples_fn\n\n\ndef _get_transform_features_signature(model, tf_transform_output):\n  \"\"\"Returns a serving signature that applies tf.Transform to features.\"\"\"\n\n  # We need to track the layers in the model in order to save it.\n  # TODO(b/162357359): Revise once the bug is resolved.\n  model.tft_layer_eval = tf_transform_output.transform_features_layer()\n\n  @tf.function(input_signature=[\n      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n  ])\n  def transform_features_fn(serialized_tf_example):\n    \"\"\"Returns the transformed_features to be fed as input to evaluator.\"\"\"\n    raw_feature_spec = tf_transform_output.raw_feature_spec()\n    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n    transformed_features = model.tft_layer_eval(raw_features)\n    logging.info('eval_transformed_features = %s', transformed_features)\n    return transformed_features\n\n  return transform_features_fn\n\n\ndef export_serving_model(tf_transform_output, model, output_dir):\n  \"\"\"Exports a keras model for serving.\n  Args:\n    tf_transform_output: Wrapper around output of tf.Transform.\n    model: A keras model to export for serving.\n    output_dir: A directory where the model will be exported to.\n  \"\"\"\n  # The layer has to be saved to the model for keras tracking purpases.\n  model.tft_layer = tf_transform_output.transform_features_layer()\n\n  signatures = {\n      'serving_default':\n          _get_tf_examples_serving_signature(model, tf_transform_output),\n      'transform_features':\n          _get_transform_features_signature(model, tf_transform_output),\n  }\n\n  model.save(output_dir, save_format='tf', signatures=signatures)\n\n\ndef _build_keras_model(tf_transform_output: TFTransformOutput\n                       ) -> tf.keras.Model:\n  \"\"\"Creates a DNN Keras model for classifying taxi data.\n\n  Args:\n    tf_transform_output: [TFTransformOutput], the outputs from Transform\n\n  Returns:\n    A keras Model.\n  \"\"\"\n  feature_spec = tf_transform_output.transformed_feature_spec().copy()\n  feature_spec.pop(_LABEL_KEY)\n\n  inputs = {}\n  for key, spec in feature_spec.items():\n    if isinstance(spec, tf.io.VarLenFeature):\n      inputs[key] = tf.keras.layers.Input(\n          shape=[None], name=key, dtype=spec.dtype, sparse=True)\n    elif isinstance(spec, tf.io.FixedLenFeature):\n      # TODO(b/208879020): Move into schema such that spec.shape is [1] and not\n      # [] for scalars.\n      inputs[key] = tf.keras.layers.Input(\n          shape=spec.shape or [1], name=key, dtype=spec.dtype)\n    else:\n      raise ValueError('Spec type is not supported: ', key, spec)\n  \n  output = tf.keras.layers.Concatenate()(tf.nest.flatten(inputs))\n  output = tf.keras.layers.Dense(100, activation='relu')(output)\n  output = tf.keras.layers.Dense(70, activation='relu')(output)\n  output = tf.keras.layers.Dense(50, activation='relu')(output)\n  output = tf.keras.layers.Dense(20, activation='relu')(output)\n  output = tf.keras.layers.Dense(1)(output)\n  return tf.keras.Model(inputs=inputs, outputs=output)\n\n\n# TFX Trainer will call this function.\ndef run_fn(fn_args: tfx.components.FnArgs):\n  \"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n\n  train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor, \n                            tf_transform_output, _BATCH_SIZE)\n  eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor, \n                           tf_transform_output, _BATCH_SIZE)\n\n  model = _build_keras_model(tf_transform_output)\n\n  model.compile(\n      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n      metrics=[tf.keras.metrics.BinaryAccuracy()])\n\n  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n      log_dir=fn_args.model_run_dir, update_freq='batch')\n\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps,\n      callbacks=[tensorboard_callback])\n\n  # Export the model.\n  export_serving_model(tf_transform_output, model, fn_args.serving_model_dir)\n\n\"\"\"\nExplanation: Transformコンポーネントがデータを特徴量に変換したら、次にモデルをトレーニングします。\nトレーナー\nTrainerコンポーネントは、TensorFlow で定義したモデルをトレーニングします。デフォルトでは、Trainer は Estimator API をサポートします。Keras API を使用するには、トレーナーのコンストラクターでcustom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor)をセットアップして Generic Trainer を指定する必要があります。\nTrainer は、SchemaGenからのスキーマ、Transformからの変換されたデータとグラフ、トレーニング パラメータ、およびユーザー定義されたモデル コードを含むモジュールを入力として受け取ります。\n以下のユーザー定義モデル コードの例を見てみましょう（TensorFlow Keras API の概要については、チュートリアルを参照してください）。\nEnd of explanation\n\"\"\"\n\n\ntrainer = tfx.components.Trainer(\n    module_file=os.path.abspath(_taxi_trainer_module_file),\n    examples=transform.outputs['transformed_examples'],\n    transform_graph=transform.outputs['transform_graph'],\n    schema=schema_gen.outputs['schema'],\n    train_args=tfx.proto.TrainArgs(num_steps=10000),\n    eval_args=tfx.proto.EvalArgs(num_steps=5000))\ncontext.run(trainer, enable_cache=True)\n\n\"\"\"\nExplanation: 次に、このモデル コードをTrainerコンポーネントに渡し、それを実行してモデルをトレーニングします。\nEnd of explanation\n\"\"\"\n\n\nmodel_artifact_dir = trainer.outputs['model'].get()[0].uri\npp.pprint(os.listdir(model_artifact_dir))\nmodel_dir = os.path.join(model_artifact_dir, 'Format-Serving')\npp.pprint(os.listdir(model_dir))\n\n\"\"\"\nExplanation: TensorBoard でトレーニングを分析する\nトレーナーのアーティファクトを見てみましょう。これはモデルのサブディレクトリを含むディレクトリを指しています。\nEnd of explanation\n\"\"\"\n\n\nmodel_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri\n\n%load_ext tensorboard\n%tensorboard --logdir {model_run_artifact_dir}\n\n\"\"\"\nExplanation: オプションで、TensorBoard を Trainer に接続して、モデルの学習曲線を分析できます。\nEnd of explanation\n\"\"\"\n\n\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimport taxi_constants\nimport sys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  import importlib\n  importlib.reload(taxi_constants)\n\neval_config = tfma.EvalConfig(\n    model_specs=[\n        # This assumes a serving model with signature 'serving_default'. If\n        # using estimator based EvalSavedModel, add signature_name: 'eval' and\n        # remove the label_key.\n        tfma.ModelSpec(\n            signature_name='serving_default',\n            label_key=taxi_constants.LABEL_KEY,\n            preprocessing_function_names=['transform_features'],\n            )\n        ],\n    metrics_specs=[\n        tfma.MetricsSpec(\n            # The metrics added here are in addition to those saved with the\n            # model (assuming either a keras model or EvalSavedModel is used).\n            # Any metrics added into the saved model (for example using\n            # model.compile(..., metrics=[...]), etc) will be computed\n            # automatically.\n            # To add validation thresholds for metrics saved with the model,\n            # add them keyed by metric name to the thresholds map.\n            metrics=[\n                tfma.MetricConfig(class_name='ExampleCount'),\n                tfma.MetricConfig(class_name='BinaryAccuracy',\n                  threshold=tfma.MetricThreshold(\n                      value_threshold=tfma.GenericValueThreshold(\n                          lower_bound={'value': 0.5}),\n                      # Change threshold will be ignored if there is no\n                      # baseline model resolved from MLMD (first run).\n                      change_threshold=tfma.GenericChangeThreshold(\n                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n                          absolute={'value': -1e-10})))\n            ]\n        )\n    ],\n    slicing_specs=[\n        # An empty slice spec means the overall slice, i.e. the whole dataset.\n        tfma.SlicingSpec(),\n        # Data can be sliced along a feature column. In this case, data is\n        # sliced along feature column trip_start_hour.\n        tfma.SlicingSpec(\n            feature_keys=['trip_start_hour'])\n    ])\n\n\"\"\"\nExplanation: Evaluator\nEvaluator コンポーネントは、評価セットに対してモデル パフォーマンス指標を計算します。TensorFlow Model Analysisライブラリを使用します。Evaluatorは、オプションで、新しくトレーニングされたモデルが以前のモデルよりも優れていることを検証できます。これは、モデルを毎日自動的にトレーニングおよび検証する実稼働環境のパイプライン設定で役立ちます。このノートブックでは 1 つのモデルのみをトレーニングするため、Evaluatorはモデルに自動的に「good」というラベルを付けます。\nEvaluatorは、ExampleGenからのデータ、Trainerからのトレーニング済みモデル、およびスライス構成を入力として受け取ります。スライス構成により、特徴値に関する指標をスライスすることができます (たとえば、午前 8 時から午後 8 時までのタクシー乗車でモデルがどのように動作するかなど)。 この構成の例は、以下を参照してください。\nEnd of explanation\n\"\"\"\n\n\n# Use TFMA to compute a evaluation statistics over features of a model and\n# validate them against a baseline.\n\n# The model resolver is only required if performing model validation in addition\n# to evaluation. In this case we validate against the latest blessed model. If\n# no model has been blessed before (as in this case) the evaluator will make our\n# candidate the first blessed model.\nmodel_resolver = tfx.dsl.Resolver(\n      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n      model_blessing=tfx.dsl.Channel(\n          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n              'latest_blessed_model_resolver')\ncontext.run(model_resolver, enable_cache=True)\n\nevaluator = tfx.components.Evaluator(\n    examples=example_gen.outputs['examples'],\n    model=trainer.outputs['model'],\n    baseline_model=model_resolver.outputs['model'],\n    eval_config=eval_config)\ncontext.run(evaluator, enable_cache=True)\n\n\"\"\"\nExplanation: 次に、この構成を Evaluatorに渡して実行します。\nEnd of explanation\n\"\"\"\n\n\nevaluator.outputs\n\n\"\"\"\nExplanation: Evaluator の出力アーティファクトを調べてみましょう。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(evaluator.outputs['evaluation'])\n\n\"\"\"\nExplanation: evaluation出力を使用すると、評価セット全体のグローバル指標のデフォルトの視覚化を表示できます。\nEnd of explanation\n\"\"\"\n\n\nimport tensorflow_model_analysis as tfma\n\n# Get the TFMA output result path and load the result.\nPATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\ntfma_result = tfma.load_eval_result(PATH_TO_RESULT)\n\n# Show data sliced along feature column trip_start_hour.\ntfma.view.render_slicing_metrics(\n    tfma_result, slicing_column='trip_start_hour')\n\n\"\"\"\nExplanation: スライスされた評価メトリクスの視覚化を表示するには、TensorFlow Model Analysis ライブラリを直接呼び出します。\nEnd of explanation\n\"\"\"\n\n\nblessing_uri = evaluator.outputs['blessing'].get()[0].uri\n!ls -l {blessing_uri}\n\n\"\"\"\nExplanation: この視覚化は同じ指標を示していますが、評価セット全体ではなく、trip_start_hourのすべての特徴値で計算されています。\nTensorFlow モデル分析は、公平性インジケーターやモデル パフォーマンスの時系列のプロットなど、他の多くの視覚化をサポートしています。 詳細については、チュートリアルを参照してください。\n構成にしきい値を追加したため、検証出力も利用できます。{code 0}blessing{/code 0} アーティファクトの存在は、モデルが検証に合格したことを示しています。これは実行される最初の検証であるため、候補は自動的に bless されます。\nEnd of explanation\n\"\"\"\n\n\nPATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\nprint(tfma.load_validation_result(PATH_TO_RESULT))\n\n\"\"\"\nExplanation: 検証結果レコードを読み込み、成功を確認することもできます。\nEnd of explanation\n\"\"\"\n\n\npusher = tfx.components.Pusher(\n    model=trainer.outputs['model'],\n    model_blessing=evaluator.outputs['blessing'],\n    push_destination=tfx.proto.PushDestination(\n        filesystem=tfx.proto.PushDestination.Filesystem(\n            base_directory=_serving_model_dir)))\ncontext.run(pusher, enable_cache=True)\n\n\"\"\"\nExplanation: Pusher\nPusher コンポーネントは通常、TFX パイプラインの最後にあります。このコンポーネントはモデルが検証に合格したかどうかをチェックし、合格した場合はモデルを _serving_model_dirにエクスポートします。\nEnd of explanation\n\"\"\"\n\n\npusher.outputs\n\n\"\"\"\nExplanation: 次にPusherの出力アーティファクトを調べてみましょう。\nEnd of explanation\n\"\"\"\n\n\npush_uri = pusher.outputs['pushed_model'].get()[0].uri\nmodel = tf.saved_model.load(push_uri)\n\nfor item in model.signatures.items():\n  pp.pprint(item)\n\n\"\"\"\nExplanation: 特に、Pusher はモデルを次のような SavedModel 形式でエクスポートします。\nEnd of explanation\n\"\"\"\n\n_, _, data = twpca.datasets.jittered_neuron()\nmodel = TWPCA(data, n_components=1, warpinit='identity')\n\nnp.all(np.isclose(model.params['warp'], np.arange(model.shared_length), atol=1e-5, rtol=2))\n\nnp.nanmax(np.abs(model.transform() - data)) < 1e-5\n\n\"\"\"\nExplanation: check identity warp does not change data appreciably\nEnd of explanation\n\"\"\"\n\n\nmodel = TWPCA(data, n_components=1, warpinit='shift')\n\nplt.imshow(np.squeeze(model.transform()))\n\n\"\"\"\nExplanation: check that shift initialization for warp solves the simple toy problem\nEnd of explanation\n\"\"\"\n\nfrom __future__ import print_function, division, unicode_literals\n\nimport oddt\nfrom oddt.datasets import dude\nprint(oddt.__version__)\n\n\"\"\"\nExplanation: <h1>DUD-E: A Database of Useful Decoys: Enhanced</h1>\nEnd of explanation\n\"\"\"\n\n\n%%bash\nmkdir -p ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/ampc/ampc.tar.gz | tar xz -C ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/cxcr4/cxcr4.tar.gz | tar xz -C ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/pur2/pur2.tar.gz | tar xz -C ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/pygm/pygm.tar.gz | tar xz -C ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/sahh/sahh.tar.gz | tar xz -C ./DUD-E_targets/\n\ndirectory = './DUD-E_targets'\n\n\"\"\"\nExplanation: We'd like to read files from DUD-E.<br/>\nYou can download different targets and different numbers of targets",
  "32k": "import numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\"\"\"\nExplanation: Simple MNIST convnet\nAuthor: fchollet<br>\nDate created: 2015/06/19<br>\nLast modified: 2020/04/21<br>\nDescription: A simple convnet that achieves ~99% test accuracy on MNIST.\nSetup\nEnd of explanation\n\"\"\"\n\n\n# Model / data parameters\nnum_classes = 10\ninput_shape = (28, 28, 1)\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") / 255\nx_test = x_test.astype(\"float32\") / 255\n# Make sure images have shape (28, 28, 1)\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\nprint(\"x_train shape:\", x_train.shape)\nprint(x_train.shape[0], \"train samples\")\nprint(x_test.shape[0], \"test samples\")\n\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\"\"\"\nExplanation: Prepare the data\nEnd of explanation\n\"\"\"\n\n\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Flatten(),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation=\"softmax\"),\n    ]\n)\n\nmodel.summary()\n\n\"\"\"\nExplanation: Build the model\nEnd of explanation\n\"\"\"\n\n\nbatch_size = 128\nepochs = 15\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n\n\"\"\"\nExplanation: Train the model\nEnd of explanation\n\"\"\"\n\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])\n\n\"\"\"\nExplanation: Evaluate the trained model\nEnd of explanation\n\"\"\"\n\n#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nExplanation: Copyright 2021 The TensorFlow Authors.\nEnd of explanation\n\"\"\"\n\n\nimport sys\nif 'google.colab' in sys.modules:\n  !pip install --upgrade pip\n\n\"\"\"\nExplanation: TFX Keras コンポーネントのチュートリアル\nTensorFlow Extended (TFX) の各コンポーネントの紹介\n注：この例は、Jupyter スタイルのノートブックで今すぐ実行できます。セットアップは必要ありません。「Google Colab で実行」をクリックするだけです\n<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n<td><a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/tfx/components_keras\"> <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a></td>\n<td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a></td>\n<td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n<td><a target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n</table></div>\n\nこの Colab ベースのチュートリアルでは、TensorFlow Extended (TFX) のそれぞれの組み込みコンポーネントをインタラクティブに説明します。\nここではデータの取り込みからモデルのプッシュ、サービングまで、エンド ツー エンドの機械学習パイプラインのすべてのステップを見ていきます。\n完了したら、このノートブックのコンテンツを TFX パイプライン ソース コードとして自動的にエクスポートできます。これは、Apache Airflow および Apache Beam とオーケストレーションできます。\n注意: このノートブックは、TFX パイプラインでのネイティブ Keras モデルの使用を示しています。TFX は TensorFlow 2 バージョンの Keras のみをサポートします。\n背景情報\nこのノートブックは、Jupyter/Colab 環境で TFX を使用する方法を示しています。 ここでは、インタラクティブなノートブックでシカゴのタクシーの例を見ていきます。\nTFX パイプラインの構造に慣れるのには、インタラクティブなノートブックで作業するのが便利です。独自のパイプラインを軽量の開発環境として開発する場合にも役立ちますが、インタラクティブ ノートブックのオーケストレーションとメタデータ アーティファクトへのアクセス方法には違いがあるので注意してください。\nオーケストレーション\nTFX の実稼働デプロイメントでは、Apache Airflow、Kubeflow Pipelines、Apache Beam などのオーケストレーターを使用して、TFX コンポーネントの事前定義済みパイプライン グラフをオーケストレーションします。インタラクティブなノートブックでは、ノートブック自体がオーケストレーターであり、ノートブック セルを実行するときにそれぞれの TFX コンポーネントを実行します。\nメタデータ\nTFX の実稼働デプロイメントでは、ML Metadata（MLMD）API を介してメタデータにアクセスします。MLMD は、メタデータ プロパティを MySQL や SQLite などのデータベースに格納し、メタデータ ペイロードをファイル システムなどの永続ストアに保存します。インタラクティブなノートブックでは、プロパティとペイロードの両方が、Jupyter ノートブックまたは Colab サーバーの /tmp ディレクトリにあるエフェメラル SQLite データベースに保存されます。\nセットアップ\nまず、必要なパッケージをインストールしてインポートし、パスを設定して、データをダウンロードします。\nPip のアップグレード\nローカルで実行する場合にシステム Pipをアップグレードしないようにするには、Colab で実行していることを確認してください。もちろん、ローカルシステムは個別にアップグレードできます。\nEnd of explanation\n\"\"\"\n\n\n!pip install -U tfx\n\n\"\"\"\nExplanation: TFX をインストールする\n注：Google Colab では、パッケージが更新されるため、このセルを初めて実行するときに、ランタイムを再起動する必要があります（[ランタイム]&gt; [ランタイムの再起動...]）。\nEnd of explanation\n\"\"\"\n\n\nimport os\nimport pprint\nimport tempfile\nimport urllib\n\nimport absl\nimport tensorflow as tf\nimport tensorflow_model_analysis as tfma\ntf.get_logger().propagate = False\npp = pprint.PrettyPrinter()\n\nfrom tfx import v1 as tfx\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n\n%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip\n\n\"\"\"\nExplanation: ランタイムを再起動しましたか？\nGoogle Colab を使用している場合は、上記のセルを初めて実行するときにランタイムを再起動する必要があります（[ランタイム]&gt; [ランタイムの再起動...]）。 これは、Colab がパッケージを読み込むために必要ですです。\nパッケージをインポートする\n標準の TFX コンポーネント クラスを含む必要なパッケージをインポートします。\nEnd of explanation\n\"\"\"\n\n\nprint('TensorFlow version: {}'.format(tf.__version__))\nprint('TFX version: {}'.format(tfx.__version__))\n\n\"\"\"\nExplanation: ライブラリのバージョンを確認します。\nEnd of explanation\n\"\"\"\n\n\n# This is the root directory for your TFX pip package installation.\n_tfx_root = tfx.__path__[0]\n\n# This is the directory containing the TFX Chicago Taxi Pipeline example.\n_taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')\n\n# This is the path where your model will be pushed for serving.\n_serving_model_dir = os.path.join(\n    tempfile.mkdtemp(), 'serving_model/taxi_simple')\n\n# Set up logging.\nabsl.logging.set_verbosity(absl.logging.INFO)\n\n\"\"\"\nExplanation: パイプライン パスを設定\nEnd of explanation\n\"\"\"\n\n\n_data_root = tempfile.mkdtemp(prefix='tfx-data')\nDATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv'\n_data_filepath = os.path.join(_data_root, \"data.csv\")\nurllib.request.urlretrieve(DATA_PATH, _data_filepath)\n\n\"\"\"\nExplanation: サンプルデータのダウンロード\nTFX パイプラインで使用するサンプル データセットをダウンロードします。\n使用しているデータセットは、シカゴ市がリリースした タクシートリップデータセットです。 このデータセットの列は次のとおりです。\n<table>\n<tr>\n<td>pickup_community_area</td>\n<td>fare</td>\n<td>trip_start_month</td>\n</tr>\n<tr>\n<td>trip_start_hour</td>\n<td>trip_start_day</td>\n<td>trip_start_timestamp</td>\n</tr>\n<tr>\n<td>pickup_latitude</td>\n<td>pickup_longitude</td>\n<td>dropoff_latitude</td>\n</tr>\n<tr>\n<td>dropoff_longitude</td>\n<td>trip_miles</td>\n<td>pickup_census_tract</td>\n</tr>\n<tr>\n<td>dropoff_census_tract</td>\n<td>payment_type</td>\n<td>company</td>\n</tr>\n<tr>\n<td>trip_seconds</td>\n<td>dropoff_community_area</td>\n<td>tips</td>\n</tr>\n</table>\n\nこのデータセットを使用して、タクシー乗車のtipsを予測するモデルを構築します。\nEnd of explanation\n\"\"\"\n\n\n!head {_data_filepath}\n\n\"\"\"\nExplanation: CSV ファイルを見てみましょう。\nEnd of explanation\n\"\"\"\n\n\n# Here, we create an InteractiveContext using default parameters. This will\n# use a temporary directory with an ephemeral ML Metadata database instance.\n# To use your own pipeline root or database, the optional properties\n# `pipeline_root` and `metadata_connection_config` may be passed to\n# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n# notebook.\ncontext = InteractiveContext()\n\n\"\"\"\nExplanation: 注：このWeb サイトは、シカゴ市の公式 Web サイト www.cityofchicago.org で公開されたデータを変更して使用するアプリケーションを提供します。シカゴ市は、この Web サイトで提供されるデータの内容、正確性、適時性、または完全性について一切の表明を行いません。この Web サイトで提供されるデータは、いつでも変更される可能性があります。かかる Web サイトで提供されるデータはユーザーの自己責任で利用されるものとします。\nInteractiveContext を作成する\n最後に、このノートブックで TFX コンポーネントをインタラクティブに実行できるようにする InteractiveContext を作成します。\nEnd of explanation\n\"\"\"\n\n\nexample_gen = tfx.components.CsvExampleGen(input_base=_data_root)\ncontext.run(example_gen, enable_cache=True)\n\n\"\"\"\nExplanation: TFX コンポーネントをインタラクティブに実行する\n次のセルでは、TFX コンポーネントを 1 つずつ作成し、それぞれを実行して、出力アーティファクトを視覚化します。\nExampleGen\nExampleGen コンポーネントは通常、TFX パイプラインの先頭にあり、以下を実行します。\n\nデータをトレーニング セットと評価セットに分割します (デフォルトでは、2/3 トレーニング + 1/3 評価)。\nデータを tf.Example 形式に変換します。 (詳細はこちら)\n他のコンポーネントがアクセスできるように、データを _tfx_root ディレクトリにコピーします。\n\nExampleGen は、データソースへのパスを入力として受け取ります。 ここでは、これはダウンロードした CSV を含む _data_root パスです。\n注意: このノートブックでは、コンポーネントを 1 つずつインスタンス化し、InteractiveContext.run() で実行しますが、実稼働環境では、すべてのコンポーネントを事前に Pipelineで指定して、オーケストレーターに渡します（TFX パイプライン ガイドの構築を参照してください）。\nキャッシュを有効にする\nノートブックで InteractiveContext を使用してパイプラインを作成している場合、個別のコンポーネントが出力をキャッシュするタイミングを制御することができます。コンポーネントが前に生成した出力アーティファクトを再利用する場合は、enable_cache を True に設定します。コードを変更するなどにより、コンポーネントの出力アーティファクトを再計算する場合は、enable_cache を False に設定します。\nEnd of explanation\n\"\"\"\n\n\nartifact = example_gen.outputs['examples'].get()[0]\nprint(artifact.split_names, artifact.uri)\n\n\"\"\"\nExplanation: ExampleGenの出力アーティファクトを調べてみましょう。このコンポーネントは、トレーニングサンプルと評価サンプルの 2 つのアーティファクトを生成します。\nEnd of explanation\n\"\"\"\n\n\n# Get the URI of the output artifact representing the training examples, which is a directory\ntrain_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')\n\n# Get the list of files in this directory (all compressed TFRecord files)\ntfrecord_filenames = [os.path.join(train_uri, name)\n                      for name in os.listdir(train_uri)]\n\n# Create a `TFRecordDataset` to read these files\ndataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n\n# Iterate over the first 3 records and decode them.\nfor tfrecord in dataset.take(3):\n  serialized_example = tfrecord.numpy()\n  example = tf.train.Example()\n  example.ParseFromString(serialized_example)\n  pp.pprint(example)\n\n\"\"\"\nExplanation: また、最初の 3 つのトレーニングサンプルも見てみます。\nEnd of explanation\n\"\"\"\n\n\nstatistics_gen = tfx.components.StatisticsGen(\n    examples=example_gen.outputs['examples'])\ncontext.run(statistics_gen, enable_cache=True)\n\n\"\"\"\nExplanation: ExampleGenがデータの取り込みを完了したので、次のステップ、データ分析に進みます。\nStatisticsGen\nStatisticsGenコンポーネントは、データ分析用のデータセットの統計を計算し、ダウンストリームのコンポーネントで使用します。これは、TensorFlow Data Validation ライブラリを使用します。\nStatisticsGenコンポーネントは、データ分析用のデータセットの統計を計算し、ダウンストリーム コンポーネントで使用します。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(statistics_gen.outputs['statistics'])\n\n\"\"\"\nExplanation: StatisticsGen の実行が完了すると、出力された統計を視覚化できます。 色々なプロットを試してみてください！\nEnd of explanation\n\"\"\"\n\n\nschema_gen = tfx.components.SchemaGen(\n    statistics=statistics_gen.outputs['statistics'],\n    infer_feature_shape=False)\ncontext.run(schema_gen, enable_cache=True)\n\n\"\"\"\nExplanation: SchemaGen\nSchemaGen コンポーネントは、データ統計に基づいてスキーマを生成します。（スキーマは、データセット内の特徴の予想される境界、タイプ、プロパティを定義します。）また、TensorFlow データ検証ライブラリも使用します。\n注意: 生成されたスキーマはベストエフォートのもので、データの基本的なプロパティだけを推論しようとします。確認し、必要に応じて修正する必要があります。\nSchemaGen は、StatisticsGen で生成した統計を入力として受け取り、デフォルトでトレーニング分割を参照します。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(schema_gen.outputs['schema'])\n\n\"\"\"\nExplanation: SchemaGen の実行が完了すると、生成されたスキーマをテーブルとして視覚化できます。\nEnd of explanation\n\"\"\"\n\n\nexample_validator = tfx.components.ExampleValidator(\n    statistics=statistics_gen.outputs['statistics'],\n    schema=schema_gen.outputs['schema'])\ncontext.run(example_validator, enable_cache=True)\n\n\"\"\"\nExplanation: データセットのそれぞれの特徴は、スキーマ テーブルのプロパティの横に行として表示されます。スキーマは、ドメインとして示される、カテゴリ特徴が取るすべての値もキャプチャします。\nスキーマの詳細については、SchemaGen のドキュメントをご覧ください。\nExampleValidator\nExampleValidator コンポーネントは、スキーマで定義された期待に基づいて、データの異常を検出します。また、TensorFlow Data Validation ライブラリも使用します。\nExampleValidator は、Statistics Gen{/code 1} からの統計と &lt;code data-md-type=\"codespan\"&gt;SchemaGen からのスキーマを入力として受け取ります。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(example_validator.outputs['anomalies'])\n\n\"\"\"\nExplanation: ExampleValidator の実行が完了すると、異常をテーブルとして視覚化できます。\nEnd of explanation\n\"\"\"\n\n\n_taxi_constants_module_file = 'taxi_constants.py'\n\n%%writefile {_taxi_constants_module_file}\n\nNUMERICAL_FEATURES = ['trip_miles', 'fare', 'trip_seconds']\n\nBUCKET_FEATURES = [\n    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n    'dropoff_longitude'\n]\n# Number of buckets used by tf.transform for encoding each feature.\nFEATURE_BUCKET_COUNT = 10\n\nCATEGORICAL_NUMERICAL_FEATURES = [\n    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n    'dropoff_community_area'\n]\n\nCATEGORICAL_STRING_FEATURES = [\n    'payment_type',\n    'company',\n]\n\n# Number of vocabulary terms used for encoding categorical features.\nVOCAB_SIZE = 1000\n\n# Count of out-of-vocab buckets in which unrecognized categorical are hashed.\nOOV_SIZE = 10\n\n# Keys\nLABEL_KEY = 'tips'\nFARE_KEY = 'fare'\n\ndef t_name(key):\n  \"\"\"\n  Rename the feature keys so that they don't clash with the raw keys when\n  running the Evaluator component.\n  Args:\n    key: The original feature key\n  Returns:\n    key with '_xf' appended\n  \"\"\"\n  return key + '_xf'\n\n\"\"\"\nExplanation: 異常テーブルでは、異常がないことがわかります。これは、分析した最初のデータセットで、スキーマはこれに合わせて調整されているため、異常がないことが予想されます。このスキーマを確認する必要があります。予期されないものは、データに異常があることを意味します。確認されたスキーマを使用して将来のデータを保護できます。ここで生成された異常は、モデルのパフォーマンスをデバッグし、データが時間の経過とともにどのように変化するかを理解し、データ エラーを特定するために使用できます。\n変換\nTransformコンポーネントは、トレーニングとサービングの両方で特徴量エンジニアリングを実行します。これは、 TensorFlow Transform ライブラリを使用します。\nTransformは、ExampleGenからのデータ、SchemaGenからのスキーマ、ユーザー定義の Transform コードを含むモジュールを入力として受け取ります。\n以下のユーザー定義の Transform コードの例を見てみましょう（TensorFlow Transform API の概要については、チュートリアルを参照してください）。まず、特徴量エンジニアリングのいくつかの定数を定義します。\n注意: %%writefile セル マジックは、セルの内容をディスク上の.pyファイルとして保存します。これにより、Transform コンポーネントはコードをモジュールとして読み込むことができます。\nEnd of explanation\n\"\"\"\n\n\n_taxi_transform_module_file = 'taxi_transform.py'\n\n%%writefile {_taxi_transform_module_file}\n\nimport tensorflow as tf\nimport tensorflow_transform as tft\n\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimport taxi_constants\nimport sys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  import importlib\n  importlib.reload(taxi_constants)\n\n_NUMERICAL_FEATURES = taxi_constants.NUMERICAL_FEATURES\n_BUCKET_FEATURES = taxi_constants.BUCKET_FEATURES\n_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\n_CATEGORICAL_NUMERICAL_FEATURES = taxi_constants.CATEGORICAL_NUMERICAL_FEATURES\n_CATEGORICAL_STRING_FEATURES = taxi_constants.CATEGORICAL_STRING_FEATURES\n_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\n_OOV_SIZE = taxi_constants.OOV_SIZE\n_FARE_KEY = taxi_constants.FARE_KEY\n_LABEL_KEY = taxi_constants.LABEL_KEY\n\n\ndef _make_one_hot(x, key):\n  \"\"\"Make a one-hot tensor to encode categorical features.\n  Args:\n    X: A dense tensor\n    key: A string key for the feature in the input\n  Returns:\n    A dense one-hot tensor as a float list\n  \"\"\"\n  integerized = tft.compute_and_apply_vocabulary(x,\n          top_k=_VOCAB_SIZE,\n          num_oov_buckets=_OOV_SIZE,\n          vocab_filename=key, name=key)\n  depth = (\n      tft.experimental.get_vocabulary_size_by_name(key) + _OOV_SIZE)\n  one_hot_encoded = tf.one_hot(\n      integerized,\n      depth=tf.cast(depth, tf.int32),\n      on_value=1.0,\n      off_value=0.0)\n  return tf.reshape(one_hot_encoded, [-1, depth])\n\n\ndef _fill_in_missing(x):\n  \"\"\"Replace missing values in a SparseTensor.\n  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n  Args:\n    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n      in the second dimension.\n  Returns:\n    A rank 1 tensor where missing values of `x` have been filled in.\n  \"\"\"\n  if not isinstance(x, tf.sparse.SparseTensor):\n    return x\n\n  default_value = '' if x.dtype == tf.string else 0\n  return tf.squeeze(\n      tf.sparse.to_dense(\n          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n          default_value),\n      axis=1)\n\n\ndef preprocessing_fn(inputs):\n  \"\"\"tf.transform's callback function for preprocessing inputs.\n  Args:\n    inputs: map from feature keys to raw not-yet-transformed features.\n  Returns:\n    Map from string feature key to transformed feature operations.\n  \"\"\"\n  outputs = {}\n  for key in _NUMERICAL_FEATURES:\n    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.\n    outputs[taxi_constants.t_name(key)] = tft.scale_to_z_score(\n        _fill_in_missing(inputs[key]), name=key)\n\n  for key in _BUCKET_FEATURES:\n    outputs[taxi_constants.t_name(key)] = tf.cast(tft.bucketize(\n            _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT, name=key),\n            dtype=tf.float32)\n\n  for key in _CATEGORICAL_STRING_FEATURES:\n    outputs[taxi_constants.t_name(key)] = _make_one_hot(_fill_in_missing(inputs[key]), key)\n\n  for key in _CATEGORICAL_NUMERICAL_FEATURES:\n    outputs[taxi_constants.t_name(key)] = _make_one_hot(tf.strings.strip(\n        tf.strings.as_string(_fill_in_missing(inputs[key]))), key)\n\n  # Was this passenger a big tipper?\n  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])\n  tips = _fill_in_missing(inputs[_LABEL_KEY])\n  outputs[_LABEL_KEY] = tf.where(\n      tf.math.is_nan(taxi_fare),\n      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\n      # Test if the tip was > 20% of the fare.\n      tf.cast(\n          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\n\n  return outputs\n\n\"\"\"\nExplanation: 次に、生データを入力として受け取り、モデルがトレーニングできる変換された特徴量を返す {code 0}preprocessing _fn を記述します。\nEnd of explanation\n\"\"\"\n\n\ntransform = tfx.components.Transform(\n    examples=example_gen.outputs['examples'],\n    schema=schema_gen.outputs['schema'],\n    module_file=os.path.abspath(_taxi_transform_module_file))\ncontext.run(transform, enable_cache=True)\n\n\"\"\"\nExplanation: 次に、この特徴量エンジニアリング コードを Transformコンポーネントに渡し、実行してデータを変換します。\nEnd of explanation\n\"\"\"\n\n\ntransform.outputs\n\n\"\"\"\nExplanation: Transformの出力アーティファクトを調べてみましょう。このコンポーネントは、2 種類の出力を生成します。\n\ntransform_graph は、前処理演算を実行できるグラフです (このグラフは、サービングモデルと評価モデルに含まれます)。\ntransformed_examplesは前処理されたトレーニングおよび評価データを表します。\nEnd of explanation\n\"\"\"\n\n\ntrain_uri = transform.outputs['transform_graph'].get()[0].uri\nos.listdir(train_uri)\n\n\"\"\"\nExplanation: transform_graph アーティファクトを見てみましょう。これは、3 つのサブディレクトリを含むディレクトリを指しています。\nEnd of explanation\n\"\"\"\n\n\n# Get the URI of the output artifact representing the transformed examples, which is a directory\ntrain_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n\n# Get the list of files in this directory (all compressed TFRecord files)\ntfrecord_filenames = [os.path.join(train_uri, name)\n                      for name in os.listdir(train_uri)]\n\n# Create a `TFRecordDataset` to read these files\ndataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n\n# Iterate over the first 3 records and decode them.\nfor tfrecord in dataset.take(3):\n  serialized_example = tfrecord.numpy()\n  example = tf.train.Example()\n  example.ParseFromString(serialized_example)\n  pp.pprint(example)\n\n\"\"\"\nExplanation: transformed_metadata サブディレクトリには、前処理されたデータのスキーマが含まれています。transform_fnサブディレクトリには、実際の前処理グラフが含まれています。metadataサブディレクトリには、元のデータのスキーマが含まれています。\nまた、最初の 3 つの変換された例も見てみます。\nEnd of explanation\n\"\"\"\n\n\n_taxi_trainer_module_file = 'taxi_trainer.py'\n\n%%writefile {_taxi_trainer_module_file}\n\nfrom typing import Dict, List, Text\n\nimport os\nimport glob\nfrom absl import logging\n\nimport datetime\nimport tensorflow as tf\nimport tensorflow_transform as tft\n\nfrom tfx import v1 as tfx\nfrom tfx_bsl.public import tfxio\nfrom tensorflow_transform import TFTransformOutput\n\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimport taxi_constants\nimport sys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  import importlib\n  importlib.reload(taxi_constants)\n\n_LABEL_KEY = taxi_constants.LABEL_KEY\n\n_BATCH_SIZE = 40\n\n\ndef _input_fn(file_pattern: List[Text],\n              data_accessor: tfx.components.DataAccessor,\n              tf_transform_output: tft.TFTransformOutput,\n              batch_size: int = 200) -> tf.data.Dataset:\n  \"\"\"Generates features and label for tuning/training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    tf_transform_output: A TFTransformOutput.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      tf_transform_output.transformed_metadata.schema)\n\ndef _get_tf_examples_serving_signature(model, tf_transform_output):\n  \"\"\"Returns a serving signature that accepts `tensorflow.Example`.\"\"\"\n\n  # We need to track the layers in the model in order to save it.\n  # TODO(b/162357359): Revise once the bug is resolved.\n  model.tft_layer_inference = tf_transform_output.transform_features_layer()\n\n  @tf.function(input_signature=[\n      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n  ])\n  def serve_tf_examples_fn(serialized_tf_example):\n    \"\"\"Returns the output to be used in the serving signature.\"\"\"\n    raw_feature_spec = tf_transform_output.raw_feature_spec()\n    # Remove label feature since these will not be present at serving time.\n    raw_feature_spec.pop(_LABEL_KEY)\n    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n    transformed_features = model.tft_layer_inference(raw_features)\n    logging.info('serve_transformed_features = %s', transformed_features)\n\n    outputs = model(transformed_features)\n    # TODO(b/154085620): Convert the predicted labels from the model using a\n    # reverse-lookup (opposite of transform.py).\n    return {'outputs': outputs}\n\n  return serve_tf_examples_fn\n\n\ndef _get_transform_features_signature(model, tf_transform_output):\n  \"\"\"Returns a serving signature that applies tf.Transform to features.\"\"\"\n\n  # We need to track the layers in the model in order to save it.\n  # TODO(b/162357359): Revise once the bug is resolved.\n  model.tft_layer_eval = tf_transform_output.transform_features_layer()\n\n  @tf.function(input_signature=[\n      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n  ])\n  def transform_features_fn(serialized_tf_example):\n    \"\"\"Returns the transformed_features to be fed as input to evaluator.\"\"\"\n    raw_feature_spec = tf_transform_output.raw_feature_spec()\n    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n    transformed_features = model.tft_layer_eval(raw_features)\n    logging.info('eval_transformed_features = %s', transformed_features)\n    return transformed_features\n\n  return transform_features_fn\n\n\ndef export_serving_model(tf_transform_output, model, output_dir):\n  \"\"\"Exports a keras model for serving.\n  Args:\n    tf_transform_output: Wrapper around output of tf.Transform.\n    model: A keras model to export for serving.\n    output_dir: A directory where the model will be exported to.\n  \"\"\"\n  # The layer has to be saved to the model for keras tracking purpases.\n  model.tft_layer = tf_transform_output.transform_features_layer()\n\n  signatures = {\n      'serving_default':\n          _get_tf_examples_serving_signature(model, tf_transform_output),\n      'transform_features':\n          _get_transform_features_signature(model, tf_transform_output),\n  }\n\n  model.save(output_dir, save_format='tf', signatures=signatures)\n\n\ndef _build_keras_model(tf_transform_output: TFTransformOutput\n                       ) -> tf.keras.Model:\n  \"\"\"Creates a DNN Keras model for classifying taxi data.\n\n  Args:\n    tf_transform_output: [TFTransformOutput], the outputs from Transform\n\n  Returns:\n    A keras Model.\n  \"\"\"\n  feature_spec = tf_transform_output.transformed_feature_spec().copy()\n  feature_spec.pop(_LABEL_KEY)\n\n  inputs = {}\n  for key, spec in feature_spec.items():\n    if isinstance(spec, tf.io.VarLenFeature):\n      inputs[key] = tf.keras.layers.Input(\n          shape=[None], name=key, dtype=spec.dtype, sparse=True)\n    elif isinstance(spec, tf.io.FixedLenFeature):\n      # TODO(b/208879020): Move into schema such that spec.shape is [1] and not\n      # [] for scalars.\n      inputs[key] = tf.keras.layers.Input(\n          shape=spec.shape or [1], name=key, dtype=spec.dtype)\n    else:\n      raise ValueError('Spec type is not supported: ', key, spec)\n  \n  output = tf.keras.layers.Concatenate()(tf.nest.flatten(inputs))\n  output = tf.keras.layers.Dense(100, activation='relu')(output)\n  output = tf.keras.layers.Dense(70, activation='relu')(output)\n  output = tf.keras.layers.Dense(50, activation='relu')(output)\n  output = tf.keras.layers.Dense(20, activation='relu')(output)\n  output = tf.keras.layers.Dense(1)(output)\n  return tf.keras.Model(inputs=inputs, outputs=output)\n\n\n# TFX Trainer will call this function.\ndef run_fn(fn_args: tfx.components.FnArgs):\n  \"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n\n  train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor, \n                            tf_transform_output, _BATCH_SIZE)\n  eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor, \n                           tf_transform_output, _BATCH_SIZE)\n\n  model = _build_keras_model(tf_transform_output)\n\n  model.compile(\n      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n      metrics=[tf.keras.metrics.BinaryAccuracy()])\n\n  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n      log_dir=fn_args.model_run_dir, update_freq='batch')\n\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps,\n      callbacks=[tensorboard_callback])\n\n  # Export the model.\n  export_serving_model(tf_transform_output, model, fn_args.serving_model_dir)\n\n\"\"\"\nExplanation: Transformコンポーネントがデータを特徴量に変換したら、次にモデルをトレーニングします。\nトレーナー\nTrainerコンポーネントは、TensorFlow で定義したモデルをトレーニングします。デフォルトでは、Trainer は Estimator API をサポートします。Keras API を使用するには、トレーナーのコンストラクターでcustom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor)をセットアップして Generic Trainer を指定する必要があります。\nTrainer は、SchemaGenからのスキーマ、Transformからの変換されたデータとグラフ、トレーニング パラメータ、およびユーザー定義されたモデル コードを含むモジュールを入力として受け取ります。\n以下のユーザー定義モデル コードの例を見てみましょう（TensorFlow Keras API の概要については、チュートリアルを参照してください）。\nEnd of explanation\n\"\"\"\n\n\ntrainer = tfx.components.Trainer(\n    module_file=os.path.abspath(_taxi_trainer_module_file),\n    examples=transform.outputs['transformed_examples'],\n    transform_graph=transform.outputs['transform_graph'],\n    schema=schema_gen.outputs['schema'],\n    train_args=tfx.proto.TrainArgs(num_steps=10000),\n    eval_args=tfx.proto.EvalArgs(num_steps=5000))\ncontext.run(trainer, enable_cache=True)\n\n\"\"\"\nExplanation: 次に、このモデル コードをTrainerコンポーネントに渡し、それを実行してモデルをトレーニングします。\nEnd of explanation\n\"\"\"\n\n\nmodel_artifact_dir = trainer.outputs['model'].get()[0].uri\npp.pprint(os.listdir(model_artifact_dir))\nmodel_dir = os.path.join(model_artifact_dir, 'Format-Serving')\npp.pprint(os.listdir(model_dir))\n\n\"\"\"\nExplanation: TensorBoard でトレーニングを分析する\nトレーナーのアーティファクトを見てみましょう。これはモデルのサブディレクトリを含むディレクトリを指しています。\nEnd of explanation\n\"\"\"\n\n\nmodel_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri\n\n%load_ext tensorboard\n%tensorboard --logdir {model_run_artifact_dir}\n\n\"\"\"\nExplanation: オプションで、TensorBoard を Trainer に接続して、モデルの学習曲線を分析できます。\nEnd of explanation\n\"\"\"\n\n\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimport taxi_constants\nimport sys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  import importlib\n  importlib.reload(taxi_constants)\n\neval_config = tfma.EvalConfig(\n    model_specs=[\n        # This assumes a serving model with signature 'serving_default'. If\n        # using estimator based EvalSavedModel, add signature_name: 'eval' and\n        # remove the label_key.\n        tfma.ModelSpec(\n            signature_name='serving_default',\n            label_key=taxi_constants.LABEL_KEY,\n            preprocessing_function_names=['transform_features'],\n            )\n        ],\n    metrics_specs=[\n        tfma.MetricsSpec(\n            # The metrics added here are in addition to those saved with the\n            # model (assuming either a keras model or EvalSavedModel is used).\n            # Any metrics added into the saved model (for example using\n            # model.compile(..., metrics=[...]), etc) will be computed\n            # automatically.\n            # To add validation thresholds for metrics saved with the model,\n            # add them keyed by metric name to the thresholds map.\n            metrics=[\n                tfma.MetricConfig(class_name='ExampleCount'),\n                tfma.MetricConfig(class_name='BinaryAccuracy',\n                  threshold=tfma.MetricThreshold(\n                      value_threshold=tfma.GenericValueThreshold(\n                          lower_bound={'value': 0.5}),\n                      # Change threshold will be ignored if there is no\n                      # baseline model resolved from MLMD (first run).\n                      change_threshold=tfma.GenericChangeThreshold(\n                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n                          absolute={'value': -1e-10})))\n            ]\n        )\n    ],\n    slicing_specs=[\n        # An empty slice spec means the overall slice, i.e. the whole dataset.\n        tfma.SlicingSpec(),\n        # Data can be sliced along a feature column. In this case, data is\n        # sliced along feature column trip_start_hour.\n        tfma.SlicingSpec(\n            feature_keys=['trip_start_hour'])\n    ])\n\n\"\"\"\nExplanation: Evaluator\nEvaluator コンポーネントは、評価セットに対してモデル パフォーマンス指標を計算します。TensorFlow Model Analysisライブラリを使用します。Evaluatorは、オプションで、新しくトレーニングされたモデルが以前のモデルよりも優れていることを検証できます。これは、モデルを毎日自動的にトレーニングおよび検証する実稼働環境のパイプライン設定で役立ちます。このノートブックでは 1 つのモデルのみをトレーニングするため、Evaluatorはモデルに自動的に「good」というラベルを付けます。\nEvaluatorは、ExampleGenからのデータ、Trainerからのトレーニング済みモデル、およびスライス構成を入力として受け取ります。スライス構成により、特徴値に関する指標をスライスすることができます (たとえば、午前 8 時から午後 8 時までのタクシー乗車でモデルがどのように動作するかなど)。 この構成の例は、以下を参照してください。\nEnd of explanation\n\"\"\"\n\n\n# Use TFMA to compute a evaluation statistics over features of a model and\n# validate them against a baseline.\n\n# The model resolver is only required if performing model validation in addition\n# to evaluation. In this case we validate against the latest blessed model. If\n# no model has been blessed before (as in this case) the evaluator will make our\n# candidate the first blessed model.\nmodel_resolver = tfx.dsl.Resolver(\n      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n      model_blessing=tfx.dsl.Channel(\n          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n              'latest_blessed_model_resolver')\ncontext.run(model_resolver, enable_cache=True)\n\nevaluator = tfx.components.Evaluator(\n    examples=example_gen.outputs['examples'],\n    model=trainer.outputs['model'],\n    baseline_model=model_resolver.outputs['model'],\n    eval_config=eval_config)\ncontext.run(evaluator, enable_cache=True)\n\n\"\"\"\nExplanation: 次に、この構成を Evaluatorに渡して実行します。\nEnd of explanation\n\"\"\"\n\n\nevaluator.outputs\n\n\"\"\"\nExplanation: Evaluator の出力アーティファクトを調べてみましょう。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(evaluator.outputs['evaluation'])\n\n\"\"\"\nExplanation: evaluation出力を使用すると、評価セット全体のグローバル指標のデフォルトの視覚化を表示できます。\nEnd of explanation\n\"\"\"\n\n\nimport tensorflow_model_analysis as tfma\n\n# Get the TFMA output result path and load the result.\nPATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\ntfma_result = tfma.load_eval_result(PATH_TO_RESULT)\n\n# Show data sliced along feature column trip_start_hour.\ntfma.view.render_slicing_metrics(\n    tfma_result, slicing_column='trip_start_hour')\n\n\"\"\"\nExplanation: スライスされた評価メトリクスの視覚化を表示するには、TensorFlow Model Analysis ライブラリを直接呼び出します。\nEnd of explanation\n\"\"\"\n\n\nblessing_uri = evaluator.outputs['blessing'].get()[0].uri\n!ls -l {blessing_uri}\n\n\"\"\"\nExplanation: この視覚化は同じ指標を示していますが、評価セット全体ではなく、trip_start_hourのすべての特徴値で計算されています。\nTensorFlow モデル分析は、公平性インジケーターやモデル パフォーマンスの時系列のプロットなど、他の多くの視覚化をサポートしています。 詳細については、チュートリアルを参照してください。\n構成にしきい値を追加したため、検証出力も利用できます。{code 0}blessing{/code 0} アーティファクトの存在は、モデルが検証に合格したことを示しています。これは実行される最初の検証であるため、候補は自動的に bless されます。\nEnd of explanation\n\"\"\"\n\n\nPATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\nprint(tfma.load_validation_result(PATH_TO_RESULT))\n\n\"\"\"\nExplanation: 検証結果レコードを読み込み、成功を確認することもできます。\nEnd of explanation\n\"\"\"\n\n\npusher = tfx.components.Pusher(\n    model=trainer.outputs['model'],\n    model_blessing=evaluator.outputs['blessing'],\n    push_destination=tfx.proto.PushDestination(\n        filesystem=tfx.proto.PushDestination.Filesystem(\n            base_directory=_serving_model_dir)))\ncontext.run(pusher, enable_cache=True)\n\n\"\"\"\nExplanation: Pusher\nPusher コンポーネントは通常、TFX パイプラインの最後にあります。このコンポーネントはモデルが検証に合格したかどうかをチェックし、合格した場合はモデルを _serving_model_dirにエクスポートします。\nEnd of explanation\n\"\"\"\n\n\npusher.outputs\n\n\"\"\"\nExplanation: 次にPusherの出力アーティファクトを調べてみましょう。\nEnd of explanation\n\"\"\"\n\n\npush_uri = pusher.outputs['pushed_model'].get()[0].uri\nmodel = tf.saved_model.load(push_uri)\n\nfor item in model.signatures.items():\n  pp.pprint(item)\n\n\"\"\"\nExplanation: 特に、Pusher はモデルを次のような SavedModel 形式でエクスポートします。\nEnd of explanation\n\"\"\"\n\n_, _, data = twpca.datasets.jittered_neuron()\nmodel = TWPCA(data, n_components=1, warpinit='identity')\n\nnp.all(np.isclose(model.params['warp'], np.arange(model.shared_length), atol=1e-5, rtol=2))\n\nnp.nanmax(np.abs(model.transform() - data)) < 1e-5\n\n\"\"\"\nExplanation: check identity warp does not change data appreciably\nEnd of explanation\n\"\"\"\n\n\nmodel = TWPCA(data, n_components=1, warpinit='shift')\n\nplt.imshow(np.squeeze(model.transform()))\n\n\"\"\"\nExplanation: check that shift initialization for warp solves the simple toy problem\nEnd of explanation\n\"\"\"\n\nfrom __future__ import print_function, division, unicode_literals\n\nimport oddt\nfrom oddt.datasets import dude\nprint(oddt.__version__)\n\n\"\"\"\nExplanation: <h1>DUD-E: A Database of Useful Decoys: Enhanced</h1>\nEnd of explanation\n\"\"\"\n\n\n%%bash\nmkdir -p ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/ampc/ampc.tar.gz | tar xz -C ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/cxcr4/cxcr4.tar.gz | tar xz -C ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/pur2/pur2.tar.gz | tar xz -C ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/pygm/pygm.tar.gz | tar xz -C ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/sahh/sahh.tar.gz | tar xz -C ./DUD-E_targets/\n\ndirectory = './DUD-E_targets'\n\n\"\"\"\nExplanation: We'd like to read files from DUD-E.<br/>\nYou can download different targets and different numbers of targets, but I used only these five:\nampc, \ncxcr4, \npur2, \npygm, \nsahh.<br/>\nEnd of explanation\n\"\"\"\n\n\ndude_database = dude(home=directory)\n\n\"\"\"\nExplanation: We will use the dude class.\nEnd of explanation\n\"\"\"\n\n\ntarget = dude_database['cxcr4']\n\n\"\"\"\nExplanation: Now we can get one target or iterate over all targets in our directory.\nLet's choose one target.\nEnd of explanation\n\"\"\"\n\n\ntarget.ligand\n\n\"\"\"\nExplanation: target has four properties: protein, ligand, actives and decoys:<br/>\nprotein - protein molecule<br/>\nligand - ligand molecule<br/>\nactives - generator containing actives<br/>\ndecoys - generator containing decoys\nEnd of explanation\n\"\"\"\n\n\nfor target in dude_database:\n    actives = list(target.actives)\n    decoys = list(target.decoys)\n    print('Target: ' + target.dude_id, \n          'Number of actives: ' + str(len(actives)), \n          'Number of decoys: ' + str(len(decoys)), \n          sep='\\t\\t')\n\n\"\"\"\nExplanation: Let's see which target has the most actives and decoys.\nEnd of explanation\n\"\"\"\n\nmax_steps = 3000\nbatch_size = 128\ndata_dir = 'data/cifar10/cifar-10-batches-bin/'\nmodel_dir = 'model/_cifar10_v2/'\n\n\"\"\"\nExplanation: 全局参数\nEnd of explanation\n\"\"\"\n\n\nX_train, y_train = cifar10_input.distorted_inputs(data_dir, batch_size)\n\nX_test, y_test = cifar10_input.inputs(eval_data=True, data_dir=data_dir, batch_size=batch_size)\n\nimage_holder = tf.placeholder(tf.float32, [batch_size, 24, 24, 3])\nlabel_holder = tf.placeholder(tf.int32, [batch_size])\n\n\"\"\"\nExplanation: 初始化权重\n如果需要，会给权重加上L2 loss。为了在后面计算神经网络的总体loss的时候被用上，需要统一存到一个collection。\n加载数据\n使用cifa10_input来获取数据，这个文件来自tensorflow github，可以下载下来直接使用。如果使用distorted_input方法，那么得到的数据是经过增强处理的。会对图片随机做出切片、翻转、修改亮度、修改对比度等操作。这样就能多样化我们的训练数据。\n得到一个tensor，batch_size大小的batch。并且可以迭代的读取下一个batch。\nEnd of explanation\n\"\"\"\n\n\nweight1 = variable_with_weight_loss([5, 5, 3, 64], stddev=0.05, lambda_value=0)\nkernel1 = tf.nn.conv2d(image_holder, weight1, [1, 1, 1, 1], padding='SAME')\nbias1 = tf.Variable(tf.constant(0.0, shape=[64]))\nconv1 = tf.nn.relu(tf.nn.bias_add(kernel1, bias1))\npool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\nnorm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n\n\"\"\"\nExplanation: 第一个卷积层\n同样的，我们使用5x5卷积核，3个通道（input_channel），64个output_channel。不对第一层的参数做正则化，所以将lambda_value设定为0。其中涉及到一个小技巧，就是在pool层，使用了3x3大小的ksize，但是使用2x2的stride，这样增加数据的丰富性。最后使用LRN。LRN最早见于Alex参见ImageNet的竞赛的那篇CNN论文中，Alex在论文中解释了LRN层模仿了生物神经系统的“侧抑制”机制，对局部神经元的活动创建竞争环境，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增加了模型的泛化能力。不过在之后的VGGNet论文中，对比了使用和不使用LRN两种模型，结果表明LRN并不能提高模型的性能。不过这里还是基于AlexNet的设计将其加上。\nEnd of explanation\n\"\"\"\n\n\nweight2 = variable_with_weight_loss(shape=[5, 5, 64, 64], stddev=5e-2, lambda_value=0.0)\nkernel2 = tf.nn.conv2d(norm1, weight2, strides=[1, 1, 1, 1], padding='SAME')\nbias2 = tf.Variable(tf.constant(0.1, shape=[64]))\nconv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2))\nnorm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\npool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n\"\"\"\nExplanation: 第二个卷积层\n\n输入64个channel，输出依然是64个channel\n设定bias的大小为0.1\n调换最大池化层和LRN的顺序，先进行LRN然后再最大池化层\n\n但是为什么要这么做，完全不知道？\n多看论文。\nEnd of explanation\n\"\"\"\n\n\nflattern = tf.reshape(pool2, [batch_size, -1])\ndim = flattern.get_shape()[1].value\nweight3 = variable_with_weight_loss(shape=[dim, 384], stddev=0.04, lambda_value=0.04)\nbias3 = tf.Variable(tf.constant(0.1, shape=[384]))\nlocal3 = tf.nn.relu(tf.matmul(flattern, weight3) + bias3)\n\n\"\"\"\nExplanation: 第一个全连接层\n\n要将卷积层拉伸\n全连接到新的隐藏层，设定为384个节点\n正态分布设定为0.04，bias设定为0.1\n重点是，在这里我们还设定weight loss的lambda数值为0.04\nEnd of explanation\n\"\"\"\n\n\nweight4 = variable_with_weight_loss(shape=[384, 192], stddev=0.04, lambda_value=0.04)\nbias4 = tf.Variable(tf.constant(0.1, shape=[192]))\nlocal4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4)\n\n\"\"\"\nExplanation: 第二个全连接层\n\n下降为192个节点，减少了一半\nEnd of explanation\n\"\"\"\n\n\nweight5 = variable_with_weight_loss(shape=[192, 10], stddev=1/192.0, lambda_value=0.0)\nbias5 = tf.Variable(tf.constant(0.0, shape=[10]))\nlogits = tf.add(tf.matmul(local4, weight5), bias5)\n\ndef loss(logits, labels):\n    labels = tf.cast(labels, tf.int64)\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=logits, labels=labels,\n        name = 'cross_entropy_per_example'\n    )\n    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n    tf.add_to_collection('losses', cross_entropy_mean)\n    \n    return tf.add_n(tf.get_collection('losses'), name='total_loss')\n\nloss = loss(logits, label_holder)\n\ntrain_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\n\n\"\"\"\nExplanation: 输出层\n\n最后有10个类别\nEnd of explanation\n\"\"\"\n\n\ntop_k_op = tf.nn.in_top_k(logits, label_holder, 1)\n\nsess = tf.InteractiveSession()\n\nsaver = tf.train.Saver()\n\ntf.global_variables_initializer().run()\n\n\"\"\"\nExplanation: 使用in_top_k来输出top k的准确率，默认使用top 1。常用的可以是top 5。\nEnd of explanation\n\"\"\"\n\n\ntf.train.start_queue_runners()\n\n\"\"\"\nExplanation: 启动caifar_input中需要用的线程队列。主要用途是图片数据增强。这里总共使用了16个线程来处理图片。\nEnd of explanation\n\"\"\"\n\n\nfor step in range(max_steps):\n    start_time = time.time()\n    image_batch, label_batch = sess.run([X_train, y_train])\n    _, loss_value = sess.run([train_op, loss], \n                             feed_dict={image_holder: image_batch, label_holder: label_batch})\n    duration = time.time() - start_time\n    if step % 10 == 0:\n        examples_per_sec = batch_size / duration\n        sec_this_batch = float(duration)\n        \n        format_str = ('step %d, loss = %.2f (%.1f examples/sec; %.3f sec/batch)')\n        print(format_str % (step, loss_value, examples_per_sec, sec_this_batch))\n\nsaver.save(sess, save_path=os.path.join(model_dir, 'model.chpt'), global_step=max_steps)\n\nnum_examples = 10000\nnum_iter = int(math.ceil(num_examples / batch_size))\nture_count = 0\ntotal_sample_count = num_iter * batch_size\nstep = 0\nwhile step < num_iter:\n    image_batch, label_batch = sess.run([X_test, y_test])\n    predictions = sess.run([top_k_op], \n                           feed_dict={image_holder: image_batch, label_holder: label_batch})\n    true_count += np.sum(predictions)\n    step += 1\n\nprecision = ture_count / total_sample_count\nprint(\"Precision @ 1 = %.3f\" % precision)\n\nsess.close()\n\n\"\"\"\nExplanation: 每次在计算之前，先执行image_train,label_train来获取一个batch_size大小的训练数据。然后，feed到train_op和loss中，训练样本。每10次迭代计算就会输出一些必要的信息。\nEnd of explanation\n\"\"\"\n\n!python -m spacy download en_core_web_sm\n\n\"\"\"\nExplanation: Versioning Example (Part 1/3)\nIn this example, we'll train an NLP model for sentiment analysis of tweets using spaCy.\nThrough this series, we'll take advantage of ModelDB's versioning system to keep track of changes.\nThis workflow requires verta&gt;=0.14.4 and spaCy&gt;=2.0.0.\n\nSetup\nDownload a spaCy model to train.\nEnd of explanation\n\"\"\"\n\n\nfrom __future__ import unicode_literals, print_function\n\nimport boto3\nimport json\nimport numpy as np\nimport pandas as pd\nimport spacy\n\n\"\"\"\nExplanation: Import libraries we'll need.\nEnd of explanation\n\"\"\"\n\n\nfrom verta import Client\n\nclient = Client('http://localhost:3000/')\nproj = client.set_project('Tweet Classification')\nexpt = client.set_experiment('SpaCy')\n\n\"\"\"\nExplanation: Bring in Verta's ModelDB client to organize our work, and log and version metadata.\nEnd of explanation\n\"\"\"\n\n\nS3_BUCKET = \"verta-starter\"\nS3_KEY = \"english-tweets.csv\"\nFILENAME = S3_KEY\n\nboto3.client('s3').download_file(S3_BUCKET, S3_KEY, FILENAME)\n\n\"\"\"\nExplanation: Prepare Data\nDownload a dataset of English tweets from S3 for us to train with.\nEnd of explanation\n\"\"\"\n\n\nimport utils\n\ndata = pd.read_csv(FILENAME).sample(frac=1).reset_index(drop=True)\nutils.clean_data(data)\n\ndata.head()\n\n\"\"\"\nExplanation: Then we'll load and clean the data.\nEnd of explanation\n\"\"\"\n\n\nfrom verta.code import Notebook\nfrom verta.configuration import Hyperparameters\nfrom verta.dataset import S3\nfrom verta.environment import Python\n\ncode_ver = Notebook()  # Notebook & git environment\nconfig_ver = Hyperparameters({'n_iter': 20})\ndataset_ver = S3(\"s3://{}/{}\".format(S3_BUCKET, S3_KEY))\nenv_ver = Python(Python.read_pip_environment())  # pip environment and Python version\n\n\"\"\"\nExplanation: Capture and Version Model Ingredients\nWe'll first capture metadata about our code, configuration, dataset, and environment using utilities from the verta library.\nEnd of explanation\n\"\"\"\n\n\nrepo = client.set_repository('Tweet Classification')\ncommit = repo.get_commit(branch='master')\n\n\"\"\"\nExplanation: Then, to log them, we'll use a ModelDB repository to prepare a commit.\nEnd of explanation\n\"\"\"\n\n\ncommit.update(\"notebooks/tweet-analysis\", code_ver)\ncommit.update(\"config/hyperparams\", config_ver)\ncommit.update(\"data/tweets\", dataset_ver)\ncommit.update(\"env/python\", env_ver)\n\ncommit.save(\"Initial model\")\n\ncommit\n\n\"\"\"\nExplanation: Now we'll add these versioned components to the commit and save it to ModelDB.\nEnd of explanation\n\"\"\"\n\n\nnlp = spacy.load('en_core_web_sm')\n\n\"\"\"\nExplanation: Train and Log Model\nWe'll use the pre-trained spaCy model we downloaded earlier...\nEnd of explanation\n\"\"\"\n\n\nimport training\n\ntraining.train(nlp, data, n_iter=20)\n\n\"\"\"\nExplanation: ...and fine-tune it with our dataset.\nEnd of explanation\n\"\"\"\n\n\nrun = client.set_experiment_run()\n\nrun.log_model(nlp)\n\n\"\"\"\nExplanation: Now that our model is good to go, we'll log it to ModelDB so our progress is never lost.\nUsing Verta's ModelDB Client, we'll create an Experiment Run to encapsulate our work, and log our model as an artifact.\nEnd of explanation\n\"\"\"\n\n\nrun.log_commit(\n    commit,\n    {\n        'notebook': \"notebooks/tweet-analysis\",\n        'hyperparameters': \"config/hyperparams\",\n        'training_data': \"data/tweets\",\n        'python_env': \"env/python\",\n    },\n)\n\n\"\"\"\nExplanation: And finally, we'll link the commit we created earlier to the Experiment Run to complete our logged model version.\nEnd of explanation\n\"\"\"\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport yaml\n\n\"\"\"\nExplanation: The following script extracts the (more) helpful reviews from the swiss reviews and saves them locally.\nFrom the extracted reviews it also saves a list with their asin identifiers.\nThe list of asin identifiers will be later used to to find the average review rating for the respective products.\nEnd of explanation\n\"\"\"\n\n\nwith open(\"data/swiss-reviews.txt\", 'r') as fp:\n    swiss_rev = fp.readlines()\n\nlen(swiss_rev)\n\nswiss_rev[2]\n\n\"\"\"\nExplanation: Load the swiss reviews\nEnd of explanation\n\"\"\"\n\n\ndef filter_helpful(line):\n    l = line.rstrip('\\n')\n    l = yaml.load(l)\n    if('helpful' in l.keys()):\n        if(l['helpful'][1] >= 5):\n            return True\n        else:\n            return False\n    else:\n        print(\"Review does not have helpful score key: \"+line)\n        return False\n\n\"\"\"\nExplanation: The filter_helpful function keeps only the reviews which had at least 5 flags/votes in the helpfulness field.\nThis amounts to a subset of around 23000 reviews. A smaller subset of around 10000 reviews was obtained as well by only keeping reviews with 10 flags/votes. The main advantage of the smaller subset is that it contains better quality reviews while its drawback is, of course, the reduced size.\n1) Extract the helpful reviews\nEnd of explanation\n\"\"\"\n\n\ndef get_helpful(data):\n    res = []\n    counter = 1\n    i = 0\n    for line in data:\n        i += 1\n        if(filter_helpful(line)):\n            if(counter % 1000 == 0):\n                print(\"Count \"+str(counter)+\" / \"+str(i))\n            counter += 1\n            res.append(line)\n    return res\n\nswiss_reviews_helpful = get_helpful(swiss_rev)\n\nlen(swiss_reviews_helpful)\n\n\"\"\"\nExplanation: Apply the filter_helpful to each swiss product review\nEnd of explanation\n\"\"\"\n\n\nwrite_file = open('data/swiss-reviews-helpful-correct-bigger.txt', 'w')\nfor item in swiss_reviews_helpful:\n  write_file.write(item)\nwrite_file.close()\n\n\"\"\"\nExplanation: Save the subset with helpful swiss product reviews\nEnd of explanation\n\"\"\"\n\n\nwith open('data/swiss-reviews-helpful-correct-bigger.txt', 'r') as fp:\n    swiss_reviews_helpful = fp.readlines()\n\n\"\"\"\nExplanation: 2) Extract the asins of the products which the helpful reviews correspond to\nEnd of explanation\n\"\"\"\n\n\ndef filter_asin(line):\n    l = line.rstrip('\\n')\n    l = yaml.load(l)\n    if('asin' in l.keys()):\n        return l['asin']\n    else:\n        return ''\n\nhelpful_asins = []\ncounter = 1\nfor item in swiss_reviews_helpful:\n    if(counter%500 == 0):\n        print(counter)\n    counter += 1\n    x = filter_asin(item)\n    if(len(x) > 0):\n        helpful_asins.append(x)\n\n\"\"\"\nExplanation: The following function simply extracts the 'asin' from the helpful reviews.\nRepetitions of the asins are of no consequence, as the list is just meant to be a check up.\nEnd of explanation\n\"\"\"\n\n\nimport pickle\n\nwith open('data/helpful_asins_bigger.pickle', 'wb') as fp:\n    pickle.dump(helpful_asins, fp)\n\n\"\"\"\nExplanation: Save the list of asins.\nEnd of explanation\n\"\"\"\n\nget_ipython().magic('load_ext autoreload')\nget_ipython().magic('autoreload 2')\n\nimport glob\nimport logging\nimport numpy as np\nimport os\n\nlogging.basicConfig(format=\n                          \"%(relativeCreated)12d [%(filename)s:%(funcName)20s():%(lineno)s] [%(process)d] %(message)s\",\n                    # filename=\"/tmp/caiman.log\",\n                    level=logging.WARNING)\n\nimport caiman as cm\nfrom caiman.source_extraction import cnmf as cnmf\nfrom caiman.utils.utils import download_demo\nimport matplotlib.pyplot as plt\n\nimport bokeh.plotting as bpl\nbpl.output_notebook()\n\n\"\"\"\nExplanation: Example of 1p online analysis using a Ring CNN + OnACID\nThe demo shows how to perform online analysis on one photon data using a Ring CNN for extracting the background followed by processing using the OnACID algorithm. The algorithm relies on the usage a GPU to efficiently estimate and apply the background model so it is recommended to have access to a GPU when running this notebook.\nEnd of explanation\n\"\"\"\n\n\nfnames=download_demo('blood_vessel_10Hz.mat')\n\n\"\"\"\nExplanation: First specify the data file(s) to be analyzed\nThe download_demo method will download the file (if not already present) and store it inside your caiman_data/example_movies folder. You can specify any path to files you want to analyze.\nEnd of explanation\n\"\"\"\n\n\nreuse_model = False                                                 # set to True to re-use an existing ring model\npath_to_model = None                                                # specify a pre-trained model here if needed \ngSig = (7, 7)                                                       # expected half size of neurons\ngnb = 2                                                             # number of background components for OnACID\ninit_batch = 500                                                    # number of frames for initialization and training\n\nparams_dict = {'fnames': fnames,\n               'var_name_hdf5': 'Y',                                # name of variable inside mat file where the data is stored\n               'fr': 10,                                            # frame rate (Hz)\n               'decay_time': 0.5,                                   # approximate length of transient event in seconds\n               'gSig': gSig,\n               'p': 0,                                              # order of AR indicator dynamics\n               'ring_CNN': True,                                    # SET TO TRUE TO USE RING CNN \n               'min_SNR': 2.65,                                     # minimum SNR for accepting new components\n               'SNR_lowest': 0.75,                                  # reject components with SNR below this value\n               'use_cnn': False,                                    # do not use CNN based test for components\n               'use_ecc': True,                                     # test eccentricity\n               'max_ecc': 2.625,                                    # reject components with eccentricity above this value\n               'rval_thr': 0.70,                                    # correlation threshold for new component inclusion\n               'rval_lowest': 0.25,                                 # reject components with corr below that value\n               'ds_factor': 1,                                      # spatial downsampling factor (increases speed but may lose some fine structure)\n               'nb': gnb,\n               'motion_correct': False,                             # Flag for motion correction\n               'init_batch': init_batch,                            # number of frames for initialization (presumably from the first file)\n               'init_method': 'bare',\n               'normalize': False,\n               'expected_comps': 1100,                               # maximum number of expected components used for memory pre-allocation (exaggerate here)\n               'sniper_mode': False,                                 # flag using a CNN to detect new neurons (o/w space correlation is used)\n               'dist_shape_update' : True,                           # flag for updating shapes in a distributed way\n               'min_num_trial': 5,                                   # number of candidate components per frame\n               'epochs': 3,                                          # number of total passes over the data\n               'stop_detection': True,                               # Run a last epoch without detecting new neurons  \n               'K': 50,                                              # initial number of components\n               'lr': 6e-4,\n               'lr_scheduler': [0.9, 6000, 10000],\n               'pct': 0.01,\n               'path_to_model': path_to_model,                       # where the ring CNN model is saved/loaded\n               'reuse_model': reuse_model                            # flag for re-using a ring CNN model          \n              }\nopts = cnmf.params.CNMFParams(params_dict=params_dict)\n\n\"\"\"\nExplanation: Set up some parameters\nHere we set up some parameters for specifying the ring model and running OnACID. We use the same params object as in batch processing with CNMF.\nEnd of explanation\n\"\"\"\n\n\nrun_onacid = True\n\nif run_onacid:\n    cnm = cnmf.online_cnmf.OnACID(params=opts)\n    cnm.fit_online()\n    fld_name = os.path.dirname(cnm.params.ring_CNN['path_to_model'])\n    res_name_nm = os.path.join(fld_name, 'onacid_results_nm.hdf5')\n    cnm.save(res_name_nm)                # save initial results (without any postprocessing)\nelse:\n    fld_name = os.path.dirname(path_to_model)\n    res_name = os.path.join(fld_name, 'onacid_results.hdf5')\n    cnm = cnmf.online_cnmf.load_OnlineCNMF(res_name)\n    cnm.params.data['fnames'] = fnames\n\n\"\"\"\nExplanation: Now run the Ring-CNN + CaImAn online algorithm (OnACID).\nThe first initbatch frames are used for training the ring-CNN model. Once the model is trained the background is subtracted and the different is used for initialization purposes. The initialization method chosen here bare will only search for a small number of neurons and is mostly used to initialize the background components. Initialization with the full CNMF can also be used by choosing cnmf.\nWe first create an OnACID object located in the module online_cnmf and we pass the parameters similarly to the case of batch processing. We then run the algorithm using the fit_online method. We then save the results inside\nthe folder where the Ring_CNN model is saved.\nEnd of explanation\n\"\"\"\n\n\nds = 10             # plot every ds frames to make more manageable figures\ninit_batch = 500\ndims, T = cnmf.utilities.get_file_size(fnames, var_name_hdf5='Y')\nT = np.array(T).sum()\nn_epochs = cnm.params.online['epochs']\nT_detect = 1e3*np.hstack((np.zeros(init_batch), cnm.t_detect))\nT_shapes = 1e3*np.hstack((np.zeros(init_batch), cnm.t_shapes))\nT_online = 1e3*np.hstack((np.zeros(init_batch), cnm.t_online)) - T_detect - T_shapes\nplt.figure()\nplt.stackplot(np.arange(len(T_detect))[::ds], T_online[::ds], T_detect[::ds], T_shapes[::ds],\n              colors=['tab:red', 'tab:purple', 'tab:brown'])\nplt.legend(labels=['process', 'detect', 'shapes'], loc=2)\nplt.title('Processing time allocation')\nplt.xlabel('Frame #')\nplt.ylabel('Processing time [ms]')\nmax_val = 80\nplt.ylim([0, max_val]);\nplt.plot([init_batch, init_batch], [0, max_val], '--k')\nfor i in range(n_epochs - 1):\n    plt.plot([(i+1)*T, (i+1)*T], [0, max_val], '--k')\nplt.xlim([0, n_epochs*T]);\nplt.savefig(os.path.join(fld_name, 'time_per_frame_ds.pdf'), bbox_inches='tight', pad_inches=0)\n\ninit_batch = 500\nplt.figure()\ntc_init = cnm.t_init*np.ones(T*n_epochs)\nds = 10\n#tc_mot = np.hstack((np.zeros(init_batch), np.cumsum(T_motion)/1000))\ntc_prc = np.cumsum(T_online)/1000#np.hstack((np.zeros(init_batch), ))\ntc_det = np.cumsum(T_detect)/1000#np.hstack((np.zeros(init_batch), ))\ntc_shp = np.cumsum(T_shapes)/1000#np.hstack((np.zeros(init_batch), ))\nplt.stackplot(np.arange(len(tc_init))[::ds], tc_init[::ds], tc_prc[::ds], tc_det[::ds], tc_shp[::ds],\n              colors=['g', 'tab:red', 'tab:purple', 'tab:brown'])\nplt.legend(labels=['initialize', 'process', 'detect', 'shapes'], loc=2)\nplt.title('Processing time allocation')\nplt.xlabel('Frame #')\nplt.ylabel('Processing time [s]')\nmax_val = (tc_prc[-1] + tc_det[-1] + tc_shp[-1] + cnm.t_init)*1.05\nfor i in range(n_epochs - 1):\n    plt.plot([(i+1)*T, (i+1)*T], [0, max_val], '--k')\nplt.xlim([0, n_epochs*T]);\nplt.ylim([0, max_val])\nplt.savefig(os.path.join(fld_name, 'time_cumulative_ds.pdf'), bbox_inches='tight', pad_inches=0)\n\nprint('Cost of estimating model and running first epoch: {:.2f}s'.format(tc_prc[T] + tc_det[T] + tc_shp[T] + tc_init[T]))\n\n\"\"\"\nExplanation: Check speed\nCreate some plots that show the speed per frame and cumulatively\nEnd of explanation\n\"\"\"\n\n\n# first compute background summary images\nimages = cm.load(fnames, var_name_hdf5='Y', subindices=slice(None, None, 2))\ncn_filter, pnr = cm.summary_images.correlation_pnr(images, gSig=3, swap_dim=False) # change swap dim if output looks weird, it is a problem with tiffile\n\nplt.figure(figsize=(15, 7))\nplt.subplot(1,2,1); plt.imshow(cn_filter); plt.colorbar()\nplt.subplot(1,2,2); plt.imshow(pnr); plt.colorbar()\n\ncnm.estimates.plot_contours_nb(img=cn_filter, idx=cnm.estimates.idx_components, line_color='white', thr=0.3)\n\n\"\"\"\nExplanation: Do some initial plotting\nEnd of explanation\n\"\"\"\n\n\ncnm.estimates.nb_view_components(img=cn_filter, denoised_color='red')\n\n\"\"\"\nExplanation: View components\nNow inspect the components extracted by OnACID. Note that if single pass was used then several components would be non-zero only for the part of the time interval indicating that they were detected online by OnACID.\nNote that if you get data rate error you can start Jupyter notebooks using:\n'jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10'\nEnd of explanation\n\"\"\"\n\n\nsave_file = True\nif save_file:\n    from caiman.utils.nn_models import create_LN_model\n    model_LN = create_LN_model(images, shape=opts.data['dims'] + (1,), n_channels=opts.ring_CNN['n_channels'],\n                               width=opts.ring_CNN['width'], use_bias=opts.ring_CNN['use_bias'], gSig=gSig[0],\n                               use_add=opts.ring_CNN['use_add'])\n    model_LN.load_weights(cnm.params.ring_CNN['path_to_model'])\n\n    # Load the data in batches and save them\n\n    m = []\n    saved_files = []\n    batch_length = 256\n    for i in range(0, T, batch_length):\n        images = cm.load(fnames, var_name_hdf5='Y', subindices=slice(i, i + batch_length))\n        images_filt = np.squeeze(model_LN.predict(np.expand_dims(images, axis=-1)))\n        temp_file = os.path.join(fld_name, 'pfc_back_removed_' + format(i, '05d') + '.h5')\n        saved_files.append(temp_file)\n        m = cm.movie(np.maximum(images - images_filt, 0))\n        m.save(temp_file)\nelse:\n    saved_files = glob.glob(os.path.join(fld_name, 'pfc_back_removed_*'))\n    saved_files.sort()\n\nfname_mmap = cm.save_memmap([saved_files], order='C', border_to_0=0)\nYr, dims, T = cm.load_memmap(fname_mmap)\nimages_mmap = Yr.T.reshape((T,) + dims, order='F')\n\n\"\"\"\nExplanation: Load ring model to filter the data\nFilter the data with the learned Ring CNN model and a create memory mapped file with the background subtracted data. We will use this to run the quality tests and screen for false positive components.\nEnd of explanation\n\"\"\"\n\n\ncnm.params.merging['merge_thr'] = 0.7\ncnm.estimates.c1 = np.zeros(cnm.estimates.A.shape[-1])\ncnm.estimates.bl = np.zeros(cnm.estimates.A.shape[-1])\ncnm.estimates.neurons_sn = np.zeros(cnm.estimates.A.shape[-1])\ncnm.estimates.g = None #np.ones((cnm.estimates.A.shape[-1], 1))*.9\ncnm.estimates.merge_components(Yr, cnm.params)\n\n\"\"\"\nExplanation: Merge components\nEnd of explanation\n\"\"\"\n\n\ncnm.params.quality\n\ncnm.estimates.evaluate_components(imgs=images_mmap, params=cnm.params)\n\ncnm.estimates.plot_contours_nb(img=cn_filter, idx=cnm.estimates.idx_components, line_color='white')\n\ncnm.estimates.nb_view_components(idx=cnm.estimates.idx_components, img=cn_filter)\n\n\"\"\"\nExplanation: Evaluate components and compare again\nWe run the component evaluation tests to screen for false positive components.\nEnd of explanation\n\"\"\"\n\n\ncnmfe_results = download_demo('online_vs_offline.npz')\nlocals().update(np.load(cnmfe_results, allow_pickle=True))\nA_patch_good = A_patch_good.item()\nestimates_gt = cnmf.estimates.Estimates(A=A_patch_good, C=C_patch_good, dims=dims)\n\n\nmaxthr=0.01\ncnm.estimates.A_thr=None\ncnm.estimates.threshold_spatial_components(maxthr=maxthr)\nestimates_gt.A_thr=None\nestimates_gt.threshold_spatial_components(maxthr=maxthr*10)\nmin_size = np.pi*(gSig[0]/1.5)**2\nmax_size = np.pi*(gSig[0]*1.5)**2\nntk = cnm.estimates.remove_small_large_neurons(min_size_neuro=min_size, max_size_neuro=2*max_size)\ngtk = estimates_gt.remove_small_large_neurons(min_size_neuro=min_size, max_size_neuro=2*max_size)\n\nm1, m2, nm1, nm2, perf = cm.base.rois.register_ROIs(estimates_gt.A_thr[:, estimates_gt.idx_components],\n                                                  cnm.estimates.A_thr[:, cnm.estimates.idx_components],\n                                                  dims, align_flag=False, thresh_cost=.7, plot_results=True,\n                                                  Cn=cn_filter, enclosed_thr=None)[:-1]\n\n\"\"\"\nExplanation: Compare against CNMF-E results\nWe download the results of CNMF-E on the same dataset and compare.\nEnd of explanation\n\"\"\"\n\n\nfor k, v in perf.items():\n    print(k + ':', '%.4f' % v, end='  ')\n\n\"\"\"\nExplanation: Print performance results\nEnd of explanation\n\"\"\"\n\n\nres_name = os.path.join(fld_name, 'onacid_results.hdf5')\ncnm.save(res_name)\n\n\"\"\"\nExplanation: Save the results\nEnd of explanation\n\"\"\"\n\n\nimport matplotlib.lines as mlines\nlp, hp = np.nanpercentile(cn_filter, [5, 98])\nA_onacid = cnm.estimates.A_thr.toarray().copy()\nA_onacid /= A_onacid.max(0)\n\nA_TP = estimates_gt.A[:, m1].toarray() #cnm.estimates.A[:, cnm.estimates.idx_components[m2]].toarray()\nA_TP = A_TP.reshape(dims + (-1,), order='F').transpose(2,0,1)\nA_FN = estimates_gt.A[:, nm1].toarray()\nA_FN = A_FN.reshape(dims + (-1,), order='F').transpose(2,0,1)\nA_FP = A_onacid[:,cnm.estimates.idx_components[nm2]]\nA_FP = A_FP.reshape(dims + (-1,), order='F').transpose(2,0,1)\n\n\nplt.figure(figsize=(15, 12))\nplt.imshow(cn_filter, vmin=lp, vmax=hp, cmap='viridis')\nplt.colorbar();\n\nfor aa in A_TP:\n    plt.contour(aa, [0.05], colors='k');\nfor aa in A_FN:\n    plt.contour(aa, [0.05], colors='r');\nfor aa in A_FP:\n    plt.contour(aa, [0.25], colors='w');\ncl = ['k', 'r', 'w']\nlb = ['both', 'CNMF-E only', 'ring CNN only']\nday = [mlines.Line2D([], [], color=cl[i], label=lb[i]) for i in range(3)]\nplt.legend(handles=day, loc=3)\nplt.axis('off');\nplt.margins(0, 0);\nplt.savefig(os.path.join(fld_name, 'ring_CNN_contours_gSig_3.pdf'), bbox_inches='tight', pad_inches=0)\n\nA_rej = cnm.estimates.A[:, cnm.estimates.idx_components_bad].toarray()\nA_rej = A_rej.reshape(dims + (-1,), order='F').transpose(2,0,1)\nplt.figure(figsize=(15, 15))\nplt.imshow(cn_filter, vmin=lp, vmax=hp, cmap='viridis')\nplt.title('Rejected Components')\nfor aa in A_rej:\n    plt.contour(aa, [0.05], colors='w');\n\n\"\"\"\nExplanation: Make some plots\nEnd of explanation\n\"\"\"\n\n\nfrom caiman.utils.nn_models import create_LN_model\nmodel_LN = create_LN_model(images, shape=opts.data['dims'] + (1,), n_channels=opts.ring_CNN['n_channels'],\n                           width=opts.ring_CNN['width'], use_bias=opts.ring_CNN['use_bias'], gSig=gSig[0],\n                           use_add=opts.ring_CNN['use_add'])\nmodel_LN.load_weights(cnm.params.ring_CNN['path_to_model'])\n\nW = model_LN.get_weights()\n\nplt.figure(figsize=(10, 10))\nplt.subplot(2,2,1); plt.imshow(np.squeeze(W[0][:,:,:,0])); plt.colorbar(); plt.title('Ring Kernel 1')\nplt.subplot(2,2,2); plt.imshow(np.squeeze(W[0][:,:,:,1])); plt.colorbar(); plt.title('Ring Kernel 2')\nplt.subplot(2,2,3); plt.imshow(np.squeeze(W[-1][:,:,0])); plt.colorbar(); plt.title('Multiplicative Layer 1')\nplt.subplot(2,2,4); plt.imshow(np.squeeze(W[-1][:,:,1])); plt.colorbar(); plt.title('Multiplicative Layer 2');\n\n\"\"\"\nExplanation: Show the learned filters\nEnd of explanation\n\"\"\"\n\n\nm1 = cm.load(fnames, var_name_hdf5='Y')  # original data\nm2 = cm.load(fname_mmap)  # background subtracted data\nm3 = m1 - m2  # estimated background\nm4 = cm.movie(cnm.estimates.A[:,cnm.estimates.idx_components].dot(cnm.estimates.C[cnm.estimates.idx_components])).reshape(dims + (T,)).transpose(2,0,1)\n              # estimated components\n\nnn = 0.01\nmm = 1 - nn/4   # normalize movies by quantiles\nm1 = (m1 - np.quantile(m1[:1000], nn))/(np.quantile(m1[:1000], mm) - np.quantile(m1[:1000], nn))\nm2 = (m2 - np.quantile(m2[:1000], nn))/(np.quantile(m2[:1000], mm) - np.quantile(m2[:1000], nn))\nm3 = (m3 - np.quantile(m3[:1000], nn))/(np.quantile(m3[:1000], mm) - np.quantile(m3[:1000], nn))\nm4 = (m4 - np.quantile(m4[:1000], nn))/(np.quantile(m4[:1000], mm) - np.quantile(m4[:1000], nn))\n\nm = cm.concatenate((cm.concatenate((m1.transpose(0,2,1), m3.transpose(0,2,1)), axis=2),\n                    cm.concatenate((m2.transpose(0,2,1), m4), axis=2)), axis=1)\n\nm[:3000].play(magnification=2, q_min=1, plot_text=True,\n              save_movie=True, movie_name=os.path.join(fld_name, 'movie.avi'))\n\n\"\"\"\nExplanation: Make a movie\nEnd of explanation\n\"\"\"\n\n#$HIDE_INPUT$\nimport pandas as pd\nfrom IPython.display import display\n\nred_wine = pd.read_csv('../input/dl-course-data/red-wine.csv')\n\n# Create training and validation splits\ndf_train = red_wine.sample(frac=0.7, random_state=0)\ndf_valid = red_wine.drop(df_train.index)\ndisplay(df_train.head(4))\n\n# Scale to [0, 1]\nmax_ = df_train.max(axis=0)\nmin_ = df_train.min(axis=0)\ndf_train = (df_train - min_) / (max_ - min_)\ndf_valid = (df_valid - min_) / (max_ - min_)\n\n# Split features and target\nX_train = df_train.drop('quality', axis=1)\nX_valid = df_valid.drop('quality', axis=1)\ny_train = df_train['quality']\ny_valid = df_valid['quality']\n\n\"\"\"\nExplanation: Introduction\nIn the first two lessons, we learned how to build fully-connected networks out of stacks of dense layers. When first created, all of the network's weights are set randomly -- the network doesn't \"know\" anything yet. In this lesson we're going to see how to train a neural network; we're going to see how neural networks learn.\nAs with all machine learning tasks, we begin with a set of training data. Each example in the training data consists of some features (the inputs) together with an expected target (the output). Training the network means adjusting its weights in such a way that it can transform the features into the target. In the 80 Cereals dataset, for instance, we want a network that can take each cereal's 'sugar', 'fiber', and 'protein' content and produce a prediction for that cereal's 'calories'. If we can successfully train a network to do that, its weights must represent in some way the relationship between those features and that target as expressed in the training data.\nIn addition to the training data, we need two more things:\n- A \"loss function\" that measures how good the network's predictions are.\n- An \"optimizer\" that can tell the network how to change its weights.\nThe Loss Function\nWe've seen how to design an architecture for a network, but we haven't seen how to tell a network what problem to solve. This is the job of the loss function.\nThe loss function measures the disparity between the the target's true value and the value the model predicts. \nDifferent problems call for different loss functions. We have been looking at regression problems, where the task is to predict some numerical value -- calories in 80 Cereals, rating in Red Wine Quality. Other regression tasks might be predicting the price of a house or the fuel efficiency of a car.\nA common loss function for regression problems is the mean absolute error or MAE. For each prediction y_pred, MAE measures the disparity from the true target y_true by an absolute difference abs(y_true - y_pred).\nThe total MAE loss on a dataset is the mean of all these absolute differences.\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/VDcvkZN.png\" width=\"500\" alt=\"A graph depicting error bars from data points to the fitted line..\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>The mean absolute error is the average length between the fitted curve and the data points.\n</center></figcaption>\n</figure>\n\nBesides MAE, other loss functions you might see for regression problems are the mean-squared error (MSE) or the Huber loss (both available in Keras).\nDuring training, the model will use the loss function as a guide for finding the correct values of its weights (lower loss is better). In other words, the loss function tells the network its objective.\nThe Optimizer - Stochastic Gradient Descent\nWe've described the problem we want the network to solve, but now we need to say how to solve it. This is the job of the optimizer. The optimizer is an algorithm that adjusts the weights to minimize the loss.\nVirtually all of the optimization algorithms used in deep learning belong to a family called stochastic gradient descent. They are iterative algorithms that train a network in steps. One step of training goes like this:\n1. Sample some training data and run it through the network to make predictions.\n2. Measure the loss between the predictions and the true values.\n3. Finally, adjust the weights in a direction that makes the loss smaller.\nThen just do this over and over until the loss is as small as you like (or until it won't decrease any further.)\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/rFI1tIk.gif\" width=\"1600\" alt=\"Fitting a line batch by batch. The loss decreases and the weights approach their true values.\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>Training a neural network with Stochastic Gradient Descent.\n</center></figcaption>\n</figure>\n\nEach iteration's sample of training data is called a minibatch (or often just \"batch\"), while a complete round of the training data is called an epoch. The number of epochs you train for is how many times the network will see each training example.\nThe animation shows the linear model from Lesson 1 being trained with SGD. The pale red dots depict the entire training set, while the solid red dots are the minibatches. Every time SGD sees a new minibatch, it will shift the weights (w the slope and b the y-intercept) toward their correct values on that batch. Batch after batch, the line eventually converges to its best fit. You can see that the loss gets smaller as the weights get closer to their true values.\nLearning Rate and Batch Size\nNotice that the line only makes a small shift in the direction of each batch (instead of moving all the way). The size of these shifts is determined by the learning rate. A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values.\nThe learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds. Their interaction is often subtle and the right choice for these parameters isn't always obvious. (We'll explore these effects in the exercise.)\nFortunately, for most work it won't be necessary to do an extensive hyperparameter search to get satisfactory results. Adam is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is \"self tuning\", in a sense). Adam is a great general-purpose optimizer.\nAdding the Loss and Optimizer\nAfter defining a model, you can add a loss function and optimizer with the model's compile method:\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"mae\",\n)\nNotice that we are able to specify the loss and optimizer with just a string. You can also access these directly through the Keras API -- if you wanted to tune parameters, for instance -- but for us, the defaults will work fine.\n<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n    <strong>What's In a Name?</strong><br>\nThe <strong>gradient</strong> is a vector that tells us in what direction the weights need to go. More precisely, it tells us how to change the weights to make the loss change <em>fastest</em>. We call our process gradient <strong>descent</strong> because it uses the gradient to <em>descend</em> the loss curve towards a minimum. <strong>Stochastic</strong> means \"determined by chance.\" Our training is <em>stochastic</em> because the minibatches are <em>random samples</em> from the dataset. And that's why it's called SGD!\n</blockquote>\n\nExample - Red Wine Quality\nNow we know everything we need to start training deep learning models. So let's see it in action! We'll use the Red Wine Quality dataset.\nThis dataset consists of physiochemical measurements from about 1600 Portuguese red wines. Also included is a quality rating for each wine from blind taste-tests. How well can we predict a wine's perceived quality from these measurements?\nWe've put all of the data preparation into this next hidden cell. It's not essential to what follows so feel free to skip it. One thing you might note for now though is that we've rescaled each feature to lie in the interval $[0, 1]$. As we'll discuss more in Lesson 5, neural networks tend to perform best when their inputs are on a common scale.\nEnd of explanation\n\"\"\"\n\n\nprint(X_train.shape)\n\n\"\"\"\nExplanation: How many inputs should this network have? We can discover this by looking at the number of columns in the data matrix. Be sure not to include the target ('quality') here -- only the input features.\nEnd of explanation\n\"\"\"\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(512, activation='relu', input_shape=[11]),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(1),\n])\n\n\"\"\"\nExplanation: Eleven columns means eleven inputs.\nWe've chosen a three-layer network with over 1500 neurons. This network should be capable of learning fairly complex relationships in the data.\nEnd of explanation\n\"\"\"\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='mae',\n)\n\n\"\"\"\nExplanation: Deciding the architecture of your model should be part of a process. Start simple and use the validation loss as your guide. You'll learn more about model development in the exercises.\nAfter defining the model, we compile in the optimizer and loss function.\nEnd of explanation\n\"\"\"\n\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=256,\n    epochs=10,\n)\n\n\"\"\"\nExplanation: Now we're ready to start the training! We've told Keras to feed the optimizer 256 rows of the training data at a time (the batch_size) and to do that 10 times all the way through the dataset (the epochs).\nEnd of explanation\n\"\"\"\n\n\nimport pandas as pd\n\n# convert the training history to a dataframe\nhistory_df = pd.DataFrame(history.history)\n# use Pandas native plot method\nhistory_df['loss'].plot();\n\n\"\"\"\nExplanation: You can see that Keras will keep you updated on the loss as the model trains.\nOften, a better way to view the loss though is to plot it. The fit method in fact keeps a record of the loss produced during training in a History object. We'll convert the data to a Pandas dataframe, which makes the plotting easy.\nEnd of explanation\n\"\"\"\n\nimport time\nfrom datetime import datetime\n\nfrom typing import List\n\nimport numpy as np\nimport pandas as pd\n\nimport google.auth\n\nfrom google.cloud import logging_v2\nfrom google.cloud.monitoring_dashboard.v1 import DashboardsServiceClient\nfrom google.cloud.logging_v2 import MetricsServiceV2Client\nfrom google.cloud.monitoring_v3.query import Query\nfrom google.cloud.monitoring_v3 import MetricServiceClient\n\nimport matplotlib.pyplot as plt\n\n\"\"\"\nExplanation: Analyzing Locust Load Testing Results\nThis Notebook demonstrates how to analyze AI Platform Prediction load testing runs using metrics captured in Cloud Monitoring. \nThis Notebook build on the 02-perf-testing.ipynb notebook that shows how to configure and run load tests against AI Platform Prediction using Locust.io. The outlined testing process results in a Pandas dataframe that aggregates the standard AI Platform Prediction metrics with a set of custom, log-based metrics generated from log entries captured by the Locust testing script.\nThe Notebook covers the following steps:\n1. Retrieve and consolidate test results from Cloud Monitoring\n2. Analyze and visualize utilization and latency results\nSetup\nThis notebook was tested on AI Platform Notebooks using the standard TF 2.2 image.\nImport libraries\nEnd of explanation\n\"\"\"\n\n\nPROJECT_ID = '[your-project-id]' # Set your project Id\nMODEL_NAME = 'resnet_classifier'\nMODEL_VERSION = 'v1'\nLOG_NAME = 'locust' # Set your log name\nTEST_ID = 'test-20200829-190943' # Set your test Id\nTEST_START_TIME = datetime.fromisoformat('2020-08-28T21:30:00-00:00') # Set your test start time\nTEST_END_TIME = datetime.fromisoformat('2020-08-29T22:00:00-00:00') # Set your test end time\n\n\n\"\"\"\nExplanation: Configure GCP environment settings\nEnd of explanation\n\"\"\"\n\n\ncreds , _ = google.auth.default()\nclient = MetricServiceClient(credentials=creds)\n\nproject_path = client.project_path(PROJECT_ID)\nfilter = 'metric.type=starts_with(\"ml.googleapis.com/prediction\")'\n\nfor descriptor in client.list_metric_descriptors(project_path, filter_=filter):\n    print(descriptor.type)\n\n\"\"\"\nExplanation: 1. Retrieve and consolidate test results\nLocust's web interface along with a Cloud Monitoring dashboard provide a cursory view into performance of a tested AI Platform Prediction model version. A more thorough analysis can be performed by consolidating metrics collected during a test and using data analytics and visualization tools.\nIn this section, you will retrieve the metrics captured in Cloud Monitoring and consolidate them into a single Pandas dataframe.\n1.1 List available AI Platform Prediction metrics\nEnd of explanation\n\"\"\"\n\n\nfilter = 'metric.type=starts_with(\"logging.googleapis.com/user\")'\n\nfor descriptor in client.list_metric_descriptors(project_path, filter_=filter):\n    print(descriptor.type)\n\n\"\"\"\nExplanation: 1.2. List custom log based metrics\nEnd of explanation\n\"\"\"\n\n\ndef retrieve_metrics(client, project_id, start_time, end_time, model, model_version, test_id, log_name):\n    \"\"\"\n    Retrieves test metrics from Cloud Monitoring.\n    \"\"\"\n    def _get_aipp_metric(metric_type: str, labels: List[str]=[], metric_name=None)-> pd.DataFrame:\n        \"\"\"\n        Retrieves a specified AIPP metric.\n        \"\"\"\n        query = Query(client, project_id, metric_type=metric_type)\n        query = query.select_interval(end_time, start_time)\n        query = query.select_resources(model_id=model)\n        query = query.select_resources(version_id=model_version)\n        \n        if metric_name:\n            labels = ['metric'] + labels \n        df = query.as_dataframe(labels=labels)\n        \n        if not df.empty:\n            if metric_name:\n                df.columns.set_levels([metric_name], level=0, inplace=True)\n            df = df.set_index(df.index.round('T'))\n        \n        return df\n    \n    def _get_locust_metric(metric_type: str, labels: List[str]=[], metric_name=None)-> pd.DataFrame:\n        \"\"\"\n        Retrieves a specified custom log-based metric.\n        \"\"\"\n        query = Query(client, project_id, metric_type=metric_type)\n        query = query.select_interval(end_time, start_time)\n        query = query.select_metrics(log=log_name)\n        query = query.select_metrics(test_id=test_id)\n        \n        if metric_name:\n            labels = ['metric'] + labels \n        df = query.as_dataframe(labels=labels)\n        \n        if not df.empty: \n            if metric_name:\n                df.columns.set_levels([metric_name], level=0, inplace=True)\n            df = df.apply(lambda row: [metric.mean for metric in row])\n            df = df.set_index(df.index.round('T'))\n        \n        return df\n    \n    # Retrieve GPU duty cycle\n    metric_type = 'ml.googleapis.com/prediction/online/accelerator/duty_cycle'\n    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'duty_cycle')\n    df = metric\n\n    # Retrieve CPU utilization\n    metric_type = 'ml.googleapis.com/prediction/online/cpu/utilization'\n    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'cpu_utilization')\n    if not metric.empty:\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve prediction count\n    metric_type = 'ml.googleapis.com/prediction/prediction_count'\n    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'prediction_count')\n    if not metric.empty:\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve responses per second\n    metric_type = 'ml.googleapis.com/prediction/response_count'\n    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'response_rate')\n    if not metric.empty:\n        metric = (metric/60).round(2)\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve backend latencies\n    metric_type = 'ml.googleapis.com/prediction/latencies'\n    metric = _get_aipp_metric(metric_type, ['latency_type', 'replica_id', 'signature'])\n    if not metric.empty:\n        metric = metric.apply(lambda row: [round(latency.mean/1000,1) for latency in row])\n        metric.columns.set_names(['metric', 'replica_id', 'signature'], inplace=True)\n        level_values = ['Latency: ' + value for value in metric.columns.get_level_values(level=0)]\n        metric.columns.set_levels(level_values, level=0, inplace=True)\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve Locust latency\n    metric_type = 'logging.googleapis.com/user/locust_latency'\n    metric = _get_locust_metric(metric_type, ['replica_id', 'signature'], 'Latency: client')\n    if not metric.empty:\n        metric = metric.round(2).replace([0], np.nan)\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve Locust user count\n    metric_",
  "64k": "import numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\"\"\"\nExplanation: Simple MNIST convnet\nAuthor: fchollet<br>\nDate created: 2015/06/19<br>\nLast modified: 2020/04/21<br>\nDescription: A simple convnet that achieves ~99% test accuracy on MNIST.\nSetup\nEnd of explanation\n\"\"\"\n\n\n# Model / data parameters\nnum_classes = 10\ninput_shape = (28, 28, 1)\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") / 255\nx_test = x_test.astype(\"float32\") / 255\n# Make sure images have shape (28, 28, 1)\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\nprint(\"x_train shape:\", x_train.shape)\nprint(x_train.shape[0], \"train samples\")\nprint(x_test.shape[0], \"test samples\")\n\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\"\"\"\nExplanation: Prepare the data\nEnd of explanation\n\"\"\"\n\n\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Flatten(),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation=\"softmax\"),\n    ]\n)\n\nmodel.summary()\n\n\"\"\"\nExplanation: Build the model\nEnd of explanation\n\"\"\"\n\n\nbatch_size = 128\nepochs = 15\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n\n\"\"\"\nExplanation: Train the model\nEnd of explanation\n\"\"\"\n\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])\n\n\"\"\"\nExplanation: Evaluate the trained model\nEnd of explanation\n\"\"\"\n\n#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nExplanation: Copyright 2021 The TensorFlow Authors.\nEnd of explanation\n\"\"\"\n\n\nimport sys\nif 'google.colab' in sys.modules:\n  !pip install --upgrade pip\n\n\"\"\"\nExplanation: TFX Keras コンポーネントのチュートリアル\nTensorFlow Extended (TFX) の各コンポーネントの紹介\n注：この例は、Jupyter スタイルのノートブックで今すぐ実行できます。セットアップは必要ありません。「Google Colab で実行」をクリックするだけです\n<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n<td><a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/tfx/components_keras\"> <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a></td>\n<td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a></td>\n<td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n<td><a target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n</table></div>\n\nこの Colab ベースのチュートリアルでは、TensorFlow Extended (TFX) のそれぞれの組み込みコンポーネントをインタラクティブに説明します。\nここではデータの取り込みからモデルのプッシュ、サービングまで、エンド ツー エンドの機械学習パイプラインのすべてのステップを見ていきます。\n完了したら、このノートブックのコンテンツを TFX パイプライン ソース コードとして自動的にエクスポートできます。これは、Apache Airflow および Apache Beam とオーケストレーションできます。\n注意: このノートブックは、TFX パイプラインでのネイティブ Keras モデルの使用を示しています。TFX は TensorFlow 2 バージョンの Keras のみをサポートします。\n背景情報\nこのノートブックは、Jupyter/Colab 環境で TFX を使用する方法を示しています。 ここでは、インタラクティブなノートブックでシカゴのタクシーの例を見ていきます。\nTFX パイプラインの構造に慣れるのには、インタラクティブなノートブックで作業するのが便利です。独自のパイプラインを軽量の開発環境として開発する場合にも役立ちますが、インタラクティブ ノートブックのオーケストレーションとメタデータ アーティファクトへのアクセス方法には違いがあるので注意してください。\nオーケストレーション\nTFX の実稼働デプロイメントでは、Apache Airflow、Kubeflow Pipelines、Apache Beam などのオーケストレーターを使用して、TFX コンポーネントの事前定義済みパイプライン グラフをオーケストレーションします。インタラクティブなノートブックでは、ノートブック自体がオーケストレーターであり、ノートブック セルを実行するときにそれぞれの TFX コンポーネントを実行します。\nメタデータ\nTFX の実稼働デプロイメントでは、ML Metadata（MLMD）API を介してメタデータにアクセスします。MLMD は、メタデータ プロパティを MySQL や SQLite などのデータベースに格納し、メタデータ ペイロードをファイル システムなどの永続ストアに保存します。インタラクティブなノートブックでは、プロパティとペイロードの両方が、Jupyter ノートブックまたは Colab サーバーの /tmp ディレクトリにあるエフェメラル SQLite データベースに保存されます。\nセットアップ\nまず、必要なパッケージをインストールしてインポートし、パスを設定して、データをダウンロードします。\nPip のアップグレード\nローカルで実行する場合にシステム Pipをアップグレードしないようにするには、Colab で実行していることを確認してください。もちろん、ローカルシステムは個別にアップグレードできます。\nEnd of explanation\n\"\"\"\n\n\n!pip install -U tfx\n\n\"\"\"\nExplanation: TFX をインストールする\n注：Google Colab では、パッケージが更新されるため、このセルを初めて実行するときに、ランタイムを再起動する必要があります（[ランタイム]&gt; [ランタイムの再起動...]）。\nEnd of explanation\n\"\"\"\n\n\nimport os\nimport pprint\nimport tempfile\nimport urllib\n\nimport absl\nimport tensorflow as tf\nimport tensorflow_model_analysis as tfma\ntf.get_logger().propagate = False\npp = pprint.PrettyPrinter()\n\nfrom tfx import v1 as tfx\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n\n%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip\n\n\"\"\"\nExplanation: ランタイムを再起動しましたか？\nGoogle Colab を使用している場合は、上記のセルを初めて実行するときにランタイムを再起動する必要があります（[ランタイム]&gt; [ランタイムの再起動...]）。 これは、Colab がパッケージを読み込むために必要ですです。\nパッケージをインポートする\n標準の TFX コンポーネント クラスを含む必要なパッケージをインポートします。\nEnd of explanation\n\"\"\"\n\n\nprint('TensorFlow version: {}'.format(tf.__version__))\nprint('TFX version: {}'.format(tfx.__version__))\n\n\"\"\"\nExplanation: ライブラリのバージョンを確認します。\nEnd of explanation\n\"\"\"\n\n\n# This is the root directory for your TFX pip package installation.\n_tfx_root = tfx.__path__[0]\n\n# This is the directory containing the TFX Chicago Taxi Pipeline example.\n_taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')\n\n# This is the path where your model will be pushed for serving.\n_serving_model_dir = os.path.join(\n    tempfile.mkdtemp(), 'serving_model/taxi_simple')\n\n# Set up logging.\nabsl.logging.set_verbosity(absl.logging.INFO)\n\n\"\"\"\nExplanation: パイプライン パスを設定\nEnd of explanation\n\"\"\"\n\n\n_data_root = tempfile.mkdtemp(prefix='tfx-data')\nDATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv'\n_data_filepath = os.path.join(_data_root, \"data.csv\")\nurllib.request.urlretrieve(DATA_PATH, _data_filepath)\n\n\"\"\"\nExplanation: サンプルデータのダウンロード\nTFX パイプラインで使用するサンプル データセットをダウンロードします。\n使用しているデータセットは、シカゴ市がリリースした タクシートリップデータセットです。 このデータセットの列は次のとおりです。\n<table>\n<tr>\n<td>pickup_community_area</td>\n<td>fare</td>\n<td>trip_start_month</td>\n</tr>\n<tr>\n<td>trip_start_hour</td>\n<td>trip_start_day</td>\n<td>trip_start_timestamp</td>\n</tr>\n<tr>\n<td>pickup_latitude</td>\n<td>pickup_longitude</td>\n<td>dropoff_latitude</td>\n</tr>\n<tr>\n<td>dropoff_longitude</td>\n<td>trip_miles</td>\n<td>pickup_census_tract</td>\n</tr>\n<tr>\n<td>dropoff_census_tract</td>\n<td>payment_type</td>\n<td>company</td>\n</tr>\n<tr>\n<td>trip_seconds</td>\n<td>dropoff_community_area</td>\n<td>tips</td>\n</tr>\n</table>\n\nこのデータセットを使用して、タクシー乗車のtipsを予測するモデルを構築します。\nEnd of explanation\n\"\"\"\n\n\n!head {_data_filepath}\n\n\"\"\"\nExplanation: CSV ファイルを見てみましょう。\nEnd of explanation\n\"\"\"\n\n\n# Here, we create an InteractiveContext using default parameters. This will\n# use a temporary directory with an ephemeral ML Metadata database instance.\n# To use your own pipeline root or database, the optional properties\n# `pipeline_root` and `metadata_connection_config` may be passed to\n# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n# notebook.\ncontext = InteractiveContext()\n\n\"\"\"\nExplanation: 注：このWeb サイトは、シカゴ市の公式 Web サイト www.cityofchicago.org で公開されたデータを変更して使用するアプリケーションを提供します。シカゴ市は、この Web サイトで提供されるデータの内容、正確性、適時性、または完全性について一切の表明を行いません。この Web サイトで提供されるデータは、いつでも変更される可能性があります。かかる Web サイトで提供されるデータはユーザーの自己責任で利用されるものとします。\nInteractiveContext を作成する\n最後に、このノートブックで TFX コンポーネントをインタラクティブに実行できるようにする InteractiveContext を作成します。\nEnd of explanation\n\"\"\"\n\n\nexample_gen = tfx.components.CsvExampleGen(input_base=_data_root)\ncontext.run(example_gen, enable_cache=True)\n\n\"\"\"\nExplanation: TFX コンポーネントをインタラクティブに実行する\n次のセルでは、TFX コンポーネントを 1 つずつ作成し、それぞれを実行して、出力アーティファクトを視覚化します。\nExampleGen\nExampleGen コンポーネントは通常、TFX パイプラインの先頭にあり、以下を実行します。\n\nデータをトレーニング セットと評価セットに分割します (デフォルトでは、2/3 トレーニング + 1/3 評価)。\nデータを tf.Example 形式に変換します。 (詳細はこちら)\n他のコンポーネントがアクセスできるように、データを _tfx_root ディレクトリにコピーします。\n\nExampleGen は、データソースへのパスを入力として受け取ります。 ここでは、これはダウンロードした CSV を含む _data_root パスです。\n注意: このノートブックでは、コンポーネントを 1 つずつインスタンス化し、InteractiveContext.run() で実行しますが、実稼働環境では、すべてのコンポーネントを事前に Pipelineで指定して、オーケストレーターに渡します（TFX パイプライン ガイドの構築を参照してください）。\nキャッシュを有効にする\nノートブックで InteractiveContext を使用してパイプラインを作成している場合、個別のコンポーネントが出力をキャッシュするタイミングを制御することができます。コンポーネントが前に生成した出力アーティファクトを再利用する場合は、enable_cache を True に設定します。コードを変更するなどにより、コンポーネントの出力アーティファクトを再計算する場合は、enable_cache を False に設定します。\nEnd of explanation\n\"\"\"\n\n\nartifact = example_gen.outputs['examples'].get()[0]\nprint(artifact.split_names, artifact.uri)\n\n\"\"\"\nExplanation: ExampleGenの出力アーティファクトを調べてみましょう。このコンポーネントは、トレーニングサンプルと評価サンプルの 2 つのアーティファクトを生成します。\nEnd of explanation\n\"\"\"\n\n\n# Get the URI of the output artifact representing the training examples, which is a directory\ntrain_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')\n\n# Get the list of files in this directory (all compressed TFRecord files)\ntfrecord_filenames = [os.path.join(train_uri, name)\n                      for name in os.listdir(train_uri)]\n\n# Create a `TFRecordDataset` to read these files\ndataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n\n# Iterate over the first 3 records and decode them.\nfor tfrecord in dataset.take(3):\n  serialized_example = tfrecord.numpy()\n  example = tf.train.Example()\n  example.ParseFromString(serialized_example)\n  pp.pprint(example)\n\n\"\"\"\nExplanation: また、最初の 3 つのトレーニングサンプルも見てみます。\nEnd of explanation\n\"\"\"\n\n\nstatistics_gen = tfx.components.StatisticsGen(\n    examples=example_gen.outputs['examples'])\ncontext.run(statistics_gen, enable_cache=True)\n\n\"\"\"\nExplanation: ExampleGenがデータの取り込みを完了したので、次のステップ、データ分析に進みます。\nStatisticsGen\nStatisticsGenコンポーネントは、データ分析用のデータセットの統計を計算し、ダウンストリームのコンポーネントで使用します。これは、TensorFlow Data Validation ライブラリを使用します。\nStatisticsGenコンポーネントは、データ分析用のデータセットの統計を計算し、ダウンストリーム コンポーネントで使用します。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(statistics_gen.outputs['statistics'])\n\n\"\"\"\nExplanation: StatisticsGen の実行が完了すると、出力された統計を視覚化できます。 色々なプロットを試してみてください！\nEnd of explanation\n\"\"\"\n\n\nschema_gen = tfx.components.SchemaGen(\n    statistics=statistics_gen.outputs['statistics'],\n    infer_feature_shape=False)\ncontext.run(schema_gen, enable_cache=True)\n\n\"\"\"\nExplanation: SchemaGen\nSchemaGen コンポーネントは、データ統計に基づいてスキーマを生成します。（スキーマは、データセット内の特徴の予想される境界、タイプ、プロパティを定義します。）また、TensorFlow データ検証ライブラリも使用します。\n注意: 生成されたスキーマはベストエフォートのもので、データの基本的なプロパティだけを推論しようとします。確認し、必要に応じて修正する必要があります。\nSchemaGen は、StatisticsGen で生成した統計を入力として受け取り、デフォルトでトレーニング分割を参照します。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(schema_gen.outputs['schema'])\n\n\"\"\"\nExplanation: SchemaGen の実行が完了すると、生成されたスキーマをテーブルとして視覚化できます。\nEnd of explanation\n\"\"\"\n\n\nexample_validator = tfx.components.ExampleValidator(\n    statistics=statistics_gen.outputs['statistics'],\n    schema=schema_gen.outputs['schema'])\ncontext.run(example_validator, enable_cache=True)\n\n\"\"\"\nExplanation: データセットのそれぞれの特徴は、スキーマ テーブルのプロパティの横に行として表示されます。スキーマは、ドメインとして示される、カテゴリ特徴が取るすべての値もキャプチャします。\nスキーマの詳細については、SchemaGen のドキュメントをご覧ください。\nExampleValidator\nExampleValidator コンポーネントは、スキーマで定義された期待に基づいて、データの異常を検出します。また、TensorFlow Data Validation ライブラリも使用します。\nExampleValidator は、Statistics Gen{/code 1} からの統計と &lt;code data-md-type=\"codespan\"&gt;SchemaGen からのスキーマを入力として受け取ります。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(example_validator.outputs['anomalies'])\n\n\"\"\"\nExplanation: ExampleValidator の実行が完了すると、異常をテーブルとして視覚化できます。\nEnd of explanation\n\"\"\"\n\n\n_taxi_constants_module_file = 'taxi_constants.py'\n\n%%writefile {_taxi_constants_module_file}\n\nNUMERICAL_FEATURES = ['trip_miles', 'fare', 'trip_seconds']\n\nBUCKET_FEATURES = [\n    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n    'dropoff_longitude'\n]\n# Number of buckets used by tf.transform for encoding each feature.\nFEATURE_BUCKET_COUNT = 10\n\nCATEGORICAL_NUMERICAL_FEATURES = [\n    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n    'dropoff_community_area'\n]\n\nCATEGORICAL_STRING_FEATURES = [\n    'payment_type',\n    'company',\n]\n\n# Number of vocabulary terms used for encoding categorical features.\nVOCAB_SIZE = 1000\n\n# Count of out-of-vocab buckets in which unrecognized categorical are hashed.\nOOV_SIZE = 10\n\n# Keys\nLABEL_KEY = 'tips'\nFARE_KEY = 'fare'\n\ndef t_name(key):\n  \"\"\"\n  Rename the feature keys so that they don't clash with the raw keys when\n  running the Evaluator component.\n  Args:\n    key: The original feature key\n  Returns:\n    key with '_xf' appended\n  \"\"\"\n  return key + '_xf'\n\n\"\"\"\nExplanation: 異常テーブルでは、異常がないことがわかります。これは、分析した最初のデータセットで、スキーマはこれに合わせて調整されているため、異常がないことが予想されます。このスキーマを確認する必要があります。予期されないものは、データに異常があることを意味します。確認されたスキーマを使用して将来のデータを保護できます。ここで生成された異常は、モデルのパフォーマンスをデバッグし、データが時間の経過とともにどのように変化するかを理解し、データ エラーを特定するために使用できます。\n変換\nTransformコンポーネントは、トレーニングとサービングの両方で特徴量エンジニアリングを実行します。これは、 TensorFlow Transform ライブラリを使用します。\nTransformは、ExampleGenからのデータ、SchemaGenからのスキーマ、ユーザー定義の Transform コードを含むモジュールを入力として受け取ります。\n以下のユーザー定義の Transform コードの例を見てみましょう（TensorFlow Transform API の概要については、チュートリアルを参照してください）。まず、特徴量エンジニアリングのいくつかの定数を定義します。\n注意: %%writefile セル マジックは、セルの内容をディスク上の.pyファイルとして保存します。これにより、Transform コンポーネントはコードをモジュールとして読み込むことができます。\nEnd of explanation\n\"\"\"\n\n\n_taxi_transform_module_file = 'taxi_transform.py'\n\n%%writefile {_taxi_transform_module_file}\n\nimport tensorflow as tf\nimport tensorflow_transform as tft\n\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimport taxi_constants\nimport sys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  import importlib\n  importlib.reload(taxi_constants)\n\n_NUMERICAL_FEATURES = taxi_constants.NUMERICAL_FEATURES\n_BUCKET_FEATURES = taxi_constants.BUCKET_FEATURES\n_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\n_CATEGORICAL_NUMERICAL_FEATURES = taxi_constants.CATEGORICAL_NUMERICAL_FEATURES\n_CATEGORICAL_STRING_FEATURES = taxi_constants.CATEGORICAL_STRING_FEATURES\n_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\n_OOV_SIZE = taxi_constants.OOV_SIZE\n_FARE_KEY = taxi_constants.FARE_KEY\n_LABEL_KEY = taxi_constants.LABEL_KEY\n\n\ndef _make_one_hot(x, key):\n  \"\"\"Make a one-hot tensor to encode categorical features.\n  Args:\n    X: A dense tensor\n    key: A string key for the feature in the input\n  Returns:\n    A dense one-hot tensor as a float list\n  \"\"\"\n  integerized = tft.compute_and_apply_vocabulary(x,\n          top_k=_VOCAB_SIZE,\n          num_oov_buckets=_OOV_SIZE,\n          vocab_filename=key, name=key)\n  depth = (\n      tft.experimental.get_vocabulary_size_by_name(key) + _OOV_SIZE)\n  one_hot_encoded = tf.one_hot(\n      integerized,\n      depth=tf.cast(depth, tf.int32),\n      on_value=1.0,\n      off_value=0.0)\n  return tf.reshape(one_hot_encoded, [-1, depth])\n\n\ndef _fill_in_missing(x):\n  \"\"\"Replace missing values in a SparseTensor.\n  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n  Args:\n    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n      in the second dimension.\n  Returns:\n    A rank 1 tensor where missing values of `x` have been filled in.\n  \"\"\"\n  if not isinstance(x, tf.sparse.SparseTensor):\n    return x\n\n  default_value = '' if x.dtype == tf.string else 0\n  return tf.squeeze(\n      tf.sparse.to_dense(\n          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n          default_value),\n      axis=1)\n\n\ndef preprocessing_fn(inputs):\n  \"\"\"tf.transform's callback function for preprocessing inputs.\n  Args:\n    inputs: map from feature keys to raw not-yet-transformed features.\n  Returns:\n    Map from string feature key to transformed feature operations.\n  \"\"\"\n  outputs = {}\n  for key in _NUMERICAL_FEATURES:\n    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.\n    outputs[taxi_constants.t_name(key)] = tft.scale_to_z_score(\n        _fill_in_missing(inputs[key]), name=key)\n\n  for key in _BUCKET_FEATURES:\n    outputs[taxi_constants.t_name(key)] = tf.cast(tft.bucketize(\n            _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT, name=key),\n            dtype=tf.float32)\n\n  for key in _CATEGORICAL_STRING_FEATURES:\n    outputs[taxi_constants.t_name(key)] = _make_one_hot(_fill_in_missing(inputs[key]), key)\n\n  for key in _CATEGORICAL_NUMERICAL_FEATURES:\n    outputs[taxi_constants.t_name(key)] = _make_one_hot(tf.strings.strip(\n        tf.strings.as_string(_fill_in_missing(inputs[key]))), key)\n\n  # Was this passenger a big tipper?\n  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])\n  tips = _fill_in_missing(inputs[_LABEL_KEY])\n  outputs[_LABEL_KEY] = tf.where(\n      tf.math.is_nan(taxi_fare),\n      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\n      # Test if the tip was > 20% of the fare.\n      tf.cast(\n          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\n\n  return outputs\n\n\"\"\"\nExplanation: 次に、生データを入力として受け取り、モデルがトレーニングできる変換された特徴量を返す {code 0}preprocessing _fn を記述します。\nEnd of explanation\n\"\"\"\n\n\ntransform = tfx.components.Transform(\n    examples=example_gen.outputs['examples'],\n    schema=schema_gen.outputs['schema'],\n    module_file=os.path.abspath(_taxi_transform_module_file))\ncontext.run(transform, enable_cache=True)\n\n\"\"\"\nExplanation: 次に、この特徴量エンジニアリング コードを Transformコンポーネントに渡し、実行してデータを変換します。\nEnd of explanation\n\"\"\"\n\n\ntransform.outputs\n\n\"\"\"\nExplanation: Transformの出力アーティファクトを調べてみましょう。このコンポーネントは、2 種類の出力を生成します。\n\ntransform_graph は、前処理演算を実行できるグラフです (このグラフは、サービングモデルと評価モデルに含まれます)。\ntransformed_examplesは前処理されたトレーニングおよび評価データを表します。\nEnd of explanation\n\"\"\"\n\n\ntrain_uri = transform.outputs['transform_graph'].get()[0].uri\nos.listdir(train_uri)\n\n\"\"\"\nExplanation: transform_graph アーティファクトを見てみましょう。これは、3 つのサブディレクトリを含むディレクトリを指しています。\nEnd of explanation\n\"\"\"\n\n\n# Get the URI of the output artifact representing the transformed examples, which is a directory\ntrain_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n\n# Get the list of files in this directory (all compressed TFRecord files)\ntfrecord_filenames = [os.path.join(train_uri, name)\n                      for name in os.listdir(train_uri)]\n\n# Create a `TFRecordDataset` to read these files\ndataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n\n# Iterate over the first 3 records and decode them.\nfor tfrecord in dataset.take(3):\n  serialized_example = tfrecord.numpy()\n  example = tf.train.Example()\n  example.ParseFromString(serialized_example)\n  pp.pprint(example)\n\n\"\"\"\nExplanation: transformed_metadata サブディレクトリには、前処理されたデータのスキーマが含まれています。transform_fnサブディレクトリには、実際の前処理グラフが含まれています。metadataサブディレクトリには、元のデータのスキーマが含まれています。\nまた、最初の 3 つの変換された例も見てみます。\nEnd of explanation\n\"\"\"\n\n\n_taxi_trainer_module_file = 'taxi_trainer.py'\n\n%%writefile {_taxi_trainer_module_file}\n\nfrom typing import Dict, List, Text\n\nimport os\nimport glob\nfrom absl import logging\n\nimport datetime\nimport tensorflow as tf\nimport tensorflow_transform as tft\n\nfrom tfx import v1 as tfx\nfrom tfx_bsl.public import tfxio\nfrom tensorflow_transform import TFTransformOutput\n\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimport taxi_constants\nimport sys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  import importlib\n  importlib.reload(taxi_constants)\n\n_LABEL_KEY = taxi_constants.LABEL_KEY\n\n_BATCH_SIZE = 40\n\n\ndef _input_fn(file_pattern: List[Text],\n              data_accessor: tfx.components.DataAccessor,\n              tf_transform_output: tft.TFTransformOutput,\n              batch_size: int = 200) -> tf.data.Dataset:\n  \"\"\"Generates features and label for tuning/training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    tf_transform_output: A TFTransformOutput.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      tf_transform_output.transformed_metadata.schema)\n\ndef _get_tf_examples_serving_signature(model, tf_transform_output):\n  \"\"\"Returns a serving signature that accepts `tensorflow.Example`.\"\"\"\n\n  # We need to track the layers in the model in order to save it.\n  # TODO(b/162357359): Revise once the bug is resolved.\n  model.tft_layer_inference = tf_transform_output.transform_features_layer()\n\n  @tf.function(input_signature=[\n      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n  ])\n  def serve_tf_examples_fn(serialized_tf_example):\n    \"\"\"Returns the output to be used in the serving signature.\"\"\"\n    raw_feature_spec = tf_transform_output.raw_feature_spec()\n    # Remove label feature since these will not be present at serving time.\n    raw_feature_spec.pop(_LABEL_KEY)\n    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n    transformed_features = model.tft_layer_inference(raw_features)\n    logging.info('serve_transformed_features = %s', transformed_features)\n\n    outputs = model(transformed_features)\n    # TODO(b/154085620): Convert the predicted labels from the model using a\n    # reverse-lookup (opposite of transform.py).\n    return {'outputs': outputs}\n\n  return serve_tf_examples_fn\n\n\ndef _get_transform_features_signature(model, tf_transform_output):\n  \"\"\"Returns a serving signature that applies tf.Transform to features.\"\"\"\n\n  # We need to track the layers in the model in order to save it.\n  # TODO(b/162357359): Revise once the bug is resolved.\n  model.tft_layer_eval = tf_transform_output.transform_features_layer()\n\n  @tf.function(input_signature=[\n      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n  ])\n  def transform_features_fn(serialized_tf_example):\n    \"\"\"Returns the transformed_features to be fed as input to evaluator.\"\"\"\n    raw_feature_spec = tf_transform_output.raw_feature_spec()\n    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n    transformed_features = model.tft_layer_eval(raw_features)\n    logging.info('eval_transformed_features = %s', transformed_features)\n    return transformed_features\n\n  return transform_features_fn\n\n\ndef export_serving_model(tf_transform_output, model, output_dir):\n  \"\"\"Exports a keras model for serving.\n  Args:\n    tf_transform_output: Wrapper around output of tf.Transform.\n    model: A keras model to export for serving.\n    output_dir: A directory where the model will be exported to.\n  \"\"\"\n  # The layer has to be saved to the model for keras tracking purpases.\n  model.tft_layer = tf_transform_output.transform_features_layer()\n\n  signatures = {\n      'serving_default':\n          _get_tf_examples_serving_signature(model, tf_transform_output),\n      'transform_features':\n          _get_transform_features_signature(model, tf_transform_output),\n  }\n\n  model.save(output_dir, save_format='tf', signatures=signatures)\n\n\ndef _build_keras_model(tf_transform_output: TFTransformOutput\n                       ) -> tf.keras.Model:\n  \"\"\"Creates a DNN Keras model for classifying taxi data.\n\n  Args:\n    tf_transform_output: [TFTransformOutput], the outputs from Transform\n\n  Returns:\n    A keras Model.\n  \"\"\"\n  feature_spec = tf_transform_output.transformed_feature_spec().copy()\n  feature_spec.pop(_LABEL_KEY)\n\n  inputs = {}\n  for key, spec in feature_spec.items():\n    if isinstance(spec, tf.io.VarLenFeature):\n      inputs[key] = tf.keras.layers.Input(\n          shape=[None], name=key, dtype=spec.dtype, sparse=True)\n    elif isinstance(spec, tf.io.FixedLenFeature):\n      # TODO(b/208879020): Move into schema such that spec.shape is [1] and not\n      # [] for scalars.\n      inputs[key] = tf.keras.layers.Input(\n          shape=spec.shape or [1], name=key, dtype=spec.dtype)\n    else:\n      raise ValueError('Spec type is not supported: ', key, spec)\n  \n  output = tf.keras.layers.Concatenate()(tf.nest.flatten(inputs))\n  output = tf.keras.layers.Dense(100, activation='relu')(output)\n  output = tf.keras.layers.Dense(70, activation='relu')(output)\n  output = tf.keras.layers.Dense(50, activation='relu')(output)\n  output = tf.keras.layers.Dense(20, activation='relu')(output)\n  output = tf.keras.layers.Dense(1)(output)\n  return tf.keras.Model(inputs=inputs, outputs=output)\n\n\n# TFX Trainer will call this function.\ndef run_fn(fn_args: tfx.components.FnArgs):\n  \"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n\n  train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor, \n                            tf_transform_output, _BATCH_SIZE)\n  eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor, \n                           tf_transform_output, _BATCH_SIZE)\n\n  model = _build_keras_model(tf_transform_output)\n\n  model.compile(\n      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n      metrics=[tf.keras.metrics.BinaryAccuracy()])\n\n  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n      log_dir=fn_args.model_run_dir, update_freq='batch')\n\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps,\n      callbacks=[tensorboard_callback])\n\n  # Export the model.\n  export_serving_model(tf_transform_output, model, fn_args.serving_model_dir)\n\n\"\"\"\nExplanation: Transformコンポーネントがデータを特徴量に変換したら、次にモデルをトレーニングします。\nトレーナー\nTrainerコンポーネントは、TensorFlow で定義したモデルをトレーニングします。デフォルトでは、Trainer は Estimator API をサポートします。Keras API を使用するには、トレーナーのコンストラクターでcustom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor)をセットアップして Generic Trainer を指定する必要があります。\nTrainer は、SchemaGenからのスキーマ、Transformからの変換されたデータとグラフ、トレーニング パラメータ、およびユーザー定義されたモデル コードを含むモジュールを入力として受け取ります。\n以下のユーザー定義モデル コードの例を見てみましょう（TensorFlow Keras API の概要については、チュートリアルを参照してください）。\nEnd of explanation\n\"\"\"\n\n\ntrainer = tfx.components.Trainer(\n    module_file=os.path.abspath(_taxi_trainer_module_file),\n    examples=transform.outputs['transformed_examples'],\n    transform_graph=transform.outputs['transform_graph'],\n    schema=schema_gen.outputs['schema'],\n    train_args=tfx.proto.TrainArgs(num_steps=10000),\n    eval_args=tfx.proto.EvalArgs(num_steps=5000))\ncontext.run(trainer, enable_cache=True)\n\n\"\"\"\nExplanation: 次に、このモデル コードをTrainerコンポーネントに渡し、それを実行してモデルをトレーニングします。\nEnd of explanation\n\"\"\"\n\n\nmodel_artifact_dir = trainer.outputs['model'].get()[0].uri\npp.pprint(os.listdir(model_artifact_dir))\nmodel_dir = os.path.join(model_artifact_dir, 'Format-Serving')\npp.pprint(os.listdir(model_dir))\n\n\"\"\"\nExplanation: TensorBoard でトレーニングを分析する\nトレーナーのアーティファクトを見てみましょう。これはモデルのサブディレクトリを含むディレクトリを指しています。\nEnd of explanation\n\"\"\"\n\n\nmodel_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri\n\n%load_ext tensorboard\n%tensorboard --logdir {model_run_artifact_dir}\n\n\"\"\"\nExplanation: オプションで、TensorBoard を Trainer に接続して、モデルの学習曲線を分析できます。\nEnd of explanation\n\"\"\"\n\n\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimport taxi_constants\nimport sys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  import importlib\n  importlib.reload(taxi_constants)\n\neval_config = tfma.EvalConfig(\n    model_specs=[\n        # This assumes a serving model with signature 'serving_default'. If\n        # using estimator based EvalSavedModel, add signature_name: 'eval' and\n        # remove the label_key.\n        tfma.ModelSpec(\n            signature_name='serving_default',\n            label_key=taxi_constants.LABEL_KEY,\n            preprocessing_function_names=['transform_features'],\n            )\n        ],\n    metrics_specs=[\n        tfma.MetricsSpec(\n            # The metrics added here are in addition to those saved with the\n            # model (assuming either a keras model or EvalSavedModel is used).\n            # Any metrics added into the saved model (for example using\n            # model.compile(..., metrics=[...]), etc) will be computed\n            # automatically.\n            # To add validation thresholds for metrics saved with the model,\n            # add them keyed by metric name to the thresholds map.\n            metrics=[\n                tfma.MetricConfig(class_name='ExampleCount'),\n                tfma.MetricConfig(class_name='BinaryAccuracy',\n                  threshold=tfma.MetricThreshold(\n                      value_threshold=tfma.GenericValueThreshold(\n                          lower_bound={'value': 0.5}),\n                      # Change threshold will be ignored if there is no\n                      # baseline model resolved from MLMD (first run).\n                      change_threshold=tfma.GenericChangeThreshold(\n                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n                          absolute={'value': -1e-10})))\n            ]\n        )\n    ],\n    slicing_specs=[\n        # An empty slice spec means the overall slice, i.e. the whole dataset.\n        tfma.SlicingSpec(),\n        # Data can be sliced along a feature column. In this case, data is\n        # sliced along feature column trip_start_hour.\n        tfma.SlicingSpec(\n            feature_keys=['trip_start_hour'])\n    ])\n\n\"\"\"\nExplanation: Evaluator\nEvaluator コンポーネントは、評価セットに対してモデル パフォーマンス指標を計算します。TensorFlow Model Analysisライブラリを使用します。Evaluatorは、オプションで、新しくトレーニングされたモデルが以前のモデルよりも優れていることを検証できます。これは、モデルを毎日自動的にトレーニングおよび検証する実稼働環境のパイプライン設定で役立ちます。このノートブックでは 1 つのモデルのみをトレーニングするため、Evaluatorはモデルに自動的に「good」というラベルを付けます。\nEvaluatorは、ExampleGenからのデータ、Trainerからのトレーニング済みモデル、およびスライス構成を入力として受け取ります。スライス構成により、特徴値に関する指標をスライスすることができます (たとえば、午前 8 時から午後 8 時までのタクシー乗車でモデルがどのように動作するかなど)。 この構成の例は、以下を参照してください。\nEnd of explanation\n\"\"\"\n\n\n# Use TFMA to compute a evaluation statistics over features of a model and\n# validate them against a baseline.\n\n# The model resolver is only required if performing model validation in addition\n# to evaluation. In this case we validate against the latest blessed model. If\n# no model has been blessed before (as in this case) the evaluator will make our\n# candidate the first blessed model.\nmodel_resolver = tfx.dsl.Resolver(\n      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n      model_blessing=tfx.dsl.Channel(\n          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n              'latest_blessed_model_resolver')\ncontext.run(model_resolver, enable_cache=True)\n\nevaluator = tfx.components.Evaluator(\n    examples=example_gen.outputs['examples'],\n    model=trainer.outputs['model'],\n    baseline_model=model_resolver.outputs['model'],\n    eval_config=eval_config)\ncontext.run(evaluator, enable_cache=True)\n\n\"\"\"\nExplanation: 次に、この構成を Evaluatorに渡して実行します。\nEnd of explanation\n\"\"\"\n\n\nevaluator.outputs\n\n\"\"\"\nExplanation: Evaluator の出力アーティファクトを調べてみましょう。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(evaluator.outputs['evaluation'])\n\n\"\"\"\nExplanation: evaluation出力を使用すると、評価セット全体のグローバル指標のデフォルトの視覚化を表示できます。\nEnd of explanation\n\"\"\"\n\n\nimport tensorflow_model_analysis as tfma\n\n# Get the TFMA output result path and load the result.\nPATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\ntfma_result = tfma.load_eval_result(PATH_TO_RESULT)\n\n# Show data sliced along feature column trip_start_hour.\ntfma.view.render_slicing_metrics(\n    tfma_result, slicing_column='trip_start_hour')\n\n\"\"\"\nExplanation: スライスされた評価メトリクスの視覚化を表示するには、TensorFlow Model Analysis ライブラリを直接呼び出します。\nEnd of explanation\n\"\"\"\n\n\nblessing_uri = evaluator.outputs['blessing'].get()[0].uri\n!ls -l {blessing_uri}\n\n\"\"\"\nExplanation: この視覚化は同じ指標を示していますが、評価セット全体ではなく、trip_start_hourのすべての特徴値で計算されています。\nTensorFlow モデル分析は、公平性インジケーターやモデル パフォーマンスの時系列のプロットなど、他の多くの視覚化をサポートしています。 詳細については、チュートリアルを参照してください。\n構成にしきい値を追加したため、検証出力も利用できます。{code 0}blessing{/code 0} アーティファクトの存在は、モデルが検証に合格したことを示しています。これは実行される最初の検証であるため、候補は自動的に bless されます。\nEnd of explanation\n\"\"\"\n\n\nPATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\nprint(tfma.load_validation_result(PATH_TO_RESULT))\n\n\"\"\"\nExplanation: 検証結果レコードを読み込み、成功を確認することもできます。\nEnd of explanation\n\"\"\"\n\n\npusher = tfx.components.Pusher(\n    model=trainer.outputs['model'],\n    model_blessing=evaluator.outputs['blessing'],\n    push_destination=tfx.proto.PushDestination(\n        filesystem=tfx.proto.PushDestination.Filesystem(\n            base_directory=_serving_model_dir)))\ncontext.run(pusher, enable_cache=True)\n\n\"\"\"\nExplanation: Pusher\nPusher コンポーネントは通常、TFX パイプラインの最後にあります。このコンポーネントはモデルが検証に合格したかどうかをチェックし、合格した場合はモデルを _serving_model_dirにエクスポートします。\nEnd of explanation\n\"\"\"\n\n\npusher.outputs\n\n\"\"\"\nExplanation: 次にPusherの出力アーティファクトを調べてみましょう。\nEnd of explanation\n\"\"\"\n\n\npush_uri = pusher.outputs['pushed_model'].get()[0].uri\nmodel = tf.saved_model.load(push_uri)\n\nfor item in model.signatures.items():\n  pp.pprint(item)\n\n\"\"\"\nExplanation: 特に、Pusher はモデルを次のような SavedModel 形式でエクスポートします。\nEnd of explanation\n\"\"\"\n\n_, _, data = twpca.datasets.jittered_neuron()\nmodel = TWPCA(data, n_components=1, warpinit='identity')\n\nnp.all(np.isclose(model.params['warp'], np.arange(model.shared_length), atol=1e-5, rtol=2))\n\nnp.nanmax(np.abs(model.transform() - data)) < 1e-5\n\n\"\"\"\nExplanation: check identity warp does not change data appreciably\nEnd of explanation\n\"\"\"\n\n\nmodel = TWPCA(data, n_components=1, warpinit='shift')\n\nplt.imshow(np.squeeze(model.transform()))\n\n\"\"\"\nExplanation: check that shift initialization for warp solves the simple toy problem\nEnd of explanation\n\"\"\"\n\nfrom __future__ import print_function, division, unicode_literals\n\nimport oddt\nfrom oddt.datasets import dude\nprint(oddt.__version__)\n\n\"\"\"\nExplanation: <h1>DUD-E: A Database of Useful Decoys: Enhanced</h1>\nEnd of explanation\n\"\"\"\n\n\n%%bash\nmkdir -p ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/ampc/ampc.tar.gz | tar xz -C ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/cxcr4/cxcr4.tar.gz | tar xz -C ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/pur2/pur2.tar.gz | tar xz -C ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/pygm/pygm.tar.gz | tar xz -C ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/sahh/sahh.tar.gz | tar xz -C ./DUD-E_targets/\n\ndirectory = './DUD-E_targets'\n\n\"\"\"\nExplanation: We'd like to read files from DUD-E.<br/>\nYou can download different targets and different numbers of targets, but I used only these five:\nampc, \ncxcr4, \npur2, \npygm, \nsahh.<br/>\nEnd of explanation\n\"\"\"\n\n\ndude_database = dude(home=directory)\n\n\"\"\"\nExplanation: We will use the dude class.\nEnd of explanation\n\"\"\"\n\n\ntarget = dude_database['cxcr4']\n\n\"\"\"\nExplanation: Now we can get one target or iterate over all targets in our directory.\nLet's choose one target.\nEnd of explanation\n\"\"\"\n\n\ntarget.ligand\n\n\"\"\"\nExplanation: target has four properties: protein, ligand, actives and decoys:<br/>\nprotein - protein molecule<br/>\nligand - ligand molecule<br/>\nactives - generator containing actives<br/>\ndecoys - generator containing decoys\nEnd of explanation\n\"\"\"\n\n\nfor target in dude_database:\n    actives = list(target.actives)\n    decoys = list(target.decoys)\n    print('Target: ' + target.dude_id, \n          'Number of actives: ' + str(len(actives)), \n          'Number of decoys: ' + str(len(decoys)), \n          sep='\\t\\t')\n\n\"\"\"\nExplanation: Let's see which target has the most actives and decoys.\nEnd of explanation\n\"\"\"\n\nmax_steps = 3000\nbatch_size = 128\ndata_dir = 'data/cifar10/cifar-10-batches-bin/'\nmodel_dir = 'model/_cifar10_v2/'\n\n\"\"\"\nExplanation: 全局参数\nEnd of explanation\n\"\"\"\n\n\nX_train, y_train = cifar10_input.distorted_inputs(data_dir, batch_size)\n\nX_test, y_test = cifar10_input.inputs(eval_data=True, data_dir=data_dir, batch_size=batch_size)\n\nimage_holder = tf.placeholder(tf.float32, [batch_size, 24, 24, 3])\nlabel_holder = tf.placeholder(tf.int32, [batch_size])\n\n\"\"\"\nExplanation: 初始化权重\n如果需要，会给权重加上L2 loss。为了在后面计算神经网络的总体loss的时候被用上，需要统一存到一个collection。\n加载数据\n使用cifa10_input来获取数据，这个文件来自tensorflow github，可以下载下来直接使用。如果使用distorted_input方法，那么得到的数据是经过增强处理的。会对图片随机做出切片、翻转、修改亮度、修改对比度等操作。这样就能多样化我们的训练数据。\n得到一个tensor，batch_size大小的batch。并且可以迭代的读取下一个batch。\nEnd of explanation\n\"\"\"\n\n\nweight1 = variable_with_weight_loss([5, 5, 3, 64], stddev=0.05, lambda_value=0)\nkernel1 = tf.nn.conv2d(image_holder, weight1, [1, 1, 1, 1], padding='SAME')\nbias1 = tf.Variable(tf.constant(0.0, shape=[64]))\nconv1 = tf.nn.relu(tf.nn.bias_add(kernel1, bias1))\npool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\nnorm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n\n\"\"\"\nExplanation: 第一个卷积层\n同样的，我们使用5x5卷积核，3个通道（input_channel），64个output_channel。不对第一层的参数做正则化，所以将lambda_value设定为0。其中涉及到一个小技巧，就是在pool层，使用了3x3大小的ksize，但是使用2x2的stride，这样增加数据的丰富性。最后使用LRN。LRN最早见于Alex参见ImageNet的竞赛的那篇CNN论文中，Alex在论文中解释了LRN层模仿了生物神经系统的“侧抑制”机制，对局部神经元的活动创建竞争环境，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增加了模型的泛化能力。不过在之后的VGGNet论文中，对比了使用和不使用LRN两种模型，结果表明LRN并不能提高模型的性能。不过这里还是基于AlexNet的设计将其加上。\nEnd of explanation\n\"\"\"\n\n\nweight2 = variable_with_weight_loss(shape=[5, 5, 64, 64], stddev=5e-2, lambda_value=0.0)\nkernel2 = tf.nn.conv2d(norm1, weight2, strides=[1, 1, 1, 1], padding='SAME')\nbias2 = tf.Variable(tf.constant(0.1, shape=[64]))\nconv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2))\nnorm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\npool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n\"\"\"\nExplanation: 第二个卷积层\n\n输入64个channel，输出依然是64个channel\n设定bias的大小为0.1\n调换最大池化层和LRN的顺序，先进行LRN然后再最大池化层\n\n但是为什么要这么做，完全不知道？\n多看论文。\nEnd of explanation\n\"\"\"\n\n\nflattern = tf.reshape(pool2, [batch_size, -1])\ndim = flattern.get_shape()[1].value\nweight3 = variable_with_weight_loss(shape=[dim, 384], stddev=0.04, lambda_value=0.04)\nbias3 = tf.Variable(tf.constant(0.1, shape=[384]))\nlocal3 = tf.nn.relu(tf.matmul(flattern, weight3) + bias3)\n\n\"\"\"\nExplanation: 第一个全连接层\n\n要将卷积层拉伸\n全连接到新的隐藏层，设定为384个节点\n正态分布设定为0.04，bias设定为0.1\n重点是，在这里我们还设定weight loss的lambda数值为0.04\nEnd of explanation\n\"\"\"\n\n\nweight4 = variable_with_weight_loss(shape=[384, 192], stddev=0.04, lambda_value=0.04)\nbias4 = tf.Variable(tf.constant(0.1, shape=[192]))\nlocal4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4)\n\n\"\"\"\nExplanation: 第二个全连接层\n\n下降为192个节点，减少了一半\nEnd of explanation\n\"\"\"\n\n\nweight5 = variable_with_weight_loss(shape=[192, 10], stddev=1/192.0, lambda_value=0.0)\nbias5 = tf.Variable(tf.constant(0.0, shape=[10]))\nlogits = tf.add(tf.matmul(local4, weight5), bias5)\n\ndef loss(logits, labels):\n    labels = tf.cast(labels, tf.int64)\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=logits, labels=labels,\n        name = 'cross_entropy_per_example'\n    )\n    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n    tf.add_to_collection('losses', cross_entropy_mean)\n    \n    return tf.add_n(tf.get_collection('losses'), name='total_loss')\n\nloss = loss(logits, label_holder)\n\ntrain_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\n\n\"\"\"\nExplanation: 输出层\n\n最后有10个类别\nEnd of explanation\n\"\"\"\n\n\ntop_k_op = tf.nn.in_top_k(logits, label_holder, 1)\n\nsess = tf.InteractiveSession()\n\nsaver = tf.train.Saver()\n\ntf.global_variables_initializer().run()\n\n\"\"\"\nExplanation: 使用in_top_k来输出top k的准确率，默认使用top 1。常用的可以是top 5。\nEnd of explanation\n\"\"\"\n\n\ntf.train.start_queue_runners()\n\n\"\"\"\nExplanation: 启动caifar_input中需要用的线程队列。主要用途是图片数据增强。这里总共使用了16个线程来处理图片。\nEnd of explanation\n\"\"\"\n\n\nfor step in range(max_steps):\n    start_time = time.time()\n    image_batch, label_batch = sess.run([X_train, y_train])\n    _, loss_value = sess.run([train_op, loss], \n                             feed_dict={image_holder: image_batch, label_holder: label_batch})\n    duration = time.time() - start_time\n    if step % 10 == 0:\n        examples_per_sec = batch_size / duration\n        sec_this_batch = float(duration)\n        \n        format_str = ('step %d, loss = %.2f (%.1f examples/sec; %.3f sec/batch)')\n        print(format_str % (step, loss_value, examples_per_sec, sec_this_batch))\n\nsaver.save(sess, save_path=os.path.join(model_dir, 'model.chpt'), global_step=max_steps)\n\nnum_examples = 10000\nnum_iter = int(math.ceil(num_examples / batch_size))\nture_count = 0\ntotal_sample_count = num_iter * batch_size\nstep = 0\nwhile step < num_iter:\n    image_batch, label_batch = sess.run([X_test, y_test])\n    predictions = sess.run([top_k_op], \n                           feed_dict={image_holder: image_batch, label_holder: label_batch})\n    true_count += np.sum(predictions)\n    step += 1\n\nprecision = ture_count / total_sample_count\nprint(\"Precision @ 1 = %.3f\" % precision)\n\nsess.close()\n\n\"\"\"\nExplanation: 每次在计算之前，先执行image_train,label_train来获取一个batch_size大小的训练数据。然后，feed到train_op和loss中，训练样本。每10次迭代计算就会输出一些必要的信息。\nEnd of explanation\n\"\"\"\n\n!python -m spacy download en_core_web_sm\n\n\"\"\"\nExplanation: Versioning Example (Part 1/3)\nIn this example, we'll train an NLP model for sentiment analysis of tweets using spaCy.\nThrough this series, we'll take advantage of ModelDB's versioning system to keep track of changes.\nThis workflow requires verta&gt;=0.14.4 and spaCy&gt;=2.0.0.\n\nSetup\nDownload a spaCy model to train.\nEnd of explanation\n\"\"\"\n\n\nfrom __future__ import unicode_literals, print_function\n\nimport boto3\nimport json\nimport numpy as np\nimport pandas as pd\nimport spacy\n\n\"\"\"\nExplanation: Import libraries we'll need.\nEnd of explanation\n\"\"\"\n\n\nfrom verta import Client\n\nclient = Client('http://localhost:3000/')\nproj = client.set_project('Tweet Classification')\nexpt = client.set_experiment('SpaCy')\n\n\"\"\"\nExplanation: Bring in Verta's ModelDB client to organize our work, and log and version metadata.\nEnd of explanation\n\"\"\"\n\n\nS3_BUCKET = \"verta-starter\"\nS3_KEY = \"english-tweets.csv\"\nFILENAME = S3_KEY\n\nboto3.client('s3').download_file(S3_BUCKET, S3_KEY, FILENAME)\n\n\"\"\"\nExplanation: Prepare Data\nDownload a dataset of English tweets from S3 for us to train with.\nEnd of explanation\n\"\"\"\n\n\nimport utils\n\ndata = pd.read_csv(FILENAME).sample(frac=1).reset_index(drop=True)\nutils.clean_data(data)\n\ndata.head()\n\n\"\"\"\nExplanation: Then we'll load and clean the data.\nEnd of explanation\n\"\"\"\n\n\nfrom verta.code import Notebook\nfrom verta.configuration import Hyperparameters\nfrom verta.dataset import S3\nfrom verta.environment import Python\n\ncode_ver = Notebook()  # Notebook & git environment\nconfig_ver = Hyperparameters({'n_iter': 20})\ndataset_ver = S3(\"s3://{}/{}\".format(S3_BUCKET, S3_KEY))\nenv_ver = Python(Python.read_pip_environment())  # pip environment and Python version\n\n\"\"\"\nExplanation: Capture and Version Model Ingredients\nWe'll first capture metadata about our code, configuration, dataset, and environment using utilities from the verta library.\nEnd of explanation\n\"\"\"\n\n\nrepo = client.set_repository('Tweet Classification')\ncommit = repo.get_commit(branch='master')\n\n\"\"\"\nExplanation: Then, to log them, we'll use a ModelDB repository to prepare a commit.\nEnd of explanation\n\"\"\"\n\n\ncommit.update(\"notebooks/tweet-analysis\", code_ver)\ncommit.update(\"config/hyperparams\", config_ver)\ncommit.update(\"data/tweets\", dataset_ver)\ncommit.update(\"env/python\", env_ver)\n\ncommit.save(\"Initial model\")\n\ncommit\n\n\"\"\"\nExplanation: Now we'll add these versioned components to the commit and save it to ModelDB.\nEnd of explanation\n\"\"\"\n\n\nnlp = spacy.load('en_core_web_sm')\n\n\"\"\"\nExplanation: Train and Log Model\nWe'll use the pre-trained spaCy model we downloaded earlier...\nEnd of explanation\n\"\"\"\n\n\nimport training\n\ntraining.train(nlp, data, n_iter=20)\n\n\"\"\"\nExplanation: ...and fine-tune it with our dataset.\nEnd of explanation\n\"\"\"\n\n\nrun = client.set_experiment_run()\n\nrun.log_model(nlp)\n\n\"\"\"\nExplanation: Now that our model is good to go, we'll log it to ModelDB so our progress is never lost.\nUsing Verta's ModelDB Client, we'll create an Experiment Run to encapsulate our work, and log our model as an artifact.\nEnd of explanation\n\"\"\"\n\n\nrun.log_commit(\n    commit,\n    {\n        'notebook': \"notebooks/tweet-analysis\",\n        'hyperparameters': \"config/hyperparams\",\n        'training_data': \"data/tweets\",\n        'python_env': \"env/python\",\n    },\n)\n\n\"\"\"\nExplanation: And finally, we'll link the commit we created earlier to the Experiment Run to complete our logged model version.\nEnd of explanation\n\"\"\"\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport yaml\n\n\"\"\"\nExplanation: The following script extracts the (more) helpful reviews from the swiss reviews and saves them locally.\nFrom the extracted reviews it also saves a list with their asin identifiers.\nThe list of asin identifiers will be later used to to find the average review rating for the respective products.\nEnd of explanation\n\"\"\"\n\n\nwith open(\"data/swiss-reviews.txt\", 'r') as fp:\n    swiss_rev = fp.readlines()\n\nlen(swiss_rev)\n\nswiss_rev[2]\n\n\"\"\"\nExplanation: Load the swiss reviews\nEnd of explanation\n\"\"\"\n\n\ndef filter_helpful(line):\n    l = line.rstrip('\\n')\n    l = yaml.load(l)\n    if('helpful' in l.keys()):\n        if(l['helpful'][1] >= 5):\n            return True\n        else:\n            return False\n    else:\n        print(\"Review does not have helpful score key: \"+line)\n        return False\n\n\"\"\"\nExplanation: The filter_helpful function keeps only the reviews which had at least 5 flags/votes in the helpfulness field.\nThis amounts to a subset of around 23000 reviews. A smaller subset of around 10000 reviews was obtained as well by only keeping reviews with 10 flags/votes. The main advantage of the smaller subset is that it contains better quality reviews while its drawback is, of course, the reduced size.\n1) Extract the helpful reviews\nEnd of explanation\n\"\"\"\n\n\ndef get_helpful(data):\n    res = []\n    counter = 1\n    i = 0\n    for line in data:\n        i += 1\n        if(filter_helpful(line)):\n            if(counter % 1000 == 0):\n                print(\"Count \"+str(counter)+\" / \"+str(i))\n            counter += 1\n            res.append(line)\n    return res\n\nswiss_reviews_helpful = get_helpful(swiss_rev)\n\nlen(swiss_reviews_helpful)\n\n\"\"\"\nExplanation: Apply the filter_helpful to each swiss product review\nEnd of explanation\n\"\"\"\n\n\nwrite_file = open('data/swiss-reviews-helpful-correct-bigger.txt', 'w')\nfor item in swiss_reviews_helpful:\n  write_file.write(item)\nwrite_file.close()\n\n\"\"\"\nExplanation: Save the subset with helpful swiss product reviews\nEnd of explanation\n\"\"\"\n\n\nwith open('data/swiss-reviews-helpful-correct-bigger.txt', 'r') as fp:\n    swiss_reviews_helpful = fp.readlines()\n\n\"\"\"\nExplanation: 2) Extract the asins of the products which the helpful reviews correspond to\nEnd of explanation\n\"\"\"\n\n\ndef filter_asin(line):\n    l = line.rstrip('\\n')\n    l = yaml.load(l)\n    if('asin' in l.keys()):\n        return l['asin']\n    else:\n        return ''\n\nhelpful_asins = []\ncounter = 1\nfor item in swiss_reviews_helpful:\n    if(counter%500 == 0):\n        print(counter)\n    counter += 1\n    x = filter_asin(item)\n    if(len(x) > 0):\n        helpful_asins.append(x)\n\n\"\"\"\nExplanation: The following function simply extracts the 'asin' from the helpful reviews.\nRepetitions of the asins are of no consequence, as the list is just meant to be a check up.\nEnd of explanation\n\"\"\"\n\n\nimport pickle\n\nwith open('data/helpful_asins_bigger.pickle', 'wb') as fp:\n    pickle.dump(helpful_asins, fp)\n\n\"\"\"\nExplanation: Save the list of asins.\nEnd of explanation\n\"\"\"\n\nget_ipython().magic('load_ext autoreload')\nget_ipython().magic('autoreload 2')\n\nimport glob\nimport logging\nimport numpy as np\nimport os\n\nlogging.basicConfig(format=\n                          \"%(relativeCreated)12d [%(filename)s:%(funcName)20s():%(lineno)s] [%(process)d] %(message)s\",\n                    # filename=\"/tmp/caiman.log\",\n                    level=logging.WARNING)\n\nimport caiman as cm\nfrom caiman.source_extraction import cnmf as cnmf\nfrom caiman.utils.utils import download_demo\nimport matplotlib.pyplot as plt\n\nimport bokeh.plotting as bpl\nbpl.output_notebook()\n\n\"\"\"\nExplanation: Example of 1p online analysis using a Ring CNN + OnACID\nThe demo shows how to perform online analysis on one photon data using a Ring CNN for extracting the background followed by processing using the OnACID algorithm. The algorithm relies on the usage a GPU to efficiently estimate and apply the background model so it is recommended to have access to a GPU when running this notebook.\nEnd of explanation\n\"\"\"\n\n\nfnames=download_demo('blood_vessel_10Hz.mat')\n\n\"\"\"\nExplanation: First specify the data file(s) to be analyzed\nThe download_demo method will download the file (if not already present) and store it inside your caiman_data/example_movies folder. You can specify any path to files you want to analyze.\nEnd of explanation\n\"\"\"\n\n\nreuse_model = False                                                 # set to True to re-use an existing ring model\npath_to_model = None                                                # specify a pre-trained model here if needed \ngSig = (7, 7)                                                       # expected half size of neurons\ngnb = 2                                                             # number of background components for OnACID\ninit_batch = 500                                                    # number of frames for initialization and training\n\nparams_dict = {'fnames': fnames,\n               'var_name_hdf5': 'Y',                                # name of variable inside mat file where the data is stored\n               'fr': 10,                                            # frame rate (Hz)\n               'decay_time': 0.5,                                   # approximate length of transient event in seconds\n               'gSig': gSig,\n               'p': 0,                                              # order of AR indicator dynamics\n               'ring_CNN': True,                                    # SET TO TRUE TO USE RING CNN \n               'min_SNR': 2.65,                                     # minimum SNR for accepting new components\n               'SNR_lowest': 0.75,                                  # reject components with SNR below this value\n               'use_cnn': False,                                    # do not use CNN based test for components\n               'use_ecc': True,                                     # test eccentricity\n               'max_ecc': 2.625,                                    # reject components with eccentricity above this value\n               'rval_thr': 0.70,                                    # correlation threshold for new component inclusion\n               'rval_lowest': 0.25,                                 # reject components with corr below that value\n               'ds_factor': 1,                                      # spatial downsampling factor (increases speed but may lose some fine structure)\n               'nb': gnb,\n               'motion_correct': False,                             # Flag for motion correction\n               'init_batch': init_batch,                            # number of frames for initialization (presumably from the first file)\n               'init_method': 'bare',\n               'normalize': False,\n               'expected_comps': 1100,                               # maximum number of expected components used for memory pre-allocation (exaggerate here)\n               'sniper_mode': False,                                 # flag using a CNN to detect new neurons (o/w space correlation is used)\n               'dist_shape_update' : True,                           # flag for updating shapes in a distributed way\n               'min_num_trial': 5,                                   # number of candidate components per frame\n               'epochs': 3,                                          # number of total passes over the data\n               'stop_detection': True,                               # Run a last epoch without detecting new neurons  \n               'K': 50,                                              # initial number of components\n               'lr': 6e-4,\n               'lr_scheduler': [0.9, 6000, 10000],\n               'pct': 0.01,\n               'path_to_model': path_to_model,                       # where the ring CNN model is saved/loaded\n               'reuse_model': reuse_model                            # flag for re-using a ring CNN model          \n              }\nopts = cnmf.params.CNMFParams(params_dict=params_dict)\n\n\"\"\"\nExplanation: Set up some parameters\nHere we set up some parameters for specifying the ring model and running OnACID. We use the same params object as in batch processing with CNMF.\nEnd of explanation\n\"\"\"\n\n\nrun_onacid = True\n\nif run_onacid:\n    cnm = cnmf.online_cnmf.OnACID(params=opts)\n    cnm.fit_online()\n    fld_name = os.path.dirname(cnm.params.ring_CNN['path_to_model'])\n    res_name_nm = os.path.join(fld_name, 'onacid_results_nm.hdf5')\n    cnm.save(res_name_nm)                # save initial results (without any postprocessing)\nelse:\n    fld_name = os.path.dirname(path_to_model)\n    res_name = os.path.join(fld_name, 'onacid_results.hdf5')\n    cnm = cnmf.online_cnmf.load_OnlineCNMF(res_name)\n    cnm.params.data['fnames'] = fnames\n\n\"\"\"\nExplanation: Now run the Ring-CNN + CaImAn online algorithm (OnACID).\nThe first initbatch frames are used for training the ring-CNN model. Once the model is trained the background is subtracted and the different is used for initialization purposes. The initialization method chosen here bare will only search for a small number of neurons and is mostly used to initialize the background components. Initialization with the full CNMF can also be used by choosing cnmf.\nWe first create an OnACID object located in the module online_cnmf and we pass the parameters similarly to the case of batch processing. We then run the algorithm using the fit_online method. We then save the results inside\nthe folder where the Ring_CNN model is saved.\nEnd of explanation\n\"\"\"\n\n\nds = 10             # plot every ds frames to make more manageable figures\ninit_batch = 500\ndims, T = cnmf.utilities.get_file_size(fnames, var_name_hdf5='Y')\nT = np.array(T).sum()\nn_epochs = cnm.params.online['epochs']\nT_detect = 1e3*np.hstack((np.zeros(init_batch), cnm.t_detect))\nT_shapes = 1e3*np.hstack((np.zeros(init_batch), cnm.t_shapes))\nT_online = 1e3*np.hstack((np.zeros(init_batch), cnm.t_online)) - T_detect - T_shapes\nplt.figure()\nplt.stackplot(np.arange(len(T_detect))[::ds], T_online[::ds], T_detect[::ds], T_shapes[::ds],\n              colors=['tab:red', 'tab:purple', 'tab:brown'])\nplt.legend(labels=['process', 'detect', 'shapes'], loc=2)\nplt.title('Processing time allocation')\nplt.xlabel('Frame #')\nplt.ylabel('Processing time [ms]')\nmax_val = 80\nplt.ylim([0, max_val]);\nplt.plot([init_batch, init_batch], [0, max_val], '--k')\nfor i in range(n_epochs - 1):\n    plt.plot([(i+1)*T, (i+1)*T], [0, max_val], '--k')\nplt.xlim([0, n_epochs*T]);\nplt.savefig(os.path.join(fld_name, 'time_per_frame_ds.pdf'), bbox_inches='tight', pad_inches=0)\n\ninit_batch = 500\nplt.figure()\ntc_init = cnm.t_init*np.ones(T*n_epochs)\nds = 10\n#tc_mot = np.hstack((np.zeros(init_batch), np.cumsum(T_motion)/1000))\ntc_prc = np.cumsum(T_online)/1000#np.hstack((np.zeros(init_batch), ))\ntc_det = np.cumsum(T_detect)/1000#np.hstack((np.zeros(init_batch), ))\ntc_shp = np.cumsum(T_shapes)/1000#np.hstack((np.zeros(init_batch), ))\nplt.stackplot(np.arange(len(tc_init))[::ds], tc_init[::ds], tc_prc[::ds], tc_det[::ds], tc_shp[::ds],\n              colors=['g', 'tab:red', 'tab:purple', 'tab:brown'])\nplt.legend(labels=['initialize', 'process', 'detect', 'shapes'], loc=2)\nplt.title('Processing time allocation')\nplt.xlabel('Frame #')\nplt.ylabel('Processing time [s]')\nmax_val = (tc_prc[-1] + tc_det[-1] + tc_shp[-1] + cnm.t_init)*1.05\nfor i in range(n_epochs - 1):\n    plt.plot([(i+1)*T, (i+1)*T], [0, max_val], '--k')\nplt.xlim([0, n_epochs*T]);\nplt.ylim([0, max_val])\nplt.savefig(os.path.join(fld_name, 'time_cumulative_ds.pdf'), bbox_inches='tight', pad_inches=0)\n\nprint('Cost of estimating model and running first epoch: {:.2f}s'.format(tc_prc[T] + tc_det[T] + tc_shp[T] + tc_init[T]))\n\n\"\"\"\nExplanation: Check speed\nCreate some plots that show the speed per frame and cumulatively\nEnd of explanation\n\"\"\"\n\n\n# first compute background summary images\nimages = cm.load(fnames, var_name_hdf5='Y', subindices=slice(None, None, 2))\ncn_filter, pnr = cm.summary_images.correlation_pnr(images, gSig=3, swap_dim=False) # change swap dim if output looks weird, it is a problem with tiffile\n\nplt.figure(figsize=(15, 7))\nplt.subplot(1,2,1); plt.imshow(cn_filter); plt.colorbar()\nplt.subplot(1,2,2); plt.imshow(pnr); plt.colorbar()\n\ncnm.estimates.plot_contours_nb(img=cn_filter, idx=cnm.estimates.idx_components, line_color='white', thr=0.3)\n\n\"\"\"\nExplanation: Do some initial plotting\nEnd of explanation\n\"\"\"\n\n\ncnm.estimates.nb_view_components(img=cn_filter, denoised_color='red')\n\n\"\"\"\nExplanation: View components\nNow inspect the components extracted by OnACID. Note that if single pass was used then several components would be non-zero only for the part of the time interval indicating that they were detected online by OnACID.\nNote that if you get data rate error you can start Jupyter notebooks using:\n'jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10'\nEnd of explanation\n\"\"\"\n\n\nsave_file = True\nif save_file:\n    from caiman.utils.nn_models import create_LN_model\n    model_LN = create_LN_model(images, shape=opts.data['dims'] + (1,), n_channels=opts.ring_CNN['n_channels'],\n                               width=opts.ring_CNN['width'], use_bias=opts.ring_CNN['use_bias'], gSig=gSig[0],\n                               use_add=opts.ring_CNN['use_add'])\n    model_LN.load_weights(cnm.params.ring_CNN['path_to_model'])\n\n    # Load the data in batches and save them\n\n    m = []\n    saved_files = []\n    batch_length = 256\n    for i in range(0, T, batch_length):\n        images = cm.load(fnames, var_name_hdf5='Y', subindices=slice(i, i + batch_length))\n        images_filt = np.squeeze(model_LN.predict(np.expand_dims(images, axis=-1)))\n        temp_file = os.path.join(fld_name, 'pfc_back_removed_' + format(i, '05d') + '.h5')\n        saved_files.append(temp_file)\n        m = cm.movie(np.maximum(images - images_filt, 0))\n        m.save(temp_file)\nelse:\n    saved_files = glob.glob(os.path.join(fld_name, 'pfc_back_removed_*'))\n    saved_files.sort()\n\nfname_mmap = cm.save_memmap([saved_files], order='C', border_to_0=0)\nYr, dims, T = cm.load_memmap(fname_mmap)\nimages_mmap = Yr.T.reshape((T,) + dims, order='F')\n\n\"\"\"\nExplanation: Load ring model to filter the data\nFilter the data with the learned Ring CNN model and a create memory mapped file with the background subtracted data. We will use this to run the quality tests and screen for false positive components.\nEnd of explanation\n\"\"\"\n\n\ncnm.params.merging['merge_thr'] = 0.7\ncnm.estimates.c1 = np.zeros(cnm.estimates.A.shape[-1])\ncnm.estimates.bl = np.zeros(cnm.estimates.A.shape[-1])\ncnm.estimates.neurons_sn = np.zeros(cnm.estimates.A.shape[-1])\ncnm.estimates.g = None #np.ones((cnm.estimates.A.shape[-1], 1))*.9\ncnm.estimates.merge_components(Yr, cnm.params)\n\n\"\"\"\nExplanation: Merge components\nEnd of explanation\n\"\"\"\n\n\ncnm.params.quality\n\ncnm.estimates.evaluate_components(imgs=images_mmap, params=cnm.params)\n\ncnm.estimates.plot_contours_nb(img=cn_filter, idx=cnm.estimates.idx_components, line_color='white')\n\ncnm.estimates.nb_view_components(idx=cnm.estimates.idx_components, img=cn_filter)\n\n\"\"\"\nExplanation: Evaluate components and compare again\nWe run the component evaluation tests to screen for false positive components.\nEnd of explanation\n\"\"\"\n\n\ncnmfe_results = download_demo('online_vs_offline.npz')\nlocals().update(np.load(cnmfe_results, allow_pickle=True))\nA_patch_good = A_patch_good.item()\nestimates_gt = cnmf.estimates.Estimates(A=A_patch_good, C=C_patch_good, dims=dims)\n\n\nmaxthr=0.01\ncnm.estimates.A_thr=None\ncnm.estimates.threshold_spatial_components(maxthr=maxthr)\nestimates_gt.A_thr=None\nestimates_gt.threshold_spatial_components(maxthr=maxthr*10)\nmin_size = np.pi*(gSig[0]/1.5)**2\nmax_size = np.pi*(gSig[0]*1.5)**2\nntk = cnm.estimates.remove_small_large_neurons(min_size_neuro=min_size, max_size_neuro=2*max_size)\ngtk = estimates_gt.remove_small_large_neurons(min_size_neuro=min_size, max_size_neuro=2*max_size)\n\nm1, m2, nm1, nm2, perf = cm.base.rois.register_ROIs(estimates_gt.A_thr[:, estimates_gt.idx_components],\n                                                  cnm.estimates.A_thr[:, cnm.estimates.idx_components],\n                                                  dims, align_flag=False, thresh_cost=.7, plot_results=True,\n                                                  Cn=cn_filter, enclosed_thr=None)[:-1]\n\n\"\"\"\nExplanation: Compare against CNMF-E results\nWe download the results of CNMF-E on the same dataset and compare.\nEnd of explanation\n\"\"\"\n\n\nfor k, v in perf.items():\n    print(k + ':', '%.4f' % v, end='  ')\n\n\"\"\"\nExplanation: Print performance results\nEnd of explanation\n\"\"\"\n\n\nres_name = os.path.join(fld_name, 'onacid_results.hdf5')\ncnm.save(res_name)\n\n\"\"\"\nExplanation: Save the results\nEnd of explanation\n\"\"\"\n\n\nimport matplotlib.lines as mlines\nlp, hp = np.nanpercentile(cn_filter, [5, 98])\nA_onacid = cnm.estimates.A_thr.toarray().copy()\nA_onacid /= A_onacid.max(0)\n\nA_TP = estimates_gt.A[:, m1].toarray() #cnm.estimates.A[:, cnm.estimates.idx_components[m2]].toarray()\nA_TP = A_TP.reshape(dims + (-1,), order='F').transpose(2,0,1)\nA_FN = estimates_gt.A[:, nm1].toarray()\nA_FN = A_FN.reshape(dims + (-1,), order='F').transpose(2,0,1)\nA_FP = A_onacid[:,cnm.estimates.idx_components[nm2]]\nA_FP = A_FP.reshape(dims + (-1,), order='F').transpose(2,0,1)\n\n\nplt.figure(figsize=(15, 12))\nplt.imshow(cn_filter, vmin=lp, vmax=hp, cmap='viridis')\nplt.colorbar();\n\nfor aa in A_TP:\n    plt.contour(aa, [0.05], colors='k');\nfor aa in A_FN:\n    plt.contour(aa, [0.05], colors='r');\nfor aa in A_FP:\n    plt.contour(aa, [0.25], colors='w');\ncl = ['k', 'r', 'w']\nlb = ['both', 'CNMF-E only', 'ring CNN only']\nday = [mlines.Line2D([], [], color=cl[i], label=lb[i]) for i in range(3)]\nplt.legend(handles=day, loc=3)\nplt.axis('off');\nplt.margins(0, 0);\nplt.savefig(os.path.join(fld_name, 'ring_CNN_contours_gSig_3.pdf'), bbox_inches='tight', pad_inches=0)\n\nA_rej = cnm.estimates.A[:, cnm.estimates.idx_components_bad].toarray()\nA_rej = A_rej.reshape(dims + (-1,), order='F').transpose(2,0,1)\nplt.figure(figsize=(15, 15))\nplt.imshow(cn_filter, vmin=lp, vmax=hp, cmap='viridis')\nplt.title('Rejected Components')\nfor aa in A_rej:\n    plt.contour(aa, [0.05], colors='w');\n\n\"\"\"\nExplanation: Make some plots\nEnd of explanation\n\"\"\"\n\n\nfrom caiman.utils.nn_models import create_LN_model\nmodel_LN = create_LN_model(images, shape=opts.data['dims'] + (1,), n_channels=opts.ring_CNN['n_channels'],\n                           width=opts.ring_CNN['width'], use_bias=opts.ring_CNN['use_bias'], gSig=gSig[0],\n                           use_add=opts.ring_CNN['use_add'])\nmodel_LN.load_weights(cnm.params.ring_CNN['path_to_model'])\n\nW = model_LN.get_weights()\n\nplt.figure(figsize=(10, 10))\nplt.subplot(2,2,1); plt.imshow(np.squeeze(W[0][:,:,:,0])); plt.colorbar(); plt.title('Ring Kernel 1')\nplt.subplot(2,2,2); plt.imshow(np.squeeze(W[0][:,:,:,1])); plt.colorbar(); plt.title('Ring Kernel 2')\nplt.subplot(2,2,3); plt.imshow(np.squeeze(W[-1][:,:,0])); plt.colorbar(); plt.title('Multiplicative Layer 1')\nplt.subplot(2,2,4); plt.imshow(np.squeeze(W[-1][:,:,1])); plt.colorbar(); plt.title('Multiplicative Layer 2');\n\n\"\"\"\nExplanation: Show the learned filters\nEnd of explanation\n\"\"\"\n\n\nm1 = cm.load(fnames, var_name_hdf5='Y')  # original data\nm2 = cm.load(fname_mmap)  # background subtracted data\nm3 = m1 - m2  # estimated background\nm4 = cm.movie(cnm.estimates.A[:,cnm.estimates.idx_components].dot(cnm.estimates.C[cnm.estimates.idx_components])).reshape(dims + (T,)).transpose(2,0,1)\n              # estimated components\n\nnn = 0.01\nmm = 1 - nn/4   # normalize movies by quantiles\nm1 = (m1 - np.quantile(m1[:1000], nn))/(np.quantile(m1[:1000], mm) - np.quantile(m1[:1000], nn))\nm2 = (m2 - np.quantile(m2[:1000], nn))/(np.quantile(m2[:1000], mm) - np.quantile(m2[:1000], nn))\nm3 = (m3 - np.quantile(m3[:1000], nn))/(np.quantile(m3[:1000], mm) - np.quantile(m3[:1000], nn))\nm4 = (m4 - np.quantile(m4[:1000], nn))/(np.quantile(m4[:1000], mm) - np.quantile(m4[:1000], nn))\n\nm = cm.concatenate((cm.concatenate((m1.transpose(0,2,1), m3.transpose(0,2,1)), axis=2),\n                    cm.concatenate((m2.transpose(0,2,1), m4), axis=2)), axis=1)\n\nm[:3000].play(magnification=2, q_min=1, plot_text=True,\n              save_movie=True, movie_name=os.path.join(fld_name, 'movie.avi'))\n\n\"\"\"\nExplanation: Make a movie\nEnd of explanation\n\"\"\"\n\n#$HIDE_INPUT$\nimport pandas as pd\nfrom IPython.display import display\n\nred_wine = pd.read_csv('../input/dl-course-data/red-wine.csv')\n\n# Create training and validation splits\ndf_train = red_wine.sample(frac=0.7, random_state=0)\ndf_valid = red_wine.drop(df_train.index)\ndisplay(df_train.head(4))\n\n# Scale to [0, 1]\nmax_ = df_train.max(axis=0)\nmin_ = df_train.min(axis=0)\ndf_train = (df_train - min_) / (max_ - min_)\ndf_valid = (df_valid - min_) / (max_ - min_)\n\n# Split features and target\nX_train = df_train.drop('quality', axis=1)\nX_valid = df_valid.drop('quality', axis=1)\ny_train = df_train['quality']\ny_valid = df_valid['quality']\n\n\"\"\"\nExplanation: Introduction\nIn the first two lessons, we learned how to build fully-connected networks out of stacks of dense layers. When first created, all of the network's weights are set randomly -- the network doesn't \"know\" anything yet. In this lesson we're going to see how to train a neural network; we're going to see how neural networks learn.\nAs with all machine learning tasks, we begin with a set of training data. Each example in the training data consists of some features (the inputs) together with an expected target (the output). Training the network means adjusting its weights in such a way that it can transform the features into the target. In the 80 Cereals dataset, for instance, we want a network that can take each cereal's 'sugar', 'fiber', and 'protein' content and produce a prediction for that cereal's 'calories'. If we can successfully train a network to do that, its weights must represent in some way the relationship between those features and that target as expressed in the training data.\nIn addition to the training data, we need two more things:\n- A \"loss function\" that measures how good the network's predictions are.\n- An \"optimizer\" that can tell the network how to change its weights.\nThe Loss Function\nWe've seen how to design an architecture for a network, but we haven't seen how to tell a network what problem to solve. This is the job of the loss function.\nThe loss function measures the disparity between the the target's true value and the value the model predicts. \nDifferent problems call for different loss functions. We have been looking at regression problems, where the task is to predict some numerical value -- calories in 80 Cereals, rating in Red Wine Quality. Other regression tasks might be predicting the price of a house or the fuel efficiency of a car.\nA common loss function for regression problems is the mean absolute error or MAE. For each prediction y_pred, MAE measures the disparity from the true target y_true by an absolute difference abs(y_true - y_pred).\nThe total MAE loss on a dataset is the mean of all these absolute differences.\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/VDcvkZN.png\" width=\"500\" alt=\"A graph depicting error bars from data points to the fitted line..\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>The mean absolute error is the average length between the fitted curve and the data points.\n</center></figcaption>\n</figure>\n\nBesides MAE, other loss functions you might see for regression problems are the mean-squared error (MSE) or the Huber loss (both available in Keras).\nDuring training, the model will use the loss function as a guide for finding the correct values of its weights (lower loss is better). In other words, the loss function tells the network its objective.\nThe Optimizer - Stochastic Gradient Descent\nWe've described the problem we want the network to solve, but now we need to say how to solve it. This is the job of the optimizer. The optimizer is an algorithm that adjusts the weights to minimize the loss.\nVirtually all of the optimization algorithms used in deep learning belong to a family called stochastic gradient descent. They are iterative algorithms that train a network in steps. One step of training goes like this:\n1. Sample some training data and run it through the network to make predictions.\n2. Measure the loss between the predictions and the true values.\n3. Finally, adjust the weights in a direction that makes the loss smaller.\nThen just do this over and over until the loss is as small as you like (or until it won't decrease any further.)\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/rFI1tIk.gif\" width=\"1600\" alt=\"Fitting a line batch by batch. The loss decreases and the weights approach their true values.\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>Training a neural network with Stochastic Gradient Descent.\n</center></figcaption>\n</figure>\n\nEach iteration's sample of training data is called a minibatch (or often just \"batch\"), while a complete round of the training data is called an epoch. The number of epochs you train for is how many times the network will see each training example.\nThe animation shows the linear model from Lesson 1 being trained with SGD. The pale red dots depict the entire training set, while the solid red dots are the minibatches. Every time SGD sees a new minibatch, it will shift the weights (w the slope and b the y-intercept) toward their correct values on that batch. Batch after batch, the line eventually converges to its best fit. You can see that the loss gets smaller as the weights get closer to their true values.\nLearning Rate and Batch Size\nNotice that the line only makes a small shift in the direction of each batch (instead of moving all the way). The size of these shifts is determined by the learning rate. A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values.\nThe learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds. Their interaction is often subtle and the right choice for these parameters isn't always obvious. (We'll explore these effects in the exercise.)\nFortunately, for most work it won't be necessary to do an extensive hyperparameter search to get satisfactory results. Adam is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is \"self tuning\", in a sense). Adam is a great general-purpose optimizer.\nAdding the Loss and Optimizer\nAfter defining a model, you can add a loss function and optimizer with the model's compile method:\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"mae\",\n)\nNotice that we are able to specify the loss and optimizer with just a string. You can also access these directly through the Keras API -- if you wanted to tune parameters, for instance -- but for us, the defaults will work fine.\n<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n    <strong>What's In a Name?</strong><br>\nThe <strong>gradient</strong> is a vector that tells us in what direction the weights need to go. More precisely, it tells us how to change the weights to make the loss change <em>fastest</em>. We call our process gradient <strong>descent</strong> because it uses the gradient to <em>descend</em> the loss curve towards a minimum. <strong>Stochastic</strong> means \"determined by chance.\" Our training is <em>stochastic</em> because the minibatches are <em>random samples</em> from the dataset. And that's why it's called SGD!\n</blockquote>\n\nExample - Red Wine Quality\nNow we know everything we need to start training deep learning models. So let's see it in action! We'll use the Red Wine Quality dataset.\nThis dataset consists of physiochemical measurements from about 1600 Portuguese red wines. Also included is a quality rating for each wine from blind taste-tests. How well can we predict a wine's perceived quality from these measurements?\nWe've put all of the data preparation into this next hidden cell. It's not essential to what follows so feel free to skip it. One thing you might note for now though is that we've rescaled each feature to lie in the interval $[0, 1]$. As we'll discuss more in Lesson 5, neural networks tend to perform best when their inputs are on a common scale.\nEnd of explanation\n\"\"\"\n\n\nprint(X_train.shape)\n\n\"\"\"\nExplanation: How many inputs should this network have? We can discover this by looking at the number of columns in the data matrix. Be sure not to include the target ('quality') here -- only the input features.\nEnd of explanation\n\"\"\"\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(512, activation='relu', input_shape=[11]),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(1),\n])\n\n\"\"\"\nExplanation: Eleven columns means eleven inputs.\nWe've chosen a three-layer network with over 1500 neurons. This network should be capable of learning fairly complex relationships in the data.\nEnd of explanation\n\"\"\"\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='mae',\n)\n\n\"\"\"\nExplanation: Deciding the architecture of your model should be part of a process. Start simple and use the validation loss as your guide. You'll learn more about model development in the exercises.\nAfter defining the model, we compile in the optimizer and loss function.\nEnd of explanation\n\"\"\"\n\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=256,\n    epochs=10,\n)\n\n\"\"\"\nExplanation: Now we're ready to start the training! We've told Keras to feed the optimizer 256 rows of the training data at a time (the batch_size) and to do that 10 times all the way through the dataset (the epochs).\nEnd of explanation\n\"\"\"\n\n\nimport pandas as pd\n\n# convert the training history to a dataframe\nhistory_df = pd.DataFrame(history.history)\n# use Pandas native plot method\nhistory_df['loss'].plot();\n\n\"\"\"\nExplanation: You can see that Keras will keep you updated on the loss as the model trains.\nOften, a better way to view the loss though is to plot it. The fit method in fact keeps a record of the loss produced during training in a History object. We'll convert the data to a Pandas dataframe, which makes the plotting easy.\nEnd of explanation\n\"\"\"\n\nimport time\nfrom datetime import datetime\n\nfrom typing import List\n\nimport numpy as np\nimport pandas as pd\n\nimport google.auth\n\nfrom google.cloud import logging_v2\nfrom google.cloud.monitoring_dashboard.v1 import DashboardsServiceClient\nfrom google.cloud.logging_v2 import MetricsServiceV2Client\nfrom google.cloud.monitoring_v3.query import Query\nfrom google.cloud.monitoring_v3 import MetricServiceClient\n\nimport matplotlib.pyplot as plt\n\n\"\"\"\nExplanation: Analyzing Locust Load Testing Results\nThis Notebook demonstrates how to analyze AI Platform Prediction load testing runs using metrics captured in Cloud Monitoring. \nThis Notebook build on the 02-perf-testing.ipynb notebook that shows how to configure and run load tests against AI Platform Prediction using Locust.io. The outlined testing process results in a Pandas dataframe that aggregates the standard AI Platform Prediction metrics with a set of custom, log-based metrics generated from log entries captured by the Locust testing script.\nThe Notebook covers the following steps:\n1. Retrieve and consolidate test results from Cloud Monitoring\n2. Analyze and visualize utilization and latency results\nSetup\nThis notebook was tested on AI Platform Notebooks using the standard TF 2.2 image.\nImport libraries\nEnd of explanation\n\"\"\"\n\n\nPROJECT_ID = '[your-project-id]' # Set your project Id\nMODEL_NAME = 'resnet_classifier'\nMODEL_VERSION = 'v1'\nLOG_NAME = 'locust' # Set your log name\nTEST_ID = 'test-20200829-190943' # Set your test Id\nTEST_START_TIME = datetime.fromisoformat('2020-08-28T21:30:00-00:00') # Set your test start time\nTEST_END_TIME = datetime.fromisoformat('2020-08-29T22:00:00-00:00') # Set your test end time\n\n\n\"\"\"\nExplanation: Configure GCP environment settings\nEnd of explanation\n\"\"\"\n\n\ncreds , _ = google.auth.default()\nclient = MetricServiceClient(credentials=creds)\n\nproject_path = client.project_path(PROJECT_ID)\nfilter = 'metric.type=starts_with(\"ml.googleapis.com/prediction\")'\n\nfor descriptor in client.list_metric_descriptors(project_path, filter_=filter):\n    print(descriptor.type)\n\n\"\"\"\nExplanation: 1. Retrieve and consolidate test results\nLocust's web interface along with a Cloud Monitoring dashboard provide a cursory view into performance of a tested AI Platform Prediction model version. A more thorough analysis can be performed by consolidating metrics collected during a test and using data analytics and visualization tools.\nIn this section, you will retrieve the metrics captured in Cloud Monitoring and consolidate them into a single Pandas dataframe.\n1.1 List available AI Platform Prediction metrics\nEnd of explanation\n\"\"\"\n\n\nfilter = 'metric.type=starts_with(\"logging.googleapis.com/user\")'\n\nfor descriptor in client.list_metric_descriptors(project_path, filter_=filter):\n    print(descriptor.type)\n\n\"\"\"\nExplanation: 1.2. List custom log based metrics\nEnd of explanation\n\"\"\"\n\n\ndef retrieve_metrics(client, project_id, start_time, end_time, model, model_version, test_id, log_name):\n    \"\"\"\n    Retrieves test metrics from Cloud Monitoring.\n    \"\"\"\n    def _get_aipp_metric(metric_type: str, labels: List[str]=[], metric_name=None)-> pd.DataFrame:\n        \"\"\"\n        Retrieves a specified AIPP metric.\n        \"\"\"\n        query = Query(client, project_id, metric_type=metric_type)\n        query = query.select_interval(end_time, start_time)\n        query = query.select_resources(model_id=model)\n        query = query.select_resources(version_id=model_version)\n        \n        if metric_name:\n            labels = ['metric'] + labels \n        df = query.as_dataframe(labels=labels)\n        \n        if not df.empty:\n            if metric_name:\n                df.columns.set_levels([metric_name], level=0, inplace=True)\n            df = df.set_index(df.index.round('T'))\n        \n        return df\n    \n    def _get_locust_metric(metric_type: str, labels: List[str]=[], metric_name=None)-> pd.DataFrame:\n        \"\"\"\n        Retrieves a specified custom log-based metric.\n        \"\"\"\n        query = Query(client, project_id, metric_type=metric_type)\n        query = query.select_interval(end_time, start_time)\n        query = query.select_metrics(log=log_name)\n        query = query.select_metrics(test_id=test_id)\n        \n        if metric_name:\n            labels = ['metric'] + labels \n        df = query.as_dataframe(labels=labels)\n        \n        if not df.empty: \n            if metric_name:\n                df.columns.set_levels([metric_name], level=0, inplace=True)\n            df = df.apply(lambda row: [metric.mean for metric in row])\n            df = df.set_index(df.index.round('T'))\n        \n        return df\n    \n    # Retrieve GPU duty cycle\n    metric_type = 'ml.googleapis.com/prediction/online/accelerator/duty_cycle'\n    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'duty_cycle')\n    df = metric\n\n    # Retrieve CPU utilization\n    metric_type = 'ml.googleapis.com/prediction/online/cpu/utilization'\n    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'cpu_utilization')\n    if not metric.empty:\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve prediction count\n    metric_type = 'ml.googleapis.com/prediction/prediction_count'\n    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'prediction_count')\n    if not metric.empty:\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve responses per second\n    metric_type = 'ml.googleapis.com/prediction/response_count'\n    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'response_rate')\n    if not metric.empty:\n        metric = (metric/60).round(2)\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve backend latencies\n    metric_type = 'ml.googleapis.com/prediction/latencies'\n    metric = _get_aipp_metric(metric_type, ['latency_type', 'replica_id', 'signature'])\n    if not metric.empty:\n        metric = metric.apply(lambda row: [round(latency.mean/1000,1) for latency in row])\n        metric.columns.set_names(['metric', 'replica_id', 'signature'], inplace=True)\n        level_values = ['Latency: ' + value for value in metric.columns.get_level_values(level=0)]\n        metric.columns.set_levels(level_values, level=0, inplace=True)\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve Locust latency\n    metric_type = 'logging.googleapis.com/user/locust_latency'\n    metric = _get_locust_metric(metric_type, ['replica_id', 'signature'], 'Latency: client')\n    if not metric.empty:\n        metric = metric.round(2).replace([0], np.nan)\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve Locust user count\n    metric_type = 'logging.googleapis.com/user/locust_users'\n    metric = _get_locust_metric(metric_type, ['replica_id', 'signature'], 'User count')\n    if not metric.empty:\n        metric = metric.round()\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve Locust num_failures\n    metric_type = 'logging.googleapis.com/user/num_failures'\n    metric = _get_locust_metric(metric_type, ['replica_id', 'signature'], 'Num of failures')\n    if not metric.empty:\n        metric = metric.round()\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve Locust num_failures\n    metric_type = 'logging.googleapis.com/user/num_requests'\n    metric = _get_locust_metric(metric_type, ['replica_id', 'signature'], 'Num of requests')\n    if not metric.empty:\n        metric = metric.round()\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n\n    return df\n    \n\ntest_result = retrieve_metrics(\n    client, \n    PROJECT_ID, \n    TEST_START_TIME, \n    TEST_END_TIME, \n    MODEL_NAME, \n    MODEL_VERSION,\n    TEST_ID, \n    LOG_NAME\n)\n\ntest_result.head().T\n\n\"\"\"\nExplanation: 1.3. Retrieve test metrics\nDefine a helper function that retrieves test metrics from Cloud Monitoring\nEnd of explanation\n\"\"\"\n\n\ngpu_utilization_results = test_result['duty_cycle']\ngpu_utilization_results.columns = gpu_utilization_results.columns.get_level_values(0)\nax = gpu_utilization_results.plot(figsize=(14, 9), legend=True)\nax.set_xlabel('Time', fontsize=16)\nax.set_ylabel('Utilization ratio', fontsize=16)\n_ = ax.set_title(\"GPU Utilization\", fontsize=20)\n\n\"\"\"\nExplanation: The retrieved dataframe uses hierarchical indexing for column names. The reason is that some metrics contain multiple time series. For example, the GPU duty_cycle metric includes a time series of measures per each GPU used in the deployment (denoted as replica_id). The top level of the column index is a metric name. The second level is a replica_id. The third level is a signature of a model.\nAll metrics are aligned on the same timeline. \n2. Analyzing and Visualizing test results\nIn the context of our scenario the key concern is GPU utilization at various levels of throughput and latency. The primary metric exposed by AI Platform Prediction to monitor GPU utilization is duty cycle. This metric captures an average fraction of time over the 60 second period during which the accelerator(s) were actively processing.\n2.1. GPU utilization\nEnd of explanation\n\"\"\"\n\n\ncpu_utilization_results = test_result['cpu_utilization']\ncpu_utilization_results.columns = cpu_utilization_results.columns.get_level_values(0)\nax = cpu_utilization_results.plot(figsize=(14, 9), legend=True)\nax.set_xlabel('Time', fontsize=16)\nax.set_ylabel('Utilization ratio', fontsize=16)\n_ = ax.set_title(\"CPU Utilization\", fontsize=20)\n\n\"\"\"\nExplanation: 2.2. CPU utilization\nEnd of explanation\n\"\"\"\n\n\nlatency_results = test_result[['Latency: model', 'Latency: client']]\nlatency_results.columns = latency_results.columns.get_level_values(0)\nax = latency_results.plot(figsize=(14, 9), legend=True)\nax.set_xlabel('Time', fontsize=16)\nax.set_ylabel('milisecond', fontsize=16)\n_ = ax.set_title(\"Latency\", fontsize=20)\n\n\"\"\"\nExplanation: 2.3. Latency\nEnd of explanation\n\"\"\"\n\n\nthroughput_results = test_result[['response_rate', 'User count']]\nthroughput_results.columns = throughput_results.columns.get_level_values(0)\nax = throughput_results.plot(figsize=(14, 9), legend=True)\nax.set_xlabel('Time', fontsize=16)\nax.set_ylabel('Count', fontsize=16)\n_ = ax.set_title(\"Response Rate vs User Count\", fontsize=20)\n\n\"\"\"\nExplanation: 2.4. Request throughput\nWe are going to use the response_rate metric, which tracks a number of responses returned by AI Platform Prediction over a 1 minute interval.\nEnd of explanation\n\"\"\"\n\n\nlogging_client = MetricsServiceV2Client(credentials=creds)\nparent = logging_client.project_path(PROJECT_ID)\n\nfor element in logging_client.list_log_metrics(parent):\n    metric_path = logging_client.metric_path(PROJECT_ID, element.name)\n    logging_client.delete_log_metric(metric_path)\n    print(\"Deleted metric: \", metric_path)\n\ndisplay_name = 'AI Platform Prediction and Locust'\ndashboard_service_client = DashboardsServiceClient(credentials=creds)\nparent = 'projects/{}'.format(PROJECT_ID)\nfor dashboard in dashboard_service_client.list_dashboards(parent):\n    if dashboard.display_name == display_name:\n        dashboard_service_client.delete_dashboard(dashboard.name)\n        print(\"Deleted dashboard:\", dashboard.name)\n\n\"\"\"\nExplanation: Cleaning up: delete the log-based metrics and dasboard\nEnd of explanation\n\"\"\"\n\nimport pylearn2.utils\nimport pylearn2.config\nimport theano\nimport neukrill_net.dense_dataset\nimport neukrill_net.utils\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport holoviews as hl\n%load_ext holoviews.ipython\nimport sklearn.metrics\n\ncd ..\n\nsettings = neukrill_net.utils.Settings(\"settings.json\")\nrun_settings = neukrill_net.utils.load_run_settings(\n    \"run_settings/replicate_8aug.json\", settings, force=True)\n\nmodel = pylearn2.utils.serial.load(run_settings['alt_picklepath'])\n\nc = 'train_objective'\nchannel = model.monitor.channels[c]\n\n\"\"\"\nExplanation: The following are the results we've got from online augmentation so far. Some bugs have been fixed by Scott since then so these might be redundant. If they're not redundant then they are very bad.\nLoading the pickle\nEnd of explanation\n\"\"\"\n\n\nplt.title(c)\nplt.plot(channel.example_record,channel.val_record)\n\nc = 'train_y_nll'\nchannel = model.monitor.channels[c]\nplt.title(c)\nplt.plot(channel.example_record,channel.val_record)\n\ndef plot_monitor(c = 'valid_y_nll'):\n    channel = model.monitor.channels[c]\n    plt.title(c)\n    plt.plot(channel.example_record,channel.val_record)\n    return None\nplot_monitor()\n\nplot_monitor(c=\"valid_objective\")\n\n\"\"\"\nExplanation: Replicating 8aug\nThe DensePNGDataset run with 8 augmentations got us most of the way to our best score in one go. If we can replicate that results with online augmentation then we can be pretty confident that online augmentation is a good idea. Unfortunately, it looks like we can't:\nEnd of explanation\n\"\"\"\n\n\n%run check_test_score.py run_settings/replicate_8aug.json\n\n\"\"\"\nExplanation: Would actually like to know what kind of score this model gets on the check_test_score script.\nEnd of explanation\n\"\"\"\n\n\nrun_settings = neukrill_net.utils.load_run_settings(\n    \"run_settings/online_manyaug.json\", settings, force=True)\n\nmodel = pylearn2.utils.serial.load(run_settings['alt_picklepath'])\n\nplot_monitor(c=\"valid_objective\")\n\n\"\"\"\nExplanation: So we can guess that the log loss score we're seeing is in fact correct. There are definitely some bugs in the ListDataset code.\nMany Augmentations\nWe want to be able to use online augmentations to run large combinations of different augmentations on the images. This model had almost everything turned on, a little:\nEnd of explanation\n\"\"\"\n\n\nsettings = neukrill_net.utils.Settings(\"settings.json\")\nrun_settings = neukrill_net.utils.load_run_settings(\n    \"run_settings/alexnet_based_onlineaug.json\", settings, force=True)\n\nmodel = pylearn2.utils.serial.load(run_settings['pickle abspath'])\n\nplot_monitor(c=\"train_y_nll\")\n\nplot_monitor(c=\"valid_y_nll\")\n\nplot_monitor(c=\"train_objective\")\n\nplot_monitor(c=\"valid_objective\")\n\n\"\"\"\nExplanation: Looks like it's completely incapable of learning.\nThese problems suggest that the augmentation might be garbling the images; making them useless for learning from. Or worse, garbling the order so each image doesn't correspond to its label.\nTransformer Results\nWe also have results from a network trained using a Transformer dataset, which is how online augmentation is supposed to be supported in Pylearn2.\nEnd of explanation\n\"\"\"\n\nfrom pprint import pprint\nfrom time import sleep\nfrom pynq import PL\nfrom pynq import Overlay\nfrom pynq.drivers import Trace_Buffer\nfrom pynq.iop import Pmod_TMP2\nfrom pynq.iop import PMODA\nfrom pynq.iop import PMODB\nfrom pynq.iop import ARDUINO\n\nol = Overlay(\"base.bit\")\nol.download()\npprint(PL.ip_dict)\n\n\"\"\"\nExplanation: Trace Buffer - Tracing IIC Transactions\nThe Trace_Buffer class can monitor the waveform and transations on PMODA, PMODB, and ARDUINO connectors.\nThis demo shows how to use this class to track IIC transactions. For this demo, users have to connect the Pmod TMP2 sensor to PMODA.\nStep 1: Overlay Management\nUsers have to import all the necessary classes. Make sure to use the right bitstream.\nEnd of explanation\n\"\"\"\n\n\ntmp2 = Pmod_TMP2(PMODA)\ntmp2.set_log_interval_ms(1)\n\n\"\"\"\nExplanation: Step 2: Instantiating Temperature Sensor\nAlthough this demo can also be done on PMODB, we use PMODA in this demo.\nSet the log interval to be 1ms. This means the IO Processor (IOP) will read temperature values every 1ms.\nEnd of explanation\n\"\"\"\n\n\ntr_buf = Trace_Buffer(PMODA,\"i2c\",samplerate=1000000)\n\n# Start the trace buffer\ntr_buf.start()\n\n# Issue reads for 1 second\ntmp2.start_log()\nsleep(1)\ntmp2_log = tmp2.get_log()\n\n# Stop the trace buffer\ntr_buf.stop()\n\n\"\"\"\nExplanation: Step 3: Tracking Transactions\nInstantiating the trace buffer with IIC protocol. The sample rate is set to 1MHz. Although the IIC clock is only 100kHz, we still have to use higher sample rate to keep track of IIC control signals from IOP.\nAfter starting the trace buffer DMA, also start to issue IIC reads for 1 second. Then stop the trace buffer DMA.\nEnd of explanation\n\"\"\"\n\n\n# Configuration for PMODA\nstart = 600\nstop = 10000\ntri_sel=[0x40000,0x80000]\ntri_0=[0x4,0x8]\ntri_1=[0x400,0x800]\nmask = 0x0\n\n# Parsing and decoding\ntr_buf.parse(\"i2c_trace.csv\",\n             start,stop,mask,tri_sel,tri_0,tri_1)\ntr_buf.set_metadata(['SDA','SCL'])\ntr_buf.decode(\"i2c_trace.pd\")\n\n\"\"\"\nExplanation: Step 4: Parsing and Decoding Transactions\nThe trace buffer object is able to parse the transactions into a *.csv file (saved into the same folder as this script). The input arguments for the parsing method is:\n    * start : the starting sample number of the trace.\n    * stop : the stopping sample number of the trace.\n    * tri_sel: masks for tri-state selection bits.\n    * tri_0: masks for pins selected when the corresponding tri_sel = 0.\n    * tri_0: masks for pins selected when the corresponding tri_sel = 1.\n    * mask: mask for pins selected always.\nFor PMODB, the configuration of the masks can be:\n    * tri_sel=[0x40000<<32,0x80000<<32]\n    * tri_0=[0x4<<32,0x8<<32]\n    * tri_1=[0x400<<32,0x800<<32]\n    * mask = 0x0\nThen the trace buffer object can also decode the transactions using the open-source sigrok decoders. The decoded file (*.pd) is saved into the same folder as this script.\nReference:\nhttps://sigrok.org/wiki/Main_Page\nEnd of explanation\n\"\"\"\n\n\ns0 = 1\ns1 = 5000\ntr_buf.display(s0,s1)\n\n\"\"\"\nExplanation: Step 5: Displaying the Result\nThe final waveform and decoded transactions are shown using the open-source wavedrom library. The two input arguments (s0 and s1 ) indicate the starting and stopping location where the waveform is shown. \nThe valid range for s0 and s1 is: 0 &lt; s0 &lt; s1 &lt; (stop-start), where start and stop are defined in the last step.\nReference:\nhttps://www.npmjs.com/package/wavedrom\nEnd of explanation\n\"\"\"\n\nfrom __future__ import division\nimport math, random, re\nfrom collections import defaultdict, Counter, deque\nfrom linear_algebra import dot, get_row, get_column, make_matrix, magnitude, scalar_multiply, shape, distance\nfrom functools import partial\n\nusers = [\n    { \"id\": 0, \"name\": \"Hero\" },\n    { \"id\": 1, \"name\": \"Dunn\" },\n    { \"id\": 2, \"name\": \"Sue\" },\n    { \"id\": 3, \"name\": \"Chi\" },\n    { \"id\": 4, \"name\": \"Thor\" },\n    { \"id\": 5, \"name\": \"Clive\" },\n    { \"id\": 6, \"name\": \"Hicks\" },\n    { \"id\": 7, \"name\": \"Devin\" },\n    { \"id\": 8, \"name\": \"Kate\" },\n    { \"id\": 9, \"name\": \"Klein\" }\n]\n\n\"\"\"\nExplanation: 21장 네트워크 분석\n많은 데이터 문제는 노드(node)와 그 사이를 연결하는 엣지(edge)로 구성된 네트워크(network)의 관점에서 볼 수 있다.\n예를들어, 페이스북에서는 사용자가 노드라면 그들의 친구 관계는 엣지가 된다.\n웹에서는 각 웹페이지가 노드이고 페이지 사이를 연결하는 하이퍼링크가 엣지가 된다.\n페이스북의 친구 관계는 상호적이다.\n내가 당신과 친구라면 당신은 반드시 나와 친구이다.\n즉, 이런 경우를 엣지에 방향이 없다(undirected)고 한다.\n반면 하이퍼링크는 그렇지 않다. \n내 홈페이지에는 대한민국 국회 홈페이지에 대한 링크가 있어도,\n반대로 대한민국 국회 홈페이지에는 내 홈페이지에 대한 링크가 없을 수 있다.\n이런 네트워크에는 방향이 있기 때문에 방향성 네트워크(directed network)라고 한다.\n21.1 매개 중심성\n1장에서 우리는 데이텀 네트워크에서 친구의 수를 셈으로써 중심이 되는 주요 핵심 인물을 찾았다.\n여기서는 몇 가지 추가적인 접근법을 살펴보자.\nEnd of explanation\n\"\"\"\n\n\nfriendships = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4),\n               (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]\n\n\"\"\"\nExplanation: 네트워크는 사용자와 친구 관계를 나타낸다.\nEnd of explanation\n\"\"\"\n\n\n# give each user a friends list\nfor user in users:\n    user[\"friends\"] = []\n    \n# and populate it\nfor i, j in friendships:\n    # this works because users[i] is the user whose id is i\n    users[i][\"friends\"].append(users[j]) # add i as a friend of j\n    users[j][\"friends\"].append(users[i]) # add j as a friend of i   \n\n\"\"\"\nExplanation: 친구 목록을 각 사용자의 dict에 추가하기도 했다.\nEnd of explanation\n\"\"\"\n\n\n# \n# Betweenness Centrality\n#\n\ndef shortest_paths_from(from_user):\n    \n    # 특정 사용자로부터 다른 사용자까지의 모든 최단 경로를 포함하는 dict\n    shortest_paths_to = { from_user[\"id\"] : [[]] }\n\n    # 확인해야 하는 (이전 사용자, 다음 사용자) 큐\n    # 모든 (from_user, from_user의 친구) 쌍으로 시작\n    frontier = deque((from_user, friend)\n                     for friend in from_user[\"friends\"])\n\n    # 큐가 빌 때까지 반복\n    while frontier: \n\n        prev_user, user = frontier.popleft() # 큐의 첫 번째 사용자를\n        user_id = user[\"id\"] # 제거\n\n        # 큐에 사용자를 추가하는 방법을 고려해 보면\n        # prev_user까지의 최단 경로를 이미 알고 있을 수도 있다.\n        paths_to_prev = shortest_paths_to[prev_user[\"id\"]]\n        paths_via_prev = [path + [user_id] for path in paths_to_prev]\n        \n        # 만약 최단 경로를 이미 알고 있다면\n        old_paths_to_here = shortest_paths_to.get(user_id, [])\n        \n        # 지금까지의 최단 경로는 무엇일까?\n        if old_paths_to_here:\n            min_path_length = len(old_paths_to_here[0])\n        else:\n            min_path_length = float('inf')\n                \n        # 길지 않은 새로운 경로만 저장\n        new_paths_to_here = [path_via_prev\n                             for path_via_prev in paths_via_prev\n                             if len(path_via_prev) <= min_path_length\n                             and path_via_prev not in old_paths_to_here]\n        \n        shortest_paths_to[user_id] = old_paths_to_here + new_paths_to_here\n        \n        # 아직 한번도 보지 못한 이웃을 frontier에 추가\n        frontier.extend((user, friend)\n                        for friend in user[\"friends\"]\n                        if friend[\"id\"] not in shortest_paths_to)\n\n    return shortest_paths_to\n\n\"\"\"\nExplanation: 1장에서 연결 중심성(degree centrality)을 살펴볼 때는, 우리가 직관적으로 생각했던 주요 연결고리들이 선정되지 않아 약간 아쉬웠다.\n대안으로 사용할 수 있는 지수 중 하나는 매개 중심성(betweenness centrality)인데, 이는 두 사람 사이의 최단 경로상에 빈번하게 등장하는 사람들이 큰 값을 가지는 지수이다.\n구체적으로는, 노드 $i$의 매개 중심성은 다른 모든 노드 $j,k$ 쌍의 최단 경로 중에, $i$를 거치는 경로의 비율로 계산한다.\n임의의 두 사람이 주어졌을 때 그들 간의 최단 경로를 구해야 한다.\n이 책에서는 덜 효율적이더라도 훨씬 이해하기 쉬운 'Breadth-first search'라고도 알려진 알고리즘을 사용한다.\nEnd of explanation\n\"\"\"\n\n\nfor user in users:\n    user[\"shortest_paths\"] = shortest_paths_from(user)\n\n\"\"\"\nExplanation: 그리고 각 노드에 대해 생성된 dict들을 저장하자.\nEnd of explanation\n\"\"\"\n\n\nfor user in users:\n    user[\"betweenness_centrality\"] = 0.0\n\nfor source in users:\n    source_id = source[\"id\"]\n    for target_id, paths in source[\"shortest_paths\"].items(): # python2에서는 items 대신 iteritems 사용\n        if source_id < target_id:   # 잘못해서 두 번 세지 않도록 주의하자\n            num_paths = len(paths)  # 최단 경로가 몇 개 존재하는가?\n            contrib = 1 / num_paths # 중심성에 기여하는 값\n            for path in paths:\n                for id in path:\n                    if id not in [source_id, target_id]:\n                        users[id][\"betweenness_centrality\"] += contrib\n\nfor user in users:\n    print(user[\"id\"], user[\"betweenness_centrality\"])\n\n\"\"\"\nExplanation: 그러면 이제 매개 중심성을 구할 준비가 다 되었다.\n이제 각각의 최단 경로에 포함되는 각 노드의 매개 중심성에 $1/n$을 더해 주자.\nEnd of explanation\n\"\"\"\n\n\n#\n# closeness centrality\n#\n\ndef farness(user):\n    \"\"\"모든 사용자와의 최단 거리 합\"\"\"\n    return sum(len(paths[0]) \n               for paths in user[\"shortest_paths\"].values())\n\n\"\"\"\nExplanation: 사용자 0과 9의 최단 경로 사이에는 다른 사용자가 없으므로 매개 중심성이 0이다.\n반면 사용자 3, 4, 5는 최단 경로상에 무척 빈번하게 위치하기 때문에 높은 매개 중심성을 가진다.\n\n대게 중심성의 절댓값 자체는 큰 의미를 가지지 않고, 상대값만이 의미를 가진다.\n\n그 외에 살펴볼 수 있는 중심성 지표 중 하나는 근접 중심성(closeness centrality)이다.\n먼저 각 사용자의 원접성(farness)을 계산한다. 원접성이란 from_user와 다른 모든 사용자의 최단 경로를 합한 값이다.\nEnd of explanation\n\"\"\"\n\n\nfor user in users:\n    user[\"closeness_centrality\"] = 1 / farness(user)\n\nfor user in users:\n    print(user[\"id\"], user[\"closeness_centrality\"])\n\n\"\"\"\nExplanation: 이제 근접 중심성은 간단히 계산할 수 있다.\nEnd of explanation\n\"\"\"\n\n\ndef matrix_product_entry(A, B, i, j):\n    return dot(get_row(A, i), get_column(B, j))\n\ndef matrix_multiply(A, B):\n    n1, k1 = shape(A)\n    n2, k2 = shape(B)\n    if k1 != n2:\n        raise ArithmeticError(\"incompatible shapes!\")\n                \n    return make_matrix(n1, k2, partial(matrix_product_entry, A, B))\n\ndef vector_as_matrix(v):\n    \"\"\"(list 형태의) 벡터 v를 n x 1 행렬로 변환\"\"\"\n    return [[v_i] for v_i in v]\n    \ndef vector_from_matrix(v_as_matrix):\n    \"\"\"n x 1 행렬을 리스트로 변환\"\"\"\n    return [row[0] for row in v_as_matrix]\n\ndef matrix_operate(A, v):\n    v_as_matrix = vector_as_matrix(v)\n    product = matrix_multiply(A, v_as_matrix)\n    return vector_from_matrix(product)\n\n\"\"\"\nExplanation: 계산된 근접 중심성의 편차는 더욱 작다. 네트워크 중심에 있는 노드조차 외곽에 위치한 노드들로부터 멀리 떨어져 있기 때문이다.\n여기서 봤듯이 최단 경로를 계산하는 것은 꽤나 복잡하다. 그렇기 때문에 큰 네트워크에서는 근접 중심성을 자주 사용하지 않는다.\n덜 직관적이지만 보통 더 쉽게 계산할 수 있는 고유벡터 중심성(eigenvector centrality)을 더 자주 사용한다.\n21.2 고유벡터 중심성\n고유벡터 중심성에 대해 알아보기 전에 먼저 고유벡터가 무엇인지 살펴봐야 하고, 고유벡터가 무엇인지 알기 위해서는 먼저 행렬 연산에 대해 알아봐야 한다.\n21.2.1 행렬 연산\nEnd of explanation\n\"\"\"\n\n\ndef find_eigenvector(A, tolerance=0.00001):\n    guess = [1 for __ in A]\n\n    while True:\n        result = matrix_operate(A, guess)\n        length = magnitude(result)\n        next_guess = scalar_multiply(1/length, result)\n        \n        if distance(guess, next_guess) < tolerance:\n            return next_guess, length # eigenvector, eigenvalue\n        \n        guess = next_guess\n\n\"\"\"\nExplanation: 행렬 A의 고유 벡터를 찾기 위해, 임의의 벡터 $v$를 골라 matrix_operate를 수행하고, 결과값의 크기가 1이 되게 재조정하는 과정을 반복 수행한다.\nEnd of explanation\n\"\"\"\n\n\nrotate = [[0, 1],\n          [-1, 0]]\n\n\"\"\"\nExplanation: 결과값으로 반환되는 guess를 matrix_operate를 통해 결과값의 크기가 1인 벡터로 재조정하면, 자기 자신이 반환된다. 즉, 여기서 guess는 고유벡터라는 것을 의미한다.\n모든 실수 행렬에 고유벡터와 고유값이 있는 것은 아니다. 예를 들어 시계 방향으로 90도 회전하는 연산을 하는 다음 행렬에는 곱했을 때 가지 자신이 되는 벡터는 영벡터밖에 없다.\nEnd of explanation\n\"\"\"\n\n\nflip = [[0, 1],\n        [1, 0]]\n\n\"\"\"\nExplanation: 이 행렬로 앞서 구현한 find_eignevector(rotate)를 수행하면, 영원히 끝나지 않을 것이다.\n한편, 고유벡터가 있는 행렬도 때로는 무한루프에 빠질 수 있다.\nEnd of explanation\n\"\"\"\n\n\n#\n# eigenvector centrality\n#\n\ndef entry_fn(i, j):\n    return 1 if (i, j) in friendships or (j, i) in friendships else 0\n\nn = len(users)\nadjacency_matrix = make_matrix(n, n, entry_fn)\n\nadjacency_matrix\n\n\"\"\"\nExplanation: 이 행렬은 모든 벡터 [x, y]를 [y, x]로 변환한다. 따라서 [1, 1]은 고유값이 1인 고유벡터가 된다.\n하지만 x, y값이 다른 임의의 벡터에서 출발해서 find_eigenvector를 수행하면 x, y값을 바꾸는 연산만 무한히 수행할 것이다.\n(NumPy같은 라이브러리에는 이런 케이스까지 다룰 수 있는 다양한 방법들이 구현되어 있다.)\n이런 사소한 문제에도 불구하고, 어쨌든 find_eigenvector가 결과값을 반환한다면, 그 결과값은 곧 고유벡터이다.\n21.2.2 중심성\n고유벡터가 데이터 네트워크를 이해하는데 어떻게 도움을 줄까?\n얘기를 하기 전에 먼저 네트워크를 인접행렬(adjacency matrix)의 형태로 나타내 보자. 이 행렬은 사용자 i와 사용자 j가 친구인 경우 (i, j)번째 항목에 1이 있고, 친구가 아닌 경우 0이 있는 행렬이다.\nEnd of explanation\n\"\"\"\n\n\neigenvector_centralities, _ = find_eigenvector(adjacency_matrix)\n\nfor user_id, centrality in enumerate(eigenvector_centralities):\n    print(user_id, centrality)\n\n\"\"\"\nExplanation: 각 사용자의 고유벡터 중심성이란 find_eigenvector로 찾은 사용자의 고유벡터가 된다.\nEnd of explanation\n\"\"\"\n\n\n#\n# directed graphs\n#\n\nendorsements = [(0, 1), (1, 0), (0, 2), (2, 0), (1, 2), (2, 1), (1, 3),\n                (2, 3), (3, 4), (5, 4), (5, 6), (7, 5), (6, 8), (8, 7), (8, 9)]\n\nfor user in users:\n    user[\"endorses\"] = []       # add one list to track outgoing endorsements\n    user[\"endorsed_by\"] = []    # and another to track endorsements\n    \nfor source_id, target_id in endorsements:\n    users[source_id][\"endorses\"].append(users[target_id])\n    users[target_id][\"endorsed_by\"].append(users[source_id])\n\n\"\"\"\nExplanation: 연결의 수가 많고, 중심성이 높은 사용자들한테 연결된 사용자들은 고유벡터 중심성이 높다.\n앞의 결과에 따르면 사용자 1, 사용자 2의 중심성이 가장 높은데, 이는 중심성이 높은 사람들과 세번이나 연결되었기 때문이다.\n이들로부터 멀어질수록 사용자들의 중심성은 점차 줄어든다.\n21.3 방향성 그래프(Directed graphs)와 페이지랭크\n데이텀이 인기를 별로 끌지 못하자, 순이익 팀의 부사장은 친구 모델에서 보증(endorsement)모델로 전향하는 것을 고려 중이다.\n알고 보니 사람들은 어떤 데이터 과학자들끼리 친구인지에 대해서는 별로 관심이 없었지만, 헤드헌터들은 다른 데이터 과학자로부터 존경 받는 데이터 과학자가 누구인지에 대해 관심이 많다.\n이 새로운 모델에서 관계는 상호적인 것이 아니라, 한 사람(source)이 다른 멋진 한 사람(target)의 실력에 보증을 서주는 (source, target) 쌍으로 비대칭적인 관계를 표현하게 된다.\nEnd of explanation\n\"\"\"\n\n\nendorsements_by_id = [(user[\"id\"], len(user[\"endorsed_by\"]))\n                      for user in users]\n\nsorted(endorsements_by_id, \n       key=lambda x: x[1], # (user_id, num_endorsements)\n       reverse=True)\n\n\"\"\"\nExplanation: 그리고 가장 보증을 많이 받은 데이터 과학자들의 데이터를 수집해서, 그것을 헤드헌터들한테 팔면 된다.\nEnd of explanation\n\"\"\"\n\n\ndef page_rank(users, damping = 0.85, num_iters = 100):\n    \n    # 먼저 페이지랭크를 모든 노드에 고르게 배당\n    num_users = len(users)\n    pr = { user[\"id\"] : 1 / num_users for user in users }\n\n    # 매 스텝마다 각 노드가 받는\n    # 적은 양의 페이지랭크\n    base_pr = (1 - damping) / num_users\n    \n    for __ in range(num_iters):\n        next_pr = { user[\"id\"] : base_pr for user in users }\n        for user in users:\n            # 페이지랭크를 외부로 향하는 링크에 배당한다.\n            links_pr = pr[user[\"id\"]] * damping\n            for endorsee in user[\"endorses\"]:\n                next_pr[endorsee[\"id\"]] += links_pr / len(user[\"endorses\"])\n\n        pr = next_pr\n        \n    return pr\n\nfor user_id, pr in page_rank(users).items():\n    print(user_id, pr)\n\n\"\"\"\nExplanation: 사실 '보증의 수'와 같은 숫자는 조작하기가 매우 쉽다.\n가장 간단한 방법 중 하나는, 가짜 계정을 여러 개 만들어서 그것들로 내 계정에 대한 보증을 서는 것이다.\n또 다른 방법은, 친구들끼리 짜고 서로가 서로를 보증해 주는 것이다. (아마 사용자 0, 1, 2가 이런 관계일 가능성이 크다.)\n좀 더 나은 지수는, '누가' 보증을 서는지를 고려하는 것이다.\n보증을 많이 받은 사용자가 보증을 설 때는, 보증을 적게 받은 사용자가 보증을 설 때보다 더 중요한 것으로 받아들여지는 것이 타당하다.\n그리고 사실 이것은 유명한 페이지랭크(PageRank) 알고리즘의 기본 철학이기도 하다.\n1. 네트워크 전체에는 1.0(또는 100%)의 페이지랭크가 있다.\n2. 초기에 이 페이지랭크를 모든 노드에 고르게 배당한다.\n3. 각 스텝을 거칠 때마다 각 노드에 배당된 페이지랭크의 대부분은 외부로 향하는 링크에 균등하게 배당한다.\n4. 각 스텝을 거칠 때마다 각 노드에 남아 있는 페이지랭크를 모든 노드에 고르게 배당한다.\nEnd of explanation\n\"\"\"\n\nversion = '2020-08-25'\n\n\"\"\"\nExplanation: <div style=\"width:100%; background-color: #D9EDF7; border: 1px solid #CFCFCF; text-align: left; padding: 10px;\">\n      <b>Renewable power plants: Download and process notebook</b>\n      <ul>\n        <li><a href=\"main.ipynb\">Main notebook</a></li>\n        <li>Download and process notebook</li>\n        <li><a href=\"validation_and_output.ipynb\">Validation and output notebook</a></li>\n      </ul>\n      <br>This notebook is part of the <a href=\"http://data.open-power-system-data.org/renewable_power_plants\"> Renewable power plants Data Package</a> of <a href=\"http://open-power-system-data.org\">Open Power System Data</a>.\n</div>\n\nThis script downlads and extracts the original data of renewable power plant lists from the data sources, processes and merges them. It subsequently adds the geolocation for each power plant. Finally it saves the DataFrames as pickle-files. Make sure you run the download and process Notebook before the validation and output Notebook.\n<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Script-setup\" data-toc-modified-id=\"Script-setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Script setup</a></span></li><li><span><a href=\"#Settings\" data-toc-modified-id=\"Settings-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Settings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Choose-download-option\" data-toc-modified-id=\"Choose-download-option-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Choose download option</a></span></li><li><span><a href=\"#Update-the-download-links\" data-toc-modified-id=\"Update-the-download-links-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Update the download links</a></span></li><li><span><a href=\"#Set-up-the-downloader-for-data-sources\" data-toc-modified-id=\"Set-up-the-downloader-for-data-sources-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Set up the downloader for data sources</a></span></li><li><span><a href=\"#Set-up-the-NUTS-converter\" data-toc-modified-id=\"Set-up-the-NUTS-converter-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Set up the NUTS converter</a></span></li><li><span><a href=\"#Setup-translation-dictionaries\" data-toc-modified-id=\"Setup-translation-dictionaries-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Setup translation dictionaries</a></span></li></ul></li><li><span><a href=\"#Download-and-process-per-country\" data-toc-modified-id=\"Download-and-process-per-country-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Download and process per country</a></span><ul class=\"toc-item\"><li><span><a href=\"#Germany-DE\" data-toc-modified-id=\"Germany-DE-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Germany DE</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-read\" data-toc-modified-id=\"Download-and-read-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Download and read</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Add-information-and-choose-columns\" data-toc-modified-id=\"Add-information-and-choose-columns-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Add information and choose columns</a></span></li><li><span><a href=\"#Merge-DataFrames\" data-toc-modified-id=\"Merge-DataFrames-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>Merge DataFrames</a></span></li><li><span><a href=\"#Translate-values-and-harmonize-energy-source-level-2\" data-toc-modified-id=\"Translate-values-and-harmonize-energy-source-level-2-3.1.5\"><span class=\"toc-item-num\">3.1.5&nbsp;&nbsp;</span>Translate values and harmonize energy source level 2</a></span></li><li><span><a href=\"#Transform-electrical-capacity-from-kW-to-MW\" data-toc-modified-id=\"Transform-electrical-capacity-from-kW-to-MW-3.1.6\"><span class=\"toc-item-num\">3.1.6&nbsp;&nbsp;</span>Transform electrical capacity from kW to MW</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.1.7\"><span class=\"toc-item-num\">3.1.7&nbsp;&nbsp;</span>Georeferencing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-coordinates-by-postcode\" data-toc-modified-id=\"Get-coordinates-by-postcode-3.1.7.1\"><span class=\"toc-item-num\">3.1.7.1&nbsp;&nbsp;</span>Get coordinates by postcode</a></span></li><li><span><a href=\"#Transform-geoinformation\" data-toc-modified-id=\"Transform-geoinformation-3.1.7.2\"><span class=\"toc-item-num\">3.1.7.2&nbsp;&nbsp;</span>Transform geoinformation</a></span></li></ul></li><li><span><a href=\"#Clean-data\" data-toc-modified-id=\"Clean-data-3.1.8\"><span class=\"toc-item-num\">3.1.8&nbsp;&nbsp;</span>Clean data</a></span></li><li><span><a href=\"#Assign-NUTS-codes\" data-toc-modified-id=\"Assign-NUTS-codes-3.1.9\"><span class=\"toc-item-num\">3.1.9&nbsp;&nbsp;</span>Assign NUTS codes</a></span></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-3.1.10\"><span class=\"toc-item-num\">3.1.10&nbsp;&nbsp;</span>Visualize</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.1.11\"><span class=\"toc-item-num\">3.1.11&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#Denmark-DK\" data-toc-modified-id=\"Denmark-DK-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Denmark DK</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-read\" data-toc-modified-id=\"Download-and-read-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Download and read</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Add-data-source-and-missing-information\" data-toc-modified-id=\"Add-data-source-and-missing-information-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Add data source and missing information</a></span></li><li><span><a href=\"#Correct-the-dates\" data-toc-modified-id=\"Correct-the-dates-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Correct the dates</a></span></li><li><span><a href=\"#Translate-values-and-harmonize-energy-source-level-2\" data-toc-modified-id=\"Translate-values-and-harmonize-energy-source-level-2-3.2.5\"><span class=\"toc-item-num\">3.2.5&nbsp;&nbsp;</span>Translate values and harmonize energy source level 2</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.2.6\"><span class=\"toc-item-num\">3.2.6&nbsp;&nbsp;</span>Georeferencing</a></span></li><li><span><a href=\"#Merge-DataFrames,-add-NUTS-information-and-choose-columns\" data-toc-modified-id=\"Merge-DataFrames,-add-NUTS-information-and-choose-columns-3.2.7\"><span class=\"toc-item-num\">3.2.7&nbsp;&nbsp;</span>Merge DataFrames, add NUTS information and choose columns</a></span></li><li><span><a href=\"#Select-columns\" data-toc-modified-id=\"Select-columns-3.2.8\"><span class=\"toc-item-num\">3.2.8&nbsp;&nbsp;</span>Select columns</a></span></li><li><span><a href=\"#Remove-duplicate-rows\" data-toc-modified-id=\"Remove-duplicate-rows-3.2.9\"><span class=\"toc-item-num\">3.2.9&nbsp;&nbsp;</span>Remove duplicate rows</a></span></li><li><span><a href=\"#Transform-electrical_capacity-from-kW-to-MW\" data-toc-modified-id=\"Transform-electrical_capacity-from-kW-to-MW-3.2.10\"><span class=\"toc-item-num\">3.2.10&nbsp;&nbsp;</span>Transform electrical_capacity from kW to MW</a></span></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-3.2.11\"><span class=\"toc-item-num\">3.2.11&nbsp;&nbsp;</span>Visualize</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.2.12\"><span class=\"toc-item-num\">3.2.12&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#France-FR\" data-toc-modified-id=\"France-FR-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>France FR</a></span><ul class=\"toc-item\"><li><span><a href=\"#ODRE-data\" data-toc-modified-id=\"ODRE-data-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>ODRE data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-the-data\" data-toc-modified-id=\"Load-the-data-3.3.1.1\"><span class=\"toc-item-num\">3.3.1.1&nbsp;&nbsp;</span>Load the data</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.3.1.2\"><span class=\"toc-item-num\">3.3.1.2&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Add-data-source\" data-toc-modified-id=\"Add-data-source-3.3.1.3\"><span class=\"toc-item-num\">3.3.1.3&nbsp;&nbsp;</span>Add data source</a></span></li><li><span><a href=\"#Translate-values\" data-toc-modified-id=\"Translate-values-3.3.1.4\"><span class=\"toc-item-num\">3.3.1.4&nbsp;&nbsp;</span>Translate values</a></span></li><li><span><a href=\"#Correct-site-names\" data-toc-modified-id=\"Correct-site-names-3.3.1.5\"><span class=\"toc-item-num\">3.3.1.5&nbsp;&nbsp;</span>Correct site names</a></span></li><li><span><a href=\"#Replace-suspicious-dates-with-N/A\" data-toc-modified-id=\"Replace-suspicious-dates-with-N/A-3.3.1.6\"><span class=\"toc-item-num\">3.3.1.6&nbsp;&nbsp;</span>Replace suspicious dates with N/A</a></span></li><li><span><a href=\"#Check-missing-values\" data-toc-modified-id=\"Check-missing-values-3.3.1.7\"><span class=\"toc-item-num\">3.3.1.7&nbsp;&nbsp;</span>Check missing values</a></span></li><li><span><a href=\"#Standardize-the-energy-types-and-technologies\" data-toc-modified-id=\"Standardize-the-energy-types-and-technologies-3.3.1.8\"><span class=\"toc-item-num\">3.3.1.8&nbsp;&nbsp;</span>Standardize the energy types and technologies</a></span></li><li><span><a href=\"#Standardize-source-levels-1-3-and-technology\" data-toc-modified-id=\"Standardize-source-levels-1-3-and-technology-3.3.1.9\"><span class=\"toc-item-num\">3.3.1.9&nbsp;&nbsp;</span>Standardize source levels 1-3 and technology</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.3.1.10\"><span class=\"toc-item-num\">3.3.1.10&nbsp;&nbsp;</span>Georeferencing</a></span></li><li><span><a href=\"#Convert-electrical-capacity-to-MW\" data-toc-modified-id=\"Convert-electrical-capacity-to-MW-3.3.1.11\"><span class=\"toc-item-num\">3.3.1.11&nbsp;&nbsp;</span>Convert electrical capacity to MW</a></span></li></ul></li><li><span><a href=\"#Old-data\" data-toc-modified-id=\"Old-data-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Old data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Add-data-source\" data-toc-modified-id=\"Add-data-source-3.3.2.1\"><span class=\"toc-item-num\">3.3.2.1&nbsp;&nbsp;</span>Add data source</a></span></li><li><span><a href=\"#Translate-values-and-harmonize-energy-source-level-2\" data-toc-modified-id=\"Translate-values-and-harmonize-energy-source-level-2-3.3.2.2\"><span class=\"toc-item-num\">3.3.2.2&nbsp;&nbsp;</span>Translate values and harmonize energy source level 2</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.3.2.3\"><span class=\"toc-item-num\">3.3.2.3&nbsp;&nbsp;</span>Georeferencing</a></span></li></ul></li><li><span><a href=\"#Integrate-old-and-new-data\" data-toc-modified-id=\"Integrate-old-and-new-data-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Integrate old and new data</a></span></li><li><span><a href=\"#Select-the-columns\" data-toc-modified-id=\"Select-the-columns-3.3.4\"><span class=\"toc-item-num\">3.3.4&nbsp;&nbsp;</span>Select the columns</a></span></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-3.3.5\"><span class=\"toc-item-num\">3.3.5&nbsp;&nbsp;</span>Visualize</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.3.6\"><span class=\"toc-item-num\">3.3.6&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#Poland-PL\" data-toc-modified-id=\"Poland-PL-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Poland PL</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download\" data-toc-modified-id=\"Download-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Download</a></span></li><li><span><a href=\"#Load-and-explore-the-data\" data-toc-modified-id=\"Load-and-explore-the-data-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Load and explore the data</a></span></li><li><span><a href=\"#Inspect-the-data\" data-toc-modified-id=\"Inspect-the-data-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>Inspect the data</a></span></li><li><span><a href=\"#Harmonising-energy-levels\" data-toc-modified-id=\"Harmonising-energy-levels-3.4.4\"><span class=\"toc-item-num\">3.4.4&nbsp;&nbsp;</span>Harmonising energy levels</a></span></li><li><span><a href=\"#Georeferencing-(NUTS-classification)\" data-toc-modified-id=\"Georeferencing-(NUTS-classification)-3.4.5\"><span class=\"toc-item-num\">3.4.5&nbsp;&nbsp;</span>Georeferencing (NUTS classification)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Add-NUTS-information\" data-toc-modified-id=\"Add-NUTS-information-3.4.5.1\"><span class=\"toc-item-num\">3.4.5.1&nbsp;&nbsp;</span>Add NUTS information</a></span></li></ul></li><li><span><a href=\"#Add-data-source-and-year\" data-toc-modified-id=\"Add-data-source-and-year-3.4.6\"><span class=\"toc-item-num\">3.4.6&nbsp;&nbsp;</span>Add data source and year</a></span></li><li><span><a href=\"#Select-columns\" data-toc-modified-id=\"Select-columns-3.4.7\"><span class=\"toc-item-num\">3.4.7&nbsp;&nbsp;</span>Select columns</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.4.8\"><span class=\"toc-item-num\">3.4.8&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#Switzerland-CH\" data-toc-modified-id=\"Switzerland-CH-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Switzerland CH</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-read\" data-toc-modified-id=\"Download-and-read-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Download and read</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.5.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Add-data-source\" data-toc-modified-id=\"Add-data-source-3.5.3\"><span class=\"toc-item-num\">3.5.3&nbsp;&nbsp;</span>Add data source</a></span></li><li><span><a href=\"#Harmonize-energy-source-hierarchy-and-translate-values\" data-toc-modified-id=\"Harmonize-energy-source-hierarchy-and-translate-values-3.5.4\"><span class=\"toc-item-num\">3.5.4&nbsp;&nbsp;</span>Harmonize energy source hierarchy and translate values</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.5.5\"><span class=\"toc-item-num\">3.5.5&nbsp;&nbsp;</span>Georeferencing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Postcode-to-lat/lon-(WGS84)\" data-toc-modified-id=\"Postcode-to-lat/lon-(WGS84)-3.5.5.1\"><span class=\"toc-item-num\">3.5.5.1&nbsp;&nbsp;</span>Postcode to lat/lon (WGS84)</a></span></li><li><span><a href=\"#Add-NUTS-information\" data-toc-modified-id=\"Add-NUTS-information-3.5.5.2\"><span class=\"toc-item-num\">3.5.5.2&nbsp;&nbsp;</span>Add NUTS information</a></span></li></ul></li><li><span><a href=\"#Transform-electrical_capacity-from-kW-to-MW\" data-toc-modified-id=\"Transform-electrical_capacity-from-kW-to-MW-3.5.6\"><span class=\"toc-item-num\">3.5.6&nbsp;&nbsp;</span>Transform electrical_capacity from kW to MW</a></span></li><li><span><a href=\"#Select-columns-to-keep\" data-toc-modified-id=\"Select-columns-to-keep-3.5.7\"><span class=\"toc-item-num\">3.5.7&nbsp;&nbsp;</span>Select columns to keep</a></span></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-3.5.8\"><span class=\"toc-item-num\">3.5.8&nbsp;&nbsp;</span>Visualize</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.5.9\"><span class=\"toc-item-num\">3.5.9&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#United-Kingdom-UK\" data-toc-modified-id=\"United-Kingdom-UK-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>United Kingdom UK</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-Read\" data-toc-modified-id=\"Download-and-Read-3.6.1\"><span class=\"toc-item-num\">3.6.1&nbsp;&nbsp;</span>Download and Read</a></span></li><li><span><a href=\"#Clean-the-data\" data-toc-modified-id=\"Clean-the-data-3.6.2\"><span class=\"toc-item-num\">3.6.2&nbsp;&nbsp;</span>Clean the data</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.6.3\"><span class=\"toc-item-num\">3.6.3&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Add-data-source\" data-toc-modified-id=\"Add-data-source-3.6.4\"><span class=\"toc-item-num\">3.6.4&nbsp;&nbsp;</span>Add data source</a></span></li><li><span><a href=\"#Translate-values-and-harmonise-energy-source-levels-1-3-and-technology\" data-toc-modified-id=\"Translate-values-and-harmonise-energy-source-levels-1-3-and-technology-3.6.5\"><span class=\"toc-item-num\">3.6.5&nbsp;&nbsp;</span>Translate values and harmonise energy source levels 1-3 and technology</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.6.6\"><span class=\"toc-item-num\">3.6.6&nbsp;&nbsp;</span>Georeferencing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cases-with-unknown-Easting-and-Northing-coordinates\" data-toc-modified-id=\"Cases-with-unknown-Easting-and-Northing-coordinates-3.6.6.1\"><span class=\"toc-item-num\">3.6.6.1&nbsp;&nbsp;</span>Cases with unknown Easting and Northing coordinates</a></span></li><li><span><a href=\"#Cases-for-approximation\" data-toc-modified-id=\"Cases-for-approximation-3.6.6.2\"><span class=\"toc-item-num\">3.6.6.2&nbsp;&nbsp;</span>Cases for approximation</a></span></li><li><span><a href=\"#Add-NUTS-information\" data-toc-modified-id=\"Add-NUTS-information-3.6.6.3\"><span class=\"toc-item-num\">3.6.6.3&nbsp;&nbsp;</span>Add NUTS information</a></span></li><li><span><a href=\"#Visualize-the-data\" data-toc-modified-id=\"Visualize-the-data-3.6.6.4\"><span class=\"toc-item-num\">3.6.6.4&nbsp;&nbsp;</span>Visualize the data</a></span></li></ul></li><li><span><a href=\"#Keep-only-the-columns-of-interest\" data-toc-modified-id=\"Keep-only-the-columns-of-interest-3.6.7\"><span class=\"toc-item-num\">3.6.7&nbsp;&nbsp;</span>Keep only the columns of interest</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.6.8\"><span class=\"toc-item-num\">3.6.8&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#Sweden\" data-toc-modified-id=\"Sweden-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Sweden</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-the-data\" data-toc-modified-id=\"Load-the-data-3.7.1\"><span class=\"toc-item-num\">3.7.1&nbsp;&nbsp;</span>Load the data</a></span></li><li><span><a href=\"#Clean-the-data\" data-toc-modified-id=\"Clean-the-data-3.7.2\"><span class=\"toc-item-num\">3.7.2&nbsp;&nbsp;</span>Clean the data</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.7.3\"><span class=\"toc-item-num\">3.7.3&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Correct-the-dates\" data-toc-modified-id=\"Correct-the-dates-3.7.4\"><span class=\"toc-item-num\">3.7.4&nbsp;&nbsp;</span>Correct the dates</a></span></li><li><span><a href=\"#Add-source\" data-toc-modified-id=\"Add-source-3.7.5\"><span class=\"toc-item-num\">3.7.5&nbsp;&nbsp;</span>Add source</a></span></li><li><span><a href=\"#Translate-values-and-harmonize-energy-source-levels\" data-toc-modified-id=\"Translate-values-and-harmonize-energy-source-levels-3.7.6\"><span class=\"toc-item-num\">3.7.6&nbsp;&nbsp;</span>Translate values and harmonize energy source levels</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.7.7\"><span class=\"toc-item-num\">3.7.7&nbsp;&nbsp;</span>Georeferencing</a></span></li><li><span><a href=\"#Assigning-NUTS-codes\" data-toc-modified-id=\"Assigning-NUTS-codes-3.7.8\"><span class=\"toc-item-num\">3.7.8&nbsp;&nbsp;</span>Assigning NUTS codes</a></span></li><li><span><a href=\"#Select-the-columns-to-keep\" data-toc-modified-id=\"Select-the-columns-to-keep-3.7.9\"><span class=\"toc-item-num\">3.7.9&nbsp;&nbsp;</span>Select the columns to keep</a></span></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-3.7.10\"><span class=\"toc-item-num\">3.7.10&nbsp;&nbsp;</span>Visualize</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.7.11\"><span class=\"toc-item-num\">3.7.11&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#Czech-Republic\" data-toc-modified-id=\"Czech-Republic-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Czech Republic</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-read-the-data\" data-toc-modified-id=\"Download-and-read-the-data-3.8.1\"><span class=\"toc-item-num\">3.8.1&nbsp;&nbsp;</span>Download and read the data</a></span></li><li><span><a href=\"#Clean-the-data\" data-toc-modified-id=\"Clean-the-data-3.8.2\"><span class=\"toc-item-num\">3.8.2&nbsp;&nbsp;</span>Clean the data</a></span></li><li><span><a href=\"#Reformat-the-data\" data-toc-modified-id=\"Reformat-the-data-3.8.3\"><span class=\"toc-item-num\">3.8.3&nbsp;&nbsp;</span>Reformat the data</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.8.4\"><span class=\"toc-item-num\">3.8.4&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Translate-values-and-harmonize-energy-levels\" data-toc-modified-id=\"Translate-values-and-harmonize-energy-levels-3.8.5\"><span class=\"toc-item-num\">3.8.5&nbsp;&nbsp;</span>Translate values and harmonize energy levels</a></span></li><li><span><a href=\"#Add-data-source\" data-toc-modified-id=\"Add-data-source-3.8.6\"><span class=\"toc-item-num\">3.8.6&nbsp;&nbsp;</span>Add data source</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.8.7\"><span class=\"toc-item-num\">3.8.7&nbsp;&nbsp;</span>Georeferencing</a></span></li><li><span><a href=\"#Assign-NUTS-codes\" data-toc-modified-id=\"Assign-NUTS-codes-3.8.8\"><span class=\"toc-item-num\">3.8.8&nbsp;&nbsp;</span>Assign NUTS codes</a></span></li><li><span><a href=\"#Select-the-columns-to-keep\" data-toc-modified-id=\"Select-the-columns-to-keep-3.8.9\"><span class=\"toc-item-num\">3.8.9&nbsp;&nbsp;</span>Select the columns to keep</a></span></li><li><span><a href=\"#Drop-duplicates\" data-toc-modified-id=\"Drop-duplicates-3.8.10\"><span class=\"toc-item-num\">3.8.10&nbsp;&nbsp;</span>Drop duplicates</a></span></li><li><span><a href=\"#Visualuze\" data-toc-modified-id=\"Visualuze-3.8.11\"><span class=\"toc-item-num\">3.8.11&nbsp;&nbsp;</span>Visualuze</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.8.12\"><span class=\"toc-item-num\">3.8.12&nbsp;&nbsp;</span>Save</a></span></li></ul></li></ul></li><li><span><a href=\"#Zip-the-raw-data\" data-toc-modified-id=\"Zip-the-raw-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Zip the raw data</a></span></li></ul></div>\nEnd of explanation\n\"\"\"\n\n\nimport logging\nimport os\nimport posixpath\nimport urllib.parse\nimport urllib.request\nimport re\nimport zipfile\nimport pickle\nimport urllib\nimport shutil\nimport datetime\n\nimport numpy as np\nimport pandas as pd\nimport utm  # for transforming geoinformation in the utm format\nimport requests\nimport fake_useragent\nfrom string import Template\nfrom IPython.display import display\nimport xlrd\nimport bs4\nimport bng_to_latlon\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n\n# for visualizing locations on maps\nimport cartopy.crs as ccrs \nimport cartopy.feature as cfeature\nfrom cartopy.io import shapereader\nimport geopandas\nimport shapely\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    datefmt='%d %b %Y %H:%M:%S'\n)\n\nlogger = logging.getLogger()\n\n# Create input, intermediate and output folders if they don't exist.\n# If the paths are relative, the correspoding folders will be created\n# inside the current working directory.\ninput_directory_path = os.path.join('input', 'original_data')\nintermediate_directory_path = 'intermediate'\noutput_directory_path = os.path.join('output', 'renewable_power_plants')\n\nos.makedirs(input_directory_path, exist_ok=True)\nos.makedirs(intermediate_directory_path, exist_ok=True)\nos.makedirs(output_directory_path, exist_ok=True)\n\n# Create the folder to which the Eurostat files with data at the level of the whole EU/Europe\n#are going to be downloaded\neurostat_eu_directory_path = os.path.join('input', 'eurostat_eu')\nos.makedirs(eurostat_eu_directory_path, exist_ok=True)\n\n# Define the path of the file with the list of sources.\nsource_list_filepath = os.path.join('input', 'sources.csv')\n\n# Import the utility functions and classes from the util package\nimport util.helper\nfrom util.visualizer import visualize_points\n\n\"\"\"\nExplanation: Script setup\nEnd of explanation\n\"\"\"\n\n\ndownload_from = 'original_sources'\n#download_from = 'opsd_server' \n\n\"\"\"\nExplanation: Settings\nChoose download option\nThe original data can either be downloaded from the original data sources as specified below or from the opsd-Server. Default option is to download from the original sources as the aim of the project is to stay as close to original sources as possible. However, if problems with downloads e.g. due to changing urls occur, you can still run the script with the original data from the opsd_server.\nEnd of explanation\n\"\"\"\n\n\nsource_df = pd.read_csv(source_list_filepath)\nuk_main_page = 'https://www.gov.uk/government/publications/renewable-energy-planning-database-monthly-extract'\ncurrent_link = util.helper.get_beis_link(uk_main_page)\ncurrent_filename = current_link.split('/')[-1]\n\nsource_df.loc[(source_df['country'] == 'UK') & (source_df['source'] == 'BEIS'), 'url'] = current_link\nsource_df.loc[(source_df['country'] == 'UK') & (source_df['source'] == 'BEIS'), 'filename'] = current_filename\nsource_df.to_csv(source_list_filepath, index=False, header=True)\n\nsource_df.fillna('')\n\n\"\"\"\nExplanation: Update the download links\nThe download link for the UK is updated at the end of each quarter by the source provider, BEIS. We keep up with those changes by extracting the download link automatically from the web page it is on. That way, the link does not have to be updated manually.\nNote: you must be connected to the Internet if you want to execute this step.\nEnd of explanation\n\"\"\"\n\n\nimport util.downloader\nfrom util.downloader import Downloader\ndownloader = Downloader(version, input_directory_path, source_list_filepath, download_from)\n\n\"\"\"\nExplanation: Note that, as of August 25, 2020, the following sources are available only from the OPSD server and the data will be downloaded from it even if download_from is set to 'original_sources':\n- Energinet (DK)\n- Eurostat files which contain correspondence tables between postal codes and NUTS.\nThe original links which should be downloaded from OPSD are marked as inactive in the column active in the above dataframe.\nSet up the downloader for data sources\nThe Downloader class in the util package is responsible for downloading the original files to appropriate folders. In order to access its functionality, we have to instantiate it first.\nEnd of explanation\n\"\"\"\n\n\n#import importlib\n#importlib.reload(util.nuts_converter)\n#importlib.reload(util.downloader)\n#from util.downloader import Downloader\n#downloader = Downloader(version, input_directory_path, source_list_filepath, download_from)\nfrom util.nuts_converter import NUTSConverter\nnuts_converter = NUTSConverter(downloader, eurostat_eu_directory_path)\n\n\"\"\"\nExplanation: Set up the NUTS converter\nThe NUTSConverter class in the util package uses the information on each facility's  postcode, municipalty name, municipality code, longitude, and latitude to assign it correct NUTS 2016 level 1, 2, and 3 codes.\nHere, we instantiate the converter so that we can use it later.\nEnd of explanation\n\"\"\"\n\n\n# Get column translation list\ncolumnnames = pd.read_csv(os.path.join('input', 'column_translation_list.csv'))\ncolumnnames.head(2)\n\n# Get value translation list\nvaluenames = pd.read_csv(os.path.join('input', 'value_translation_list.csv'))\nvaluenames.head(2)\n\n\"\"\"\nExplanation: Setup translation dictionaries\nColumn and value names of the original data sources will be translated to English and standardized across different sources. Standardized column names, e.g. \"electrical_capacity\" are required to merge data in one DataFrame.<br>\nThe column and the value translation lists are provided in the input folder of the Data Package.\nEnd of explanation\n\"\"\"\n\n\n# Define the lists of source names\ndownloader = Downloader(version, input_directory_path, source_list_filepath, download_from)\n\ntsos = ['50Hertz', 'Amprion', 'TenneT', 'TransnetBW']\ndatasets = ['50Hertz', 'Amprion', 'TenneT', 'TransnetBW','bnetza','bnetza_pv','bnetza_pv_historic']\n\n# Download the files and get the local file paths indexed by source names\nfilepaths = downloader.download_data_for_country('DE')\n\n# Remove the Eurostat NUTS file as it's a geoinformation source\nDE_postcode2nuts_filepath = filepaths.pop('Eurostat')\n\n# Open all data sets before processing.\nfilenames = {}\n\nfor source in filepaths:\n    filepath = filepaths[source]\n    print(source, filepath)\n    if os.path.splitext(filepath)[1] != '.xlsx' and zipfile.is_zipfile(filepath):\n        filenames[source] = zipfile.ZipFile(filepath)\n    else:\n        filenames[source] = filepath\n\n# Read TSO data from the zip files\ndfs = {}\n\nbasenames_by_tso = {\n    '50Hertz': '50Hertz Transmission GmbH EEG-Zahlungen Stammdaten 2019',\n    'Amprion': 'Amprion GmbH EEG-Zahlungen Anlagenstammdaten 2019',\n    'TenneT': 'TenneT TSO GmbH Anlagenstammdaten 2019',\n    'TransnetBW': 'TransnetBW GmbH Anlagenstammdaten 2019',\n}\n        \nfor tso in tsos:\n    filename = basenames_by_tso[tso]+'.csv'\n    print('Reading', filename)\n    #print(filenames[tso].namelist())\n    dfs[tso] = pd.read_csv(\n        filenames[tso].open(filename),\n        sep=';',\n        thousands='.',\n        decimal=',',\n\n        # Headers have to have the same order for all TSOs. Therefore just define headers here.\n        # Remove the following three lines if for next version, headers should be read out initially \n        # to then check if order is the same everywhere.\n        names=['EEG-Anlagenschlüssel', 'MASTR_Nr_EEG','Netzbetreiber Betriebsnummer','Netzbetreiber Name',\n               'Strasse_flurstueck','PLZ','Ort / Gemarkung','Gemeindeschlüssel','Bundesland',\n               'Installierte Leistung','Energieträger','Spannungsebene','Leistungsmessung','Regelbarkeit',\n               'Inbetriebnahme','Außerbetriebnahme','Netzzugang','Netzabgang'],\n        header=None,\n        skiprows=1,\n        parse_dates=[14, 15, 16, 17], #[11, 12, 13, 14]\n        #infer_datetime_format=True,\n        date_parser = lambda x: pd.to_datetime(x, errors='coerce', format='%d.%m.%Y'),\n        encoding='iso-8859-1',\n        dayfirst=True,\n        low_memory=False\n    )\n    print('Done reading ' + filename)\n\nfor filename in filenames.values():\n    if(isinstance(filename, zipfile.ZipFile)):\n        #print(filename)\n        filename.close()\n\n# define the date parser\ndef date_parser(x):\n    if type(x) == str:\n        return datetime.datetime.strptime(x, '%D.%M.%Y')\n    elif type(x) == float and pd.isnull(x):\n        return pd.NaT\n    \ndef inspect(x):\n    try:\n        converted = datetime.datetime.strptime(x, '%d.%m.%Y')\n        return False\n    except:\n        return True\n\n# Read BNetzA register\nprint('Reading bnetza: '+filenames['bnetza'])\ndfs['bnetza'] = pd.read_excel(filenames['bnetza'],\n                          sheet_name='Gesamtübersicht',\n                          header=0,\n                          converters={'4.9 Postleit-zahl': str, 'Gemeinde-Schlüssel': str}\n)\n\nskiprows = {'bnetza_pv_historic': 10, 'bnetza_pv': 9}\n\nfor dataset in ['bnetza_pv', 'bnetza_pv_historic']:\n    print(dataset)\n    print('Reading ' + dataset + ': ' + filenames[dataset])\n    xls_handle = pd.ExcelFile(filenames[dataset])\n    print('Concatenating all '+dataset+' sheets into one dataframe')\n    dfs[dataset] = pd.concat(\n        (xls_handle.parse(\n            sheet,\n            skiprows=skiprows[dataset],\n            converters={'Anlage \\nPLZ': str}\n        ) for sheet in xls_handle.sheet_names),\n        sort=True\n    )\n\n# Make sure that the column `Inbetriebnahme-datum *)` (commissioning date) in the bnetza_pv set is datetime.\nmask = dfs['bnetza_pv']['Inbetriebnahme-datum *)'].apply(lambda x: type(x) == int)\n\ndfs['bnetza_pv']['Inbetriebnahme-datum *)'] = pd.to_datetime(dfs['bnetza_pv']['Inbetriebnahme-datum *)'],\n                                                             errors='coerce',\n                                                             dayfirst=True,\n                                                             infer_datetime_format=True)\ndfs['bnetza_pv']['Inbetriebnahme-datum *)'] = dfs['bnetza_pv']['Inbetriebnahme-datum *)'].apply(\n    lambda x: x.to_datetime64()\n)\n\ndfs['bnetza_pv_historic'] = dfs['bnetza_pv_historic'].drop(['Unnamed: 7'], axis=1)\n\npickle.dump( dfs, open( \"intermediate/temp_dfs_DE_after_reading.pickle\", \"wb\" ) )\n\ndfs = pickle.load( open( \"intermediate/temp_dfs_DE_after_reading.pickle\", \"rb\" ) )\n\n\"\"\"\nExplanation: Download and process per country\nFor one country after the other, the original data is downloaded, read, processed, translated, eventually georeferenced and saved. If respective files are already in the local folder, these will be utilized.\nTo process the provided data pandas DataFrame is applied.<br>\nGermany DE\nDownload and read\nThe data which will be processed below is provided by the following data sources:\nNetztransparenz.de - Official grid transparency platform from the German Transmission System Operators (TSOs): 50Hertz, Amprion, TenneT and TransnetBW.\nBundesnetzagentur (BNetzA) - German Federal Network Agency for Electricity, Gas, Telecommunications, Posts and Railway (In separate files for data for roof-mounted PV power plants and for all other renewable energy power plants.)\nData URL for BNetzA gets updated every few month. To be sure, always check if the links (url_bnetza; url_bnetza_pv) are up to date.\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for Germany, create dictionary and show dictionary\ncolumnnames = pd.read_csv(os.path.join('input', 'column_translation_list.csv'))\nidx_DE = columnnames[columnnames['country'] == 'DE'].index\ncolumn_dict_DE = columnnames.loc[idx_DE].set_index('original_name')['opsd_name'].to_dict()\ncolumn_dict_DE\n\n# Start the column translation process for each original data source\nprint('Translation...')\nfor dataset in dfs:\n    # Remove newlines and any other duplicate whitespaces in column names:\n    dfs[dataset] = dfs[dataset].rename(columns={col: re.sub(r\"\\s+\", ' ', col) for col in dfs[dataset].columns})\n    # Do column name translations\n    print(dataset)\n    #print(list(dfs[dataset].columns))\n    dfs[dataset].rename(columns=column_dict_DE, inplace=True)\n    #print(list(dfs[dataset].columns).index('decommissioning_date'))\n    #print('--------------------------------------------')\n\nprint('done.')\n\n\"\"\"\nExplanation: Translate column names\nTo standardise the DataFrame the original column names from the German TSOs and the BNetzA wil be translated and new English column names wil be assigned to the DataFrame. The unique column names are required to merge the DataFrame.<br>\nThe column_translation_list is provided here as csv in the input folder. It is loaded in 2.3 Setup of translation dictionaries.\nEnd of explanation\n\"\"\"\n\n\n# Add data source names to the DataFrames\nfor tso in tsos:\n    dfs[tso]['data_source'] = tso\n    dfs[tso]['tso'] = tso\n\ndfs['bnetza']['data_source'] = 'BNetzA'\ndfs['bnetza_pv']['data_source'] = 'BNetzA_PV'\ndfs['bnetza_pv_historic']['data_source'] = 'BNetzA_PV_historic'\n\n# Add for the BNetzA PV data the energy source level 2\ndfs['bnetza_pv']['energy_source_level_2'] = 'Photovoltaics'\ndfs['bnetza_pv_historic']['energy_source_level_2'] = 'Photovoltaics'\n\n# Select those columns of the original data which are utilised further\ndfs['bnetza'] = dfs['bnetza'].loc[:, ('commissioning_date', 'decommissioning_date',\n                              'notification_reason', 'energy_source_level_2',\n                              'electrical_capacity_kW', 'thermal_capacity_kW',\n                              'voltage_level', 'dso', 'eeg_id', 'bnetza_id',\n                              'federal_state', 'postcode', 'municipality_code',\n                              'municipality', 'address', 'address_number',\n                              'utm_zone', 'utm_east', 'utm_north',\n                              'data_source')]\n\nfor dataset in datasets: print(dataset+':'); display(dfs[dataset].tail(2))\n\n\"\"\"\nExplanation: Add information and choose columns\nAll data source names and for the BNetzA-PV data the energy source level 2 will added.\nEnd of explanation\n\"\"\"\n\n\n# Merge DataFrames of each original source into a common DataFrame DE_renewables\ndfs_list = []\n\nfor dataset in datasets:\n    dfs_list.append(dfs[dataset])\n\nDE_renewables = pd.concat(dfs_list, sort=True)\nDE_renewables.head(2)\n\nDE_renewables.reset_index(drop=True, inplace=True)\nDE_renewables.head(2)\n\n\"\"\"\nExplanation: Merge DataFrames\nThe individual DataFrames from the TSOs (Netztransparenz.de) and BNetzA are merged.\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for Germany, create dictionary and show dictionary\nvaluenames = pd.read_csv(os.path.join('input', 'value_translation_list.csv'))\nidx_DE = valuenames[valuenames['country'] == 'DE'].index\nvalue_dict_DE = valuenames.loc[idx_DE].set_index('original_name')['opsd_name'].to_dict()\nvalue_dict_DE\n\nprint('replacing...')\n# Replace all original value names by the OPSD value names. \n# Running time: some minutes.\nDE_renewables.replace(value_dict_DE, inplace=True)\nprint('Done!')\n\nDE_renewables['postcode'] = DE_renewables['postcode'].apply(pd.to_numeric, errors='ignore')\n\n\"\"\"\nExplanation: Translate values and harmonize energy source level 2\nDifferent German terms for energy source level 2, energy source level 3, technology and voltage levels are translated and harmonized across the individual data sources. The value_translation_list is provided here as csv in the input folder. It is loaded in 2.3 Setup of translation dictionaries.\nEnd of explanation\n\"\"\"\n\n\n# Create dictionary in order to assign energy_source to its subtype\nenergy_source_dict_DE = valuenames.loc[idx_DE].set_index(\n    'opsd_name')['energy_source_level_2'].to_dict()\n\n# Column energy_source partly contains energy source level 3 and technology information,\n# thus this column is copied to new column technology...\nDE_renewables['technology'] = DE_renewables['energy_source_level_2']\n\n# ...and the energy source level 2 values are replaced by the higher level classification\nDE_renewables['energy_source_level_2'].replace(energy_source_dict_DE, inplace=True)\n\n# Choose energy source level 2 entries where energy_source is \"Bioenergy\" in order to \n# separate Bioenergy subtypes to \"energy_source_level_3\" and subtypes for the rest to \"technology\"\nidx_DE_Bioenergy = DE_renewables[DE_renewables['energy_source_level_2'] == 'Bioenergy'].index\n\n# Assign technology to energy source level 3 for all entries where energy source level 2 is \n# Bioenergy and delete those entries from technology\nDE_renewables[['energy_source_level_3']] = DE_renewables.iloc[idx_DE_Bioenergy][['technology']]\nDE_renewables.loc[idx_DE_Bioenergy]['technology'] = np.nan\n\n# Assign energy source level 1 to the dataframe\nDE_renewables['energy_source_level_1'] = 'Renewable energy'\n\n# Show the hierarchy of the energy types present in the frame\nenergy_columns = ['energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3', 'technology']\nDE_renewables[energy_columns].drop_duplicates().sort_values(by='energy_source_level_2')\n\n\"\"\"\nExplanation: Separate and assign energy source level 1 - 3 and technology\nEnd of explanation\n\"\"\"\n\n\ndrop_mask = DE_renewables['energy_source_level_2'].isin(['Other fossil fuels', 'Storage'])\nDE_renewables.drop(DE_renewables.index[drop_mask], axis=0, inplace=True)\n\n\"\"\"\nExplanation: According to the OPSD energy hierarchy, the power plants whose energy_source_level_2 is either Storage or Other fossil fuels do not belong to the class of renewable-energy facilities. Therefore, we can remove them.\nEnd of explanation\n\"\"\"\n\n\n# Electrical capacity per energy source level 2 (in MW)\nDE_renewables.groupby(['energy_source_level_2'])['electrical_capacity_kW'].sum() / 1000\n\n\"\"\"\nExplanation: Summary of DataFrame\nEnd of explanation\n\"\"\"\n\n\n# kW to MW\nDE_renewables[['electrical_capacity_kW', 'thermal_capacity_kW']] /= 1000\n\n# adapt column name\nDE_renewables.rename(columns={'electrical_capacity_kW': 'electrical_capacity',\n                              'thermal_capacity_kW': 'thermal_capacity'}, inplace=True)\n\n\"\"\"\nExplanation: Transform electrical capacity from kW to MW\nEnd of explanation\n\"\"\"\n\n\n# Read generated postcode/location file\npostcode = pd.read_csv(os.path.join('input', 'de_tso_postcode_full.csv'))\n\n# Drop possible duplicates in postcodes\npostcode.drop_duplicates('postcode', keep='last', inplace=True)\n\n# Show first entries\npostcode.head(2)\n\n\"\"\"\nExplanation: Georeferencing\nGet coordinates by postcode\n(for data with no existing geocoordinates)\nThe available post code in the original data provides a first approximation for the geocoordinates of the RE power plants.<br>\nThe BNetzA data provides the full zip code whereas due to data privacy the TSOs only report the first three digits of the power plant's post code (e.g. 024xx) and no address. Subsequently a centroid of the post code region polygon is used to find the coordinates.\nWith data from\n*  http://www.suche-postleitzahl.org/downloads?download=plz-gebiete.shp.zip\n*  http://www.suche-postleitzahl.org/downloads?download_file=plz-3stellig.shp.zip\n*  http://www.suche-postleitzahl.org/downloads\na CSV-file for all existing German post codes with matching geocoordinates has been compiled. The latitude and longitude coordinates were generated by running a PostgreSQL + PostGIS database. Additionally the respective TSO has been added to each post code. (A Link to the SQL script will follow here later)\n(License: http://www.suche-postleitzahl.org/downloads, Open Database Licence for free use. Source of data: © OpenStreetMap contributors)\nEnd of explanation\n\"\"\"\n\n\n# Take postcode and longitude/latitude information\npostcode = postcode[['postcode', 'lon', 'lat']]\n\n# Cast DE_renewables['postcode'] to int64 in order to do the natural join of the dataframes\nDE_renewables['postcode'] = pd.to_numeric(DE_renewables['postcode'], errors='coerce')\n\n# Join two dataframes\nDE_renewables = DE_renewables.merge(postcode, on=['postcode'],  how='left')\n\n\"\"\"\nExplanation: Merge geometry information by using the postcode\nEnd of explanation\n\"\"\"\n\n\nDE_renewables.groupby(['utm_zone'])['utm_zone'].count()\n\n\"\"\"\nExplanation: Transform geoinformation\n(for data with already existing geoinformation)\nIn this section the existing geoinformation (in UTM-format) will be transformed into latidude and longitude coordiates as a uniform standard for geoinformation. \nThe BNetzA data set offers UTM Geoinformation with the columns utm_zone (UTM-Zonenwert), utm_east and utm_north. Most of utm_east-values include the utm_zone-value 32 at the beginning of the number. In order to properly standardize and transform this geoinformation into latitude and longitude it is necessary to remove this utm_zone value. For all UTM entries the utm_zone 32 is used by the BNetzA.\n|utm_zone|   utm_east|   utm_north| comment|\n|---|---|---| ----|\n|32|    413151.72|  6027467.73| proper coordinates|\n|32|    32912159.6008|  5692423.9664| caused error by 32|\nHow many different utm_zone values are in the data set?\nEnd of explanation\n\"\"\"\n\n\n# Find entries with 32 value at the beginning\nidx_32 = (DE_renewables['utm_east'].astype(str).str[:2] == '32')\nidx_notnull = DE_renewables['utm_east'].notnull()\n\n# Remove 32 from utm_east entries\nDE_renewables.loc[idx_32, 'utm_east'] = DE_renewables.loc[idx_32,\n                                                          'utm_east'].astype(str).str[2:].astype(float)\n\ndef convert_to_latlon(utm_east, utm_north, utm_zone):\n    try:\n        return utm.to_latlon(utm_east, utm_north, utm_zone, 'U')\n    except:\n        return ''\n\nDE_renewables['latlon'] = DE_renewables.loc[idx_notnull, ['utm_east', 'utm_north', 'utm_zone']].apply(\n        lambda x: convert_to_latlon(x[0], x[1], x[2]), axis=1).astype(str)\n\n\"\"\"\nExplanation: Remove the utm_zone \"32\" from the utm_east value\nEnd of explanation\n\"\"\"\n\n\nlat = []\nlon = []\n\nfor row in DE_renewables['latlon']:\n    try:\n        # Split tuple format into the column lat and lon\n        row = row.lstrip('(').rstrip(')')\n        parts = row.split(',')\n        if(len(parts)<2):\n            raise Exception('This is not a proper tuple. So go to exception block.')\n        lat.append(parts[0])\n        lon.append(parts[1])\n    except:\n        # set NaN\n        lat.append(np.NaN)\n        lon.append(np.NaN)\n\nDE_renewables['latitude'] = pd.to_numeric(lat)\nDE_renewables['longitude'] = pd.to_numeric(lon)\n\n# Add new values to DataFrame lon and lat\nDE_renewables['lat'] = DE_renewables[['lat', 'latitude']].apply(\n    lambda x: x[1] if pd.isnull(x[0]) else x[0],\n    axis=1)\n\nDE_renewables['lon'] = DE_renewables[['lon', 'longitude']].apply(\n    lambda x: x[1] if pd.isnull(x[0]) else x[0],\n    axis=1)\n\n\"\"\"\nExplanation: Conversion UTM to latitude and longitude\nEnd of explanation\n\"\"\"\n\n\n#DE_renewables[DE_renewables['data_source'] == '50Hertz'].to_excel('test.xlsx')\n\nprint('Missing coordinates ', DE_renewables.lat.isnull().sum())\n\ndisplay(\n    DE_renewables[DE_renewables.lat.isnull()].groupby(\n        ['energy_source_level_2','data_source']\n    )['data_source'].count()\n)\n\nprint('Share of missing coordinates (note that NaN can mean it\\'s all fine):')\n\nDE_renewables[DE_renewables.lat.isnull()].groupby(\n        ['energy_source_level_2','data_source']\n    )['data_source'].count() / DE_renewables.groupby(\n        ['energy_source_level_2','data_source']\n    )['data_source'].count()\n\n\"\"\"\nExplanation: Check: missing coordinates by data source and type\nEnd of explanation\n\"\"\"\n\n\n# drop lonlat column that contains both, latitute and longitude\nDE_renewables.drop(['latlon', 'longitude', 'latitude'], axis=1, inplace=True)\n\n\"\"\"\nExplanation: Remove temporary columns\nEnd of explanation\n\"\"\"\n\n\npickle.dump(DE_renewables, open( \"intermediate/temp_dfs_DE_before_cleaning.pickle\", \"wb\" ) )\n\nDE_renewables = pickle.load( open( \"intermediate/temp_dfs_DE_before_cleaning.pickle\", \"rb\" ) )\n\n\"\"\"\nExplanation: Save temporary Pickle (to have a point to quickly return to if things break after this point):\nEnd of explanation\n\"\"\"\n\n\n# Remove out-of-range dates\n# Keep only values between 1900 and 2100 to rule out outliers / wrong values. \n# Also, Excel doesn't support dates before 1900..\n\nmask = ((DE_renewables['commissioning_date']>pd.Timestamp('1900')) & \n        (DE_renewables['commissioning_date']<pd.Timestamp('2100')))\nDE_renewables = DE_renewables[mask]\n\nDE_renewables['municipality_code'] = DE_renewables['municipality_code'].astype(str)\n\n# Remove spaces from municipality code\nDE_renewables['municipality_code'] = DE_renewables['municipality_code'].str.replace(' ', '', regex=False)\n\nDE_renewables['municipality_code'] = pd.to_numeric(DE_renewables['municipality_code'], errors='coerce', downcast='integer')\n\n# Merge address and address_number\nto_string = lambda x: str(x) if not pd.isnull(x) else ''\nDE_renewables['address'] = DE_renewables['address'].map(to_string) + ' ' + DE_renewables['address_number'].map(to_string)\n\n# Make sure that the column has no whitespaces at the beginning and the end\nDE_renewables['address'] = DE_renewables['address'].str.strip()\n\n# Remove the column with address numbers as it is not needed anymore\ndel DE_renewables['address_number']\n\n\"\"\"\nExplanation: Clean data\nEnd of explanation\n\"\"\"\n\n\n# Set up a temporary postcode column as a string column for joining with the appropriate NUTS correspondence table\nDE_renewables['postcode_str'] = DE_renewables['postcode'].astype(str).str[:-2]\n\nDE_renewables = nuts_converter.add_nuts_information(DE_renewables, 'DE', DE_postcode2nuts_filepath,\n                                                     postcode_column='postcode_str',\n                                                     how=['postcode', 'municipality_code', 'municipality', 'latlon'])\n\n# Drop the temporary column\nDE_renewables.drop('postcode_str', axis='columns', inplace=True)\n\n# Report the number of facilites whose NUTS codes were successfully sudetermined\ndetermined = DE_renewables['nuts_1_region'].notnull().sum()\nprint('NUTS successfully determined for', determined, 'out of', DE_renewables.shape[0], 'facilities in DE.')\n\n# Report the number of facilites whose NUTS codes could not be determined\nnot_determined = DE_renewables['nuts_1_region'].isnull().sum()\nprint('NUTS could not be determined for', not_determined, 'out of', DE_renewables.shape[0], 'facilities in DE.')\n\n\"\"\"\nExplanation: Assign NUTS codes\nEnd of explanation\n\"\"\"\n\n\nvisualize_points(DE_renewables['lat'],\n                 DE_renewables['lon'],\n                'Germany',\n                 categories=DE_renewables['energy_source_level_2']\n)\n\n\"\"\"\nExplanation: Visualize\nEnd of explanation\n\"\"\"\n\n\nDE_renewables.to_pickle('intermediate/DE_renewables.pickle')\n\ndel DE_renewables\n\n\"\"\"\nExplanation: Save\nThe merged, translated, cleaned, DataFrame will be saved temporily as a pickle file, which stores a Python object fast.\nEnd of explanation\n\"\"\"\n\n\n# Download the data for Denmark\nfilepaths = downloader.download_data_for_country('DK')\nprint(filepaths)\n\n\"\"\"\nExplanation: Denmark DK\nDownload and read\nThe data which will be processed below is provided by the following data sources:\nEnergistyrelsen (ens) / Danish Energy Agency - The wind turbines register is released by the Danish Energy Agency. \nEnerginet.dk - The data of solar power plants are released by the leading transmission network operator Denmark.\ngeonames.org - The postcode  data from Denmark is provided by Geonames and licensed under a Creative Commons Attribution 3.0 license.\nEurostat - The data for converting information on municipalities, postcodes and geographic coordinates to NUTS 2016 classification codes.\nEnd of explanation\n\"\"\"\n\n\ndef read_dk_wind_turbines(filepath, sheet_name):\n    # Reads the data on Danish wind turbines\n    # from the sheet of the given name\n    # in the file with the path.\n    # Returns the data as a Pandas dataframe.\n    \n    book = xlrd.open_workbook(filepath)\n    sheet = book.sheet_by_name(sheet_name)\n    \n    # Since the column names are in two rows, not one,\n    # collect them in two parts. The first part is\n    # fixed and contains column names.\n    header = []\n    for i in range(0, 16):\n        # Make sure that strings 1) do not contain the newline sign\n        # and 2) have no trailing blank spaces.\n        column_name = sheet.cell_value(17, i).replace(\"\\n\", \"\").strip()\n        header = header + [column_name]\n    # The second part is variable. It consists of two subparts:\n    # 1) previous years (type float)\n    # 2) the past months of the current year (type date)\n    \n    # Reading previous years as column names\n    i = 16\n    cell = sheet.cell(16, i)\n\n    while cell.ctype == xlrd.XL_CELL_NUMBER:\n        column_name = str(int(cell.value))\n        header = header + [column_name]\n        i = i + 1\n        cell = sheet.cell(16, i)\n    \n    # Reading the months of the current year as column names\n    while cell.ctype == xlrd.XL_CELL_DATE:\n        year, month, _, _, _, _ = xlrd.xldate_as_tuple(cell.value, book.datemode)\n        column_name = str(\"{}-{}\".format(year, month))\n        header = header + [column_name]\n        i = i + 1\n        cell = sheet.cell(16, i)\n        \n    # Add the final column for the total of the current year\n    header += ['{}-total'.format(header[-1].split('-')[0])]\n        \n        \n    # Skip the first 17 rows in the sheet. The rest contains the data.\n    df = pd.read_excel(filepath,\n                       sheet_name=sheet_name,\n                       skiprows=17,\n                       skipfooter=3\n                    )\n    \n    # \n    #df.drop(df.columns[len(df.columns)-1], axis=1, inplace=True)\n    \n    # Set the column names.\n    df.columns = header\n    \n    return df\n\n# Get wind turbines data\nwind_turbines_sheet_name = 'IkkeAfmeldte-Existing turbines'\nDK_wind_filepath = filepaths['Energistyrelsen']\nDK_wind_df = read_dk_wind_turbines(DK_wind_filepath,\n                                   wind_turbines_sheet_name\n                                  )\n\n# Get photovoltaic data\nDK_solar_filepath = filepaths['Energinet']\nDK_solar_df = pd.read_excel(DK_solar_filepath,\n                            sheet_name='Data',\n                            skiprows=[0],\n                            converters={'Postnr': str}\n                           )\n\n# Remove duplicates\nDK_wind_df.drop_duplicates(inplace=True)\nDK_solar_df.drop_duplicates(inplace=True)\n\n\"\"\"\nExplanation: The function for reading the data on the wind turbines.\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for Denmark, create dictionary and show dictionary\nidx_DK = columnnames[columnnames['country'] == 'DK'].index\ncolumn_dict_DK = columnnames.loc[idx_DK].set_index('original_name')['opsd_name'].to_dict()\n\n# Windows has problems reading the csv entry for east and north (DK).\n# The reason might be the difference when opening the csv between linux and\n# windows.\ncolumn_dict_DK_temp = {}\nfor k, v in column_dict_DK.items():\n    column_dict_DK_temp[k] = v\n    if v == 'utm_east' or v == 'utm_north':\n        # merge 2 lines to 1\n        new_key = ''.join(k.splitlines())\n        column_dict_DK_temp[new_key] = v\n\ncolumn_dict_DK = column_dict_DK_temp\n\ncolumn_dict_DK\n\n# Replace column names based on column_dict_DK\nDK_wind_df.rename(columns=column_dict_DK, inplace=True)\nDK_solar_df.rename(columns=column_dict_DK, inplace=True)\n\n\"\"\"\nExplanation: Translate column names\nEnd of explanation\n\"\"\"\n\n\n# Add names of the data sources to the DataFrames\nDK_wind_df['data_source'] = 'Energistyrelsen'\nDK_solar_df['data_source'] = 'Energinet.dk'\n\n# Add energy source level 2 and technology for each of the two DataFrames\nDK_wind_df['energy_source_level_2'] = 'Wind'\nDK_solar_df['energy_source_level_2'] = 'Solar'\nDK_solar_df['technology'] = 'Photovoltaics'\n\n\"\"\"\nExplanation: Add data source and missing information\nEnd of explanation\n\"\"\"\n\n\nmask=DK_solar_df['commissioning_date'] == '1970-01-01'\nDK_solar_df.loc[mask, 'commissioning_date'] = np.nan\n\n\"\"\"\nExplanation: Correct the dates\nSome dates in the Energinet dataset are equal to 1970-01-01, which should be NaN instead\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for Denmark, create dictionary and show dictionary\nidx_DK = valuenames[valuenames['country'] == 'DK'].index\nvalue_dict_DK = valuenames.loc[idx_DK].set_index('original_name')['opsd_name'].to_dict()\n\n# Replace all original value names by the OPSD value names\nDK_wind_df.replace(value_dict_DK, inplace=True)\nDK_solar_df.replace(value_dict_DK, inplace=True)\n\n\"\"\"\nExplanation: Translate values and harmonize energy source level 2\nEnd of explanation\n\"\"\"\n\n\n# Index for all values with utm information\nidx_notnull = DK_wind_df['utm_east'].notnull()\n\n# Convert from UTM values to latitude and longitude coordinates\nDK_wind_df['lonlat'] = DK_wind_df.loc[idx_notnull, ['utm_east', 'utm_north']\n                                      ].apply(lambda x: utm.to_latlon(x[0],\n                                                                      x[1],\n                                                                      32,\n                                                                      'U'), axis=1).astype(str)\n\n# Split latitude and longitude in two columns\nlat = []\nlon = []\n\nfor row in DK_wind_df['lonlat']:\n    try:\n        # Split tuple format\n        # into the column lat and lon\n        row = row.lstrip('(').rstrip(')')\n        lat.append(row.split(',')[0])\n        lon.append(row.split(',')[1])\n    except:\n        # set NAN\n        lat.append(np.NaN)\n        lon.append(np.NaN)\n\nDK_wind_df['lat'] = pd.to_numeric(lat)\nDK_wind_df['lon'] = pd.to_numeric(lon)\n\n# drop lonlat column that contains both, latitute and longitude\nDK_wind_df.drop('lonlat', axis=1, inplace=True)\n\n\"\"\"\nExplanation: Georeferencing\nUTM32 to latitude and longitude (Data from Energistyrelsen)\nThe Energistyrelsen data set offers UTM Geoinformation with the columns utm_east and utm_north belonging to the UTM zone 32. In this section the existing geoinformation (in UTM-format) will be transformed into latidude and longitude coordiates as a uniform standard for geoinformation.\nEnd of explanation\n\"\"\"\n\n\n# Get geo-information\nzip_DK_geo = zipfile.ZipFile(filepaths['Geonames'])\n\n# Read generated postcode/location file\nDK_geo = pd.read_csv(zip_DK_geo.open('DK.txt'), sep='\\t', header=None)\n\n# add column names as defined in associated readme file\nDK_geo.columns = ['country_code', 'postcode', 'place_name', 'admin_name1',\n                  'admin_code1', 'admin_name2', 'admin_code2', 'admin_name3',\n                  'admin_code3', 'lat', 'lon', 'accuracy']\n\n# Drop rows of possible duplicate postal_code\nDK_geo.drop_duplicates('postcode', keep='last', inplace=True)\nDK_geo['postcode'] = DK_geo['postcode'].astype(str)\n\n# Add longitude/latitude infomation assigned by postcode (for Energinet.dk data)\nDK_solar_df = DK_solar_df.merge(DK_geo[['postcode', 'lon', 'lat']],\n                                on=['postcode'],\n                                how='left')\n\n# Show number of units with missing coordinates separated by wind and solar\nprint('Missing Coordinates DK_wind', DK_wind_df.lat.isnull().sum(), 'out of', len(DK_wind_df.index))\nprint('Missing Coordinates DK_solar', DK_solar_df.lat.isnull().sum(), 'out of', len(DK_solar_df.index))\n\nzip_DK_geo.close()\n\n\"\"\"\nExplanation: Postcode to lat/lon (WGS84)\n(for data from Energinet.dk)\nThe available post code in the original data provides an approximation for the geocoordinates of the solar power plants.<br>\nThe postcode will be assigned to latitude and longitude coordinates with the help of the postcode table.\nEnd of explanation\n\"\"\"\n\n\n# Merge DataFrames for wind and solar into DK_renewables\ndataframes = [DK_wind_df, DK_solar_df]\nDK_renewables = pd.concat(dataframes, sort=False)\nDK_renewables = DK_renewables.reset_index()\n\n# Assign energy source level 1 to the dataframe\nDK_renewables['energy_source_level_1'] = 'Renewable energy'\n\n# Merge the address and address-number columns into one\nto_string = lambda x: str(x) if not pd.isnull(x) else \"\"\nDK_renewables['address'] = DK_renewables['address'].map(to_string) + \" \" + DK_renewables['address_number'].map(to_string)\n\n# Make sure that the column has no whitespaces at the beginning or the end\nDK_renewables['address'] = DK_renewables['address'].str.strip()\n\n# Assign NUTS codes\nDK_postcode2nuts = filepaths['Eurostat']\nDK_renewables = nuts_converter.add_nuts_information(DK_renewables, 'DK', DK_postcode2nuts,\n                                                    how=['latlon', 'postcode', 'municipality_code', 'municipality_name'])\n\n\n# Report the number of facilites whose NUTS codes were successfully sudetermined\ndetermined = DK_renewables['nuts_1_region'].notnull().sum()\nprint('NUTS successfully determined for', determined, 'out of', DK_renewables.shape[0], 'facilities in DK.')\n\n# Report the number of facilites whose NUTS codes could not be determined\nnot_determined = DK_renewables['nuts_1_region'].isnull().sum()\nprint('NUTS could not be determined for', not_determined, 'out of', DK_renewables.shape[0], 'facilities in DK.')\n\n\"\"\"\nExplanation: Merge DataFrames, add NUTS information and choose columns\nEnd of explanation\n\"\"\"\n\n\nDK_renewables[DK_renewables['nuts_1_region'].isnull()][['municipality', 'municipality_code', 'lat', 'lon']]\n\n\"\"\"\nExplanation: Let us check geoinformation on the facilities for which NUTS codes could not be determined.\nEnd of explanation\n\"\"\"\n\n\n# Select those columns of the orignal data which are utilised further\ncolumns_of_interest = ['commissioning_date', 'energy_source_level_1', 'energy_source_level_2',\n                   'technology', 'electrical_capacity_kW', 'dso', 'gsrn_id', 'postcode',\n                   'municipality_code', 'municipality', 'address',\n                   'utm_east', 'utm_north', 'lon', 'lat', 'nuts_1_region', 'nuts_2_region', 'nuts_3_region',\n                   'hub_height', 'rotor_diameter', 'manufacturer', 'model', 'data_source']\n\n# Clean DataFrame from columns other than specified above\nDK_renewables = DK_renewables.loc[:, columns_of_interest]\nDK_renewables.reset_",
  "128k": "import numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\"\"\"\nExplanation: Simple MNIST convnet\nAuthor: fchollet<br>\nDate created: 2015/06/19<br>\nLast modified: 2020/04/21<br>\nDescription: A simple convnet that achieves ~99% test accuracy on MNIST.\nSetup\nEnd of explanation\n\"\"\"\n\n\n# Model / data parameters\nnum_classes = 10\ninput_shape = (28, 28, 1)\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n# Scale images to the [0, 1] range\nx_train = x_train.astype(\"float32\") / 255\nx_test = x_test.astype(\"float32\") / 255\n# Make sure images have shape (28, 28, 1)\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\nprint(\"x_train shape:\", x_train.shape)\nprint(x_train.shape[0], \"train samples\")\nprint(x_test.shape[0], \"test samples\")\n\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\"\"\"\nExplanation: Prepare the data\nEnd of explanation\n\"\"\"\n\n\nmodel = keras.Sequential(\n    [\n        keras.Input(shape=input_shape),\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n        layers.MaxPooling2D(pool_size=(2, 2)),\n        layers.Flatten(),\n        layers.Dropout(0.5),\n        layers.Dense(num_classes, activation=\"softmax\"),\n    ]\n)\n\nmodel.summary()\n\n\"\"\"\nExplanation: Build the model\nEnd of explanation\n\"\"\"\n\n\nbatch_size = 128\nepochs = 15\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n\n\"\"\"\nExplanation: Train the model\nEnd of explanation\n\"\"\"\n\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])\n\n\"\"\"\nExplanation: Evaluate the trained model\nEnd of explanation\n\"\"\"\n\n#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nExplanation: Copyright 2021 The TensorFlow Authors.\nEnd of explanation\n\"\"\"\n\n\nimport sys\nif 'google.colab' in sys.modules:\n  !pip install --upgrade pip\n\n\"\"\"\nExplanation: TFX Keras コンポーネントのチュートリアル\nTensorFlow Extended (TFX) の各コンポーネントの紹介\n注：この例は、Jupyter スタイルのノートブックで今すぐ実行できます。セットアップは必要ありません。「Google Colab で実行」をクリックするだけです\n<div class=\"devsite-table-wrapper\"><table class=\"tfo-notebook-buttons\" align=\"left\">\n<td><a target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/tfx/components_keras\"> <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org で表示</a></td>\n<td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab で実行</a></td>\n<td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n<td><a target=\"_blank\" href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/tfx/tutorials/tfx/components_keras.ipynb\"> <img width=\"32px\" src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n</table></div>\n\nこの Colab ベースのチュートリアルでは、TensorFlow Extended (TFX) のそれぞれの組み込みコンポーネントをインタラクティブに説明します。\nここではデータの取り込みからモデルのプッシュ、サービングまで、エンド ツー エンドの機械学習パイプラインのすべてのステップを見ていきます。\n完了したら、このノートブックのコンテンツを TFX パイプライン ソース コードとして自動的にエクスポートできます。これは、Apache Airflow および Apache Beam とオーケストレーションできます。\n注意: このノートブックは、TFX パイプラインでのネイティブ Keras モデルの使用を示しています。TFX は TensorFlow 2 バージョンの Keras のみをサポートします。\n背景情報\nこのノートブックは、Jupyter/Colab 環境で TFX を使用する方法を示しています。 ここでは、インタラクティブなノートブックでシカゴのタクシーの例を見ていきます。\nTFX パイプラインの構造に慣れるのには、インタラクティブなノートブックで作業するのが便利です。独自のパイプラインを軽量の開発環境として開発する場合にも役立ちますが、インタラクティブ ノートブックのオーケストレーションとメタデータ アーティファクトへのアクセス方法には違いがあるので注意してください。\nオーケストレーション\nTFX の実稼働デプロイメントでは、Apache Airflow、Kubeflow Pipelines、Apache Beam などのオーケストレーターを使用して、TFX コンポーネントの事前定義済みパイプライン グラフをオーケストレーションします。インタラクティブなノートブックでは、ノートブック自体がオーケストレーターであり、ノートブック セルを実行するときにそれぞれの TFX コンポーネントを実行します。\nメタデータ\nTFX の実稼働デプロイメントでは、ML Metadata（MLMD）API を介してメタデータにアクセスします。MLMD は、メタデータ プロパティを MySQL や SQLite などのデータベースに格納し、メタデータ ペイロードをファイル システムなどの永続ストアに保存します。インタラクティブなノートブックでは、プロパティとペイロードの両方が、Jupyter ノートブックまたは Colab サーバーの /tmp ディレクトリにあるエフェメラル SQLite データベースに保存されます。\nセットアップ\nまず、必要なパッケージをインストールしてインポートし、パスを設定して、データをダウンロードします。\nPip のアップグレード\nローカルで実行する場合にシステム Pipをアップグレードしないようにするには、Colab で実行していることを確認してください。もちろん、ローカルシステムは個別にアップグレードできます。\nEnd of explanation\n\"\"\"\n\n\n!pip install -U tfx\n\n\"\"\"\nExplanation: TFX をインストールする\n注：Google Colab では、パッケージが更新されるため、このセルを初めて実行するときに、ランタイムを再起動する必要があります（[ランタイム]&gt; [ランタイムの再起動...]）。\nEnd of explanation\n\"\"\"\n\n\nimport os\nimport pprint\nimport tempfile\nimport urllib\n\nimport absl\nimport tensorflow as tf\nimport tensorflow_model_analysis as tfma\ntf.get_logger().propagate = False\npp = pprint.PrettyPrinter()\n\nfrom tfx import v1 as tfx\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n\n%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip\n\n\"\"\"\nExplanation: ランタイムを再起動しましたか？\nGoogle Colab を使用している場合は、上記のセルを初めて実行するときにランタイムを再起動する必要があります（[ランタイム]&gt; [ランタイムの再起動...]）。 これは、Colab がパッケージを読み込むために必要ですです。\nパッケージをインポートする\n標準の TFX コンポーネント クラスを含む必要なパッケージをインポートします。\nEnd of explanation\n\"\"\"\n\n\nprint('TensorFlow version: {}'.format(tf.__version__))\nprint('TFX version: {}'.format(tfx.__version__))\n\n\"\"\"\nExplanation: ライブラリのバージョンを確認します。\nEnd of explanation\n\"\"\"\n\n\n# This is the root directory for your TFX pip package installation.\n_tfx_root = tfx.__path__[0]\n\n# This is the directory containing the TFX Chicago Taxi Pipeline example.\n_taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')\n\n# This is the path where your model will be pushed for serving.\n_serving_model_dir = os.path.join(\n    tempfile.mkdtemp(), 'serving_model/taxi_simple')\n\n# Set up logging.\nabsl.logging.set_verbosity(absl.logging.INFO)\n\n\"\"\"\nExplanation: パイプライン パスを設定\nEnd of explanation\n\"\"\"\n\n\n_data_root = tempfile.mkdtemp(prefix='tfx-data')\nDATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv'\n_data_filepath = os.path.join(_data_root, \"data.csv\")\nurllib.request.urlretrieve(DATA_PATH, _data_filepath)\n\n\"\"\"\nExplanation: サンプルデータのダウンロード\nTFX パイプラインで使用するサンプル データセットをダウンロードします。\n使用しているデータセットは、シカゴ市がリリースした タクシートリップデータセットです。 このデータセットの列は次のとおりです。\n<table>\n<tr>\n<td>pickup_community_area</td>\n<td>fare</td>\n<td>trip_start_month</td>\n</tr>\n<tr>\n<td>trip_start_hour</td>\n<td>trip_start_day</td>\n<td>trip_start_timestamp</td>\n</tr>\n<tr>\n<td>pickup_latitude</td>\n<td>pickup_longitude</td>\n<td>dropoff_latitude</td>\n</tr>\n<tr>\n<td>dropoff_longitude</td>\n<td>trip_miles</td>\n<td>pickup_census_tract</td>\n</tr>\n<tr>\n<td>dropoff_census_tract</td>\n<td>payment_type</td>\n<td>company</td>\n</tr>\n<tr>\n<td>trip_seconds</td>\n<td>dropoff_community_area</td>\n<td>tips</td>\n</tr>\n</table>\n\nこのデータセットを使用して、タクシー乗車のtipsを予測するモデルを構築します。\nEnd of explanation\n\"\"\"\n\n\n!head {_data_filepath}\n\n\"\"\"\nExplanation: CSV ファイルを見てみましょう。\nEnd of explanation\n\"\"\"\n\n\n# Here, we create an InteractiveContext using default parameters. This will\n# use a temporary directory with an ephemeral ML Metadata database instance.\n# To use your own pipeline root or database, the optional properties\n# `pipeline_root` and `metadata_connection_config` may be passed to\n# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n# notebook.\ncontext = InteractiveContext()\n\n\"\"\"\nExplanation: 注：このWeb サイトは、シカゴ市の公式 Web サイト www.cityofchicago.org で公開されたデータを変更して使用するアプリケーションを提供します。シカゴ市は、この Web サイトで提供されるデータの内容、正確性、適時性、または完全性について一切の表明を行いません。この Web サイトで提供されるデータは、いつでも変更される可能性があります。かかる Web サイトで提供されるデータはユーザーの自己責任で利用されるものとします。\nInteractiveContext を作成する\n最後に、このノートブックで TFX コンポーネントをインタラクティブに実行できるようにする InteractiveContext を作成します。\nEnd of explanation\n\"\"\"\n\n\nexample_gen = tfx.components.CsvExampleGen(input_base=_data_root)\ncontext.run(example_gen, enable_cache=True)\n\n\"\"\"\nExplanation: TFX コンポーネントをインタラクティブに実行する\n次のセルでは、TFX コンポーネントを 1 つずつ作成し、それぞれを実行して、出力アーティファクトを視覚化します。\nExampleGen\nExampleGen コンポーネントは通常、TFX パイプラインの先頭にあり、以下を実行します。\n\nデータをトレーニング セットと評価セットに分割します (デフォルトでは、2/3 トレーニング + 1/3 評価)。\nデータを tf.Example 形式に変換します。 (詳細はこちら)\n他のコンポーネントがアクセスできるように、データを _tfx_root ディレクトリにコピーします。\n\nExampleGen は、データソースへのパスを入力として受け取ります。 ここでは、これはダウンロードした CSV を含む _data_root パスです。\n注意: このノートブックでは、コンポーネントを 1 つずつインスタンス化し、InteractiveContext.run() で実行しますが、実稼働環境では、すべてのコンポーネントを事前に Pipelineで指定して、オーケストレーターに渡します（TFX パイプライン ガイドの構築を参照してください）。\nキャッシュを有効にする\nノートブックで InteractiveContext を使用してパイプラインを作成している場合、個別のコンポーネントが出力をキャッシュするタイミングを制御することができます。コンポーネントが前に生成した出力アーティファクトを再利用する場合は、enable_cache を True に設定します。コードを変更するなどにより、コンポーネントの出力アーティファクトを再計算する場合は、enable_cache を False に設定します。\nEnd of explanation\n\"\"\"\n\n\nartifact = example_gen.outputs['examples'].get()[0]\nprint(artifact.split_names, artifact.uri)\n\n\"\"\"\nExplanation: ExampleGenの出力アーティファクトを調べてみましょう。このコンポーネントは、トレーニングサンプルと評価サンプルの 2 つのアーティファクトを生成します。\nEnd of explanation\n\"\"\"\n\n\n# Get the URI of the output artifact representing the training examples, which is a directory\ntrain_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')\n\n# Get the list of files in this directory (all compressed TFRecord files)\ntfrecord_filenames = [os.path.join(train_uri, name)\n                      for name in os.listdir(train_uri)]\n\n# Create a `TFRecordDataset` to read these files\ndataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n\n# Iterate over the first 3 records and decode them.\nfor tfrecord in dataset.take(3):\n  serialized_example = tfrecord.numpy()\n  example = tf.train.Example()\n  example.ParseFromString(serialized_example)\n  pp.pprint(example)\n\n\"\"\"\nExplanation: また、最初の 3 つのトレーニングサンプルも見てみます。\nEnd of explanation\n\"\"\"\n\n\nstatistics_gen = tfx.components.StatisticsGen(\n    examples=example_gen.outputs['examples'])\ncontext.run(statistics_gen, enable_cache=True)\n\n\"\"\"\nExplanation: ExampleGenがデータの取り込みを完了したので、次のステップ、データ分析に進みます。\nStatisticsGen\nStatisticsGenコンポーネントは、データ分析用のデータセットの統計を計算し、ダウンストリームのコンポーネントで使用します。これは、TensorFlow Data Validation ライブラリを使用します。\nStatisticsGenコンポーネントは、データ分析用のデータセットの統計を計算し、ダウンストリーム コンポーネントで使用します。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(statistics_gen.outputs['statistics'])\n\n\"\"\"\nExplanation: StatisticsGen の実行が完了すると、出力された統計を視覚化できます。 色々なプロットを試してみてください！\nEnd of explanation\n\"\"\"\n\n\nschema_gen = tfx.components.SchemaGen(\n    statistics=statistics_gen.outputs['statistics'],\n    infer_feature_shape=False)\ncontext.run(schema_gen, enable_cache=True)\n\n\"\"\"\nExplanation: SchemaGen\nSchemaGen コンポーネントは、データ統計に基づいてスキーマを生成します。（スキーマは、データセット内の特徴の予想される境界、タイプ、プロパティを定義します。）また、TensorFlow データ検証ライブラリも使用します。\n注意: 生成されたスキーマはベストエフォートのもので、データの基本的なプロパティだけを推論しようとします。確認し、必要に応じて修正する必要があります。\nSchemaGen は、StatisticsGen で生成した統計を入力として受け取り、デフォルトでトレーニング分割を参照します。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(schema_gen.outputs['schema'])\n\n\"\"\"\nExplanation: SchemaGen の実行が完了すると、生成されたスキーマをテーブルとして視覚化できます。\nEnd of explanation\n\"\"\"\n\n\nexample_validator = tfx.components.ExampleValidator(\n    statistics=statistics_gen.outputs['statistics'],\n    schema=schema_gen.outputs['schema'])\ncontext.run(example_validator, enable_cache=True)\n\n\"\"\"\nExplanation: データセットのそれぞれの特徴は、スキーマ テーブルのプロパティの横に行として表示されます。スキーマは、ドメインとして示される、カテゴリ特徴が取るすべての値もキャプチャします。\nスキーマの詳細については、SchemaGen のドキュメントをご覧ください。\nExampleValidator\nExampleValidator コンポーネントは、スキーマで定義された期待に基づいて、データの異常を検出します。また、TensorFlow Data Validation ライブラリも使用します。\nExampleValidator は、Statistics Gen{/code 1} からの統計と &lt;code data-md-type=\"codespan\"&gt;SchemaGen からのスキーマを入力として受け取ります。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(example_validator.outputs['anomalies'])\n\n\"\"\"\nExplanation: ExampleValidator の実行が完了すると、異常をテーブルとして視覚化できます。\nEnd of explanation\n\"\"\"\n\n\n_taxi_constants_module_file = 'taxi_constants.py'\n\n%%writefile {_taxi_constants_module_file}\n\nNUMERICAL_FEATURES = ['trip_miles', 'fare', 'trip_seconds']\n\nBUCKET_FEATURES = [\n    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n    'dropoff_longitude'\n]\n# Number of buckets used by tf.transform for encoding each feature.\nFEATURE_BUCKET_COUNT = 10\n\nCATEGORICAL_NUMERICAL_FEATURES = [\n    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n    'dropoff_community_area'\n]\n\nCATEGORICAL_STRING_FEATURES = [\n    'payment_type',\n    'company',\n]\n\n# Number of vocabulary terms used for encoding categorical features.\nVOCAB_SIZE = 1000\n\n# Count of out-of-vocab buckets in which unrecognized categorical are hashed.\nOOV_SIZE = 10\n\n# Keys\nLABEL_KEY = 'tips'\nFARE_KEY = 'fare'\n\ndef t_name(key):\n  \"\"\"\n  Rename the feature keys so that they don't clash with the raw keys when\n  running the Evaluator component.\n  Args:\n    key: The original feature key\n  Returns:\n    key with '_xf' appended\n  \"\"\"\n  return key + '_xf'\n\n\"\"\"\nExplanation: 異常テーブルでは、異常がないことがわかります。これは、分析した最初のデータセットで、スキーマはこれに合わせて調整されているため、異常がないことが予想されます。このスキーマを確認する必要があります。予期されないものは、データに異常があることを意味します。確認されたスキーマを使用して将来のデータを保護できます。ここで生成された異常は、モデルのパフォーマンスをデバッグし、データが時間の経過とともにどのように変化するかを理解し、データ エラーを特定するために使用できます。\n変換\nTransformコンポーネントは、トレーニングとサービングの両方で特徴量エンジニアリングを実行します。これは、 TensorFlow Transform ライブラリを使用します。\nTransformは、ExampleGenからのデータ、SchemaGenからのスキーマ、ユーザー定義の Transform コードを含むモジュールを入力として受け取ります。\n以下のユーザー定義の Transform コードの例を見てみましょう（TensorFlow Transform API の概要については、チュートリアルを参照してください）。まず、特徴量エンジニアリングのいくつかの定数を定義します。\n注意: %%writefile セル マジックは、セルの内容をディスク上の.pyファイルとして保存します。これにより、Transform コンポーネントはコードをモジュールとして読み込むことができます。\nEnd of explanation\n\"\"\"\n\n\n_taxi_transform_module_file = 'taxi_transform.py'\n\n%%writefile {_taxi_transform_module_file}\n\nimport tensorflow as tf\nimport tensorflow_transform as tft\n\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimport taxi_constants\nimport sys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  import importlib\n  importlib.reload(taxi_constants)\n\n_NUMERICAL_FEATURES = taxi_constants.NUMERICAL_FEATURES\n_BUCKET_FEATURES = taxi_constants.BUCKET_FEATURES\n_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\n_CATEGORICAL_NUMERICAL_FEATURES = taxi_constants.CATEGORICAL_NUMERICAL_FEATURES\n_CATEGORICAL_STRING_FEATURES = taxi_constants.CATEGORICAL_STRING_FEATURES\n_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\n_OOV_SIZE = taxi_constants.OOV_SIZE\n_FARE_KEY = taxi_constants.FARE_KEY\n_LABEL_KEY = taxi_constants.LABEL_KEY\n\n\ndef _make_one_hot(x, key):\n  \"\"\"Make a one-hot tensor to encode categorical features.\n  Args:\n    X: A dense tensor\n    key: A string key for the feature in the input\n  Returns:\n    A dense one-hot tensor as a float list\n  \"\"\"\n  integerized = tft.compute_and_apply_vocabulary(x,\n          top_k=_VOCAB_SIZE,\n          num_oov_buckets=_OOV_SIZE,\n          vocab_filename=key, name=key)\n  depth = (\n      tft.experimental.get_vocabulary_size_by_name(key) + _OOV_SIZE)\n  one_hot_encoded = tf.one_hot(\n      integerized,\n      depth=tf.cast(depth, tf.int32),\n      on_value=1.0,\n      off_value=0.0)\n  return tf.reshape(one_hot_encoded, [-1, depth])\n\n\ndef _fill_in_missing(x):\n  \"\"\"Replace missing values in a SparseTensor.\n  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n  Args:\n    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n      in the second dimension.\n  Returns:\n    A rank 1 tensor where missing values of `x` have been filled in.\n  \"\"\"\n  if not isinstance(x, tf.sparse.SparseTensor):\n    return x\n\n  default_value = '' if x.dtype == tf.string else 0\n  return tf.squeeze(\n      tf.sparse.to_dense(\n          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n          default_value),\n      axis=1)\n\n\ndef preprocessing_fn(inputs):\n  \"\"\"tf.transform's callback function for preprocessing inputs.\n  Args:\n    inputs: map from feature keys to raw not-yet-transformed features.\n  Returns:\n    Map from string feature key to transformed feature operations.\n  \"\"\"\n  outputs = {}\n  for key in _NUMERICAL_FEATURES:\n    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.\n    outputs[taxi_constants.t_name(key)] = tft.scale_to_z_score(\n        _fill_in_missing(inputs[key]), name=key)\n\n  for key in _BUCKET_FEATURES:\n    outputs[taxi_constants.t_name(key)] = tf.cast(tft.bucketize(\n            _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT, name=key),\n            dtype=tf.float32)\n\n  for key in _CATEGORICAL_STRING_FEATURES:\n    outputs[taxi_constants.t_name(key)] = _make_one_hot(_fill_in_missing(inputs[key]), key)\n\n  for key in _CATEGORICAL_NUMERICAL_FEATURES:\n    outputs[taxi_constants.t_name(key)] = _make_one_hot(tf.strings.strip(\n        tf.strings.as_string(_fill_in_missing(inputs[key]))), key)\n\n  # Was this passenger a big tipper?\n  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])\n  tips = _fill_in_missing(inputs[_LABEL_KEY])\n  outputs[_LABEL_KEY] = tf.where(\n      tf.math.is_nan(taxi_fare),\n      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\n      # Test if the tip was > 20% of the fare.\n      tf.cast(\n          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\n\n  return outputs\n\n\"\"\"\nExplanation: 次に、生データを入力として受け取り、モデルがトレーニングできる変換された特徴量を返す {code 0}preprocessing _fn を記述します。\nEnd of explanation\n\"\"\"\n\n\ntransform = tfx.components.Transform(\n    examples=example_gen.outputs['examples'],\n    schema=schema_gen.outputs['schema'],\n    module_file=os.path.abspath(_taxi_transform_module_file))\ncontext.run(transform, enable_cache=True)\n\n\"\"\"\nExplanation: 次に、この特徴量エンジニアリング コードを Transformコンポーネントに渡し、実行してデータを変換します。\nEnd of explanation\n\"\"\"\n\n\ntransform.outputs\n\n\"\"\"\nExplanation: Transformの出力アーティファクトを調べてみましょう。このコンポーネントは、2 種類の出力を生成します。\n\ntransform_graph は、前処理演算を実行できるグラフです (このグラフは、サービングモデルと評価モデルに含まれます)。\ntransformed_examplesは前処理されたトレーニングおよび評価データを表します。\nEnd of explanation\n\"\"\"\n\n\ntrain_uri = transform.outputs['transform_graph'].get()[0].uri\nos.listdir(train_uri)\n\n\"\"\"\nExplanation: transform_graph アーティファクトを見てみましょう。これは、3 つのサブディレクトリを含むディレクトリを指しています。\nEnd of explanation\n\"\"\"\n\n\n# Get the URI of the output artifact representing the transformed examples, which is a directory\ntrain_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n\n# Get the list of files in this directory (all compressed TFRecord files)\ntfrecord_filenames = [os.path.join(train_uri, name)\n                      for name in os.listdir(train_uri)]\n\n# Create a `TFRecordDataset` to read these files\ndataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n\n# Iterate over the first 3 records and decode them.\nfor tfrecord in dataset.take(3):\n  serialized_example = tfrecord.numpy()\n  example = tf.train.Example()\n  example.ParseFromString(serialized_example)\n  pp.pprint(example)\n\n\"\"\"\nExplanation: transformed_metadata サブディレクトリには、前処理されたデータのスキーマが含まれています。transform_fnサブディレクトリには、実際の前処理グラフが含まれています。metadataサブディレクトリには、元のデータのスキーマが含まれています。\nまた、最初の 3 つの変換された例も見てみます。\nEnd of explanation\n\"\"\"\n\n\n_taxi_trainer_module_file = 'taxi_trainer.py'\n\n%%writefile {_taxi_trainer_module_file}\n\nfrom typing import Dict, List, Text\n\nimport os\nimport glob\nfrom absl import logging\n\nimport datetime\nimport tensorflow as tf\nimport tensorflow_transform as tft\n\nfrom tfx import v1 as tfx\nfrom tfx_bsl.public import tfxio\nfrom tensorflow_transform import TFTransformOutput\n\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimport taxi_constants\nimport sys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  import importlib\n  importlib.reload(taxi_constants)\n\n_LABEL_KEY = taxi_constants.LABEL_KEY\n\n_BATCH_SIZE = 40\n\n\ndef _input_fn(file_pattern: List[Text],\n              data_accessor: tfx.components.DataAccessor,\n              tf_transform_output: tft.TFTransformOutput,\n              batch_size: int = 200) -> tf.data.Dataset:\n  \"\"\"Generates features and label for tuning/training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    tf_transform_output: A TFTransformOutput.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      tf_transform_output.transformed_metadata.schema)\n\ndef _get_tf_examples_serving_signature(model, tf_transform_output):\n  \"\"\"Returns a serving signature that accepts `tensorflow.Example`.\"\"\"\n\n  # We need to track the layers in the model in order to save it.\n  # TODO(b/162357359): Revise once the bug is resolved.\n  model.tft_layer_inference = tf_transform_output.transform_features_layer()\n\n  @tf.function(input_signature=[\n      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n  ])\n  def serve_tf_examples_fn(serialized_tf_example):\n    \"\"\"Returns the output to be used in the serving signature.\"\"\"\n    raw_feature_spec = tf_transform_output.raw_feature_spec()\n    # Remove label feature since these will not be present at serving time.\n    raw_feature_spec.pop(_LABEL_KEY)\n    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n    transformed_features = model.tft_layer_inference(raw_features)\n    logging.info('serve_transformed_features = %s', transformed_features)\n\n    outputs = model(transformed_features)\n    # TODO(b/154085620): Convert the predicted labels from the model using a\n    # reverse-lookup (opposite of transform.py).\n    return {'outputs': outputs}\n\n  return serve_tf_examples_fn\n\n\ndef _get_transform_features_signature(model, tf_transform_output):\n  \"\"\"Returns a serving signature that applies tf.Transform to features.\"\"\"\n\n  # We need to track the layers in the model in order to save it.\n  # TODO(b/162357359): Revise once the bug is resolved.\n  model.tft_layer_eval = tf_transform_output.transform_features_layer()\n\n  @tf.function(input_signature=[\n      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n  ])\n  def transform_features_fn(serialized_tf_example):\n    \"\"\"Returns the transformed_features to be fed as input to evaluator.\"\"\"\n    raw_feature_spec = tf_transform_output.raw_feature_spec()\n    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n    transformed_features = model.tft_layer_eval(raw_features)\n    logging.info('eval_transformed_features = %s', transformed_features)\n    return transformed_features\n\n  return transform_features_fn\n\n\ndef export_serving_model(tf_transform_output, model, output_dir):\n  \"\"\"Exports a keras model for serving.\n  Args:\n    tf_transform_output: Wrapper around output of tf.Transform.\n    model: A keras model to export for serving.\n    output_dir: A directory where the model will be exported to.\n  \"\"\"\n  # The layer has to be saved to the model for keras tracking purpases.\n  model.tft_layer = tf_transform_output.transform_features_layer()\n\n  signatures = {\n      'serving_default':\n          _get_tf_examples_serving_signature(model, tf_transform_output),\n      'transform_features':\n          _get_transform_features_signature(model, tf_transform_output),\n  }\n\n  model.save(output_dir, save_format='tf', signatures=signatures)\n\n\ndef _build_keras_model(tf_transform_output: TFTransformOutput\n                       ) -> tf.keras.Model:\n  \"\"\"Creates a DNN Keras model for classifying taxi data.\n\n  Args:\n    tf_transform_output: [TFTransformOutput], the outputs from Transform\n\n  Returns:\n    A keras Model.\n  \"\"\"\n  feature_spec = tf_transform_output.transformed_feature_spec().copy()\n  feature_spec.pop(_LABEL_KEY)\n\n  inputs = {}\n  for key, spec in feature_spec.items():\n    if isinstance(spec, tf.io.VarLenFeature):\n      inputs[key] = tf.keras.layers.Input(\n          shape=[None], name=key, dtype=spec.dtype, sparse=True)\n    elif isinstance(spec, tf.io.FixedLenFeature):\n      # TODO(b/208879020): Move into schema such that spec.shape is [1] and not\n      # [] for scalars.\n      inputs[key] = tf.keras.layers.Input(\n          shape=spec.shape or [1], name=key, dtype=spec.dtype)\n    else:\n      raise ValueError('Spec type is not supported: ', key, spec)\n  \n  output = tf.keras.layers.Concatenate()(tf.nest.flatten(inputs))\n  output = tf.keras.layers.Dense(100, activation='relu')(output)\n  output = tf.keras.layers.Dense(70, activation='relu')(output)\n  output = tf.keras.layers.Dense(50, activation='relu')(output)\n  output = tf.keras.layers.Dense(20, activation='relu')(output)\n  output = tf.keras.layers.Dense(1)(output)\n  return tf.keras.Model(inputs=inputs, outputs=output)\n\n\n# TFX Trainer will call this function.\ndef run_fn(fn_args: tfx.components.FnArgs):\n  \"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n\n  train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor, \n                            tf_transform_output, _BATCH_SIZE)\n  eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor, \n                           tf_transform_output, _BATCH_SIZE)\n\n  model = _build_keras_model(tf_transform_output)\n\n  model.compile(\n      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n      metrics=[tf.keras.metrics.BinaryAccuracy()])\n\n  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n      log_dir=fn_args.model_run_dir, update_freq='batch')\n\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps,\n      callbacks=[tensorboard_callback])\n\n  # Export the model.\n  export_serving_model(tf_transform_output, model, fn_args.serving_model_dir)\n\n\"\"\"\nExplanation: Transformコンポーネントがデータを特徴量に変換したら、次にモデルをトレーニングします。\nトレーナー\nTrainerコンポーネントは、TensorFlow で定義したモデルをトレーニングします。デフォルトでは、Trainer は Estimator API をサポートします。Keras API を使用するには、トレーナーのコンストラクターでcustom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor)をセットアップして Generic Trainer を指定する必要があります。\nTrainer は、SchemaGenからのスキーマ、Transformからの変換されたデータとグラフ、トレーニング パラメータ、およびユーザー定義されたモデル コードを含むモジュールを入力として受け取ります。\n以下のユーザー定義モデル コードの例を見てみましょう（TensorFlow Keras API の概要については、チュートリアルを参照してください）。\nEnd of explanation\n\"\"\"\n\n\ntrainer = tfx.components.Trainer(\n    module_file=os.path.abspath(_taxi_trainer_module_file),\n    examples=transform.outputs['transformed_examples'],\n    transform_graph=transform.outputs['transform_graph'],\n    schema=schema_gen.outputs['schema'],\n    train_args=tfx.proto.TrainArgs(num_steps=10000),\n    eval_args=tfx.proto.EvalArgs(num_steps=5000))\ncontext.run(trainer, enable_cache=True)\n\n\"\"\"\nExplanation: 次に、このモデル コードをTrainerコンポーネントに渡し、それを実行してモデルをトレーニングします。\nEnd of explanation\n\"\"\"\n\n\nmodel_artifact_dir = trainer.outputs['model'].get()[0].uri\npp.pprint(os.listdir(model_artifact_dir))\nmodel_dir = os.path.join(model_artifact_dir, 'Format-Serving')\npp.pprint(os.listdir(model_dir))\n\n\"\"\"\nExplanation: TensorBoard でトレーニングを分析する\nトレーナーのアーティファクトを見てみましょう。これはモデルのサブディレクトリを含むディレクトリを指しています。\nEnd of explanation\n\"\"\"\n\n\nmodel_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri\n\n%load_ext tensorboard\n%tensorboard --logdir {model_run_artifact_dir}\n\n\"\"\"\nExplanation: オプションで、TensorBoard を Trainer に接続して、モデルの学習曲線を分析できます。\nEnd of explanation\n\"\"\"\n\n\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimport taxi_constants\nimport sys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  import importlib\n  importlib.reload(taxi_constants)\n\neval_config = tfma.EvalConfig(\n    model_specs=[\n        # This assumes a serving model with signature 'serving_default'. If\n        # using estimator based EvalSavedModel, add signature_name: 'eval' and\n        # remove the label_key.\n        tfma.ModelSpec(\n            signature_name='serving_default',\n            label_key=taxi_constants.LABEL_KEY,\n            preprocessing_function_names=['transform_features'],\n            )\n        ],\n    metrics_specs=[\n        tfma.MetricsSpec(\n            # The metrics added here are in addition to those saved with the\n            # model (assuming either a keras model or EvalSavedModel is used).\n            # Any metrics added into the saved model (for example using\n            # model.compile(..., metrics=[...]), etc) will be computed\n            # automatically.\n            # To add validation thresholds for metrics saved with the model,\n            # add them keyed by metric name to the thresholds map.\n            metrics=[\n                tfma.MetricConfig(class_name='ExampleCount'),\n                tfma.MetricConfig(class_name='BinaryAccuracy',\n                  threshold=tfma.MetricThreshold(\n                      value_threshold=tfma.GenericValueThreshold(\n                          lower_bound={'value': 0.5}),\n                      # Change threshold will be ignored if there is no\n                      # baseline model resolved from MLMD (first run).\n                      change_threshold=tfma.GenericChangeThreshold(\n                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n                          absolute={'value': -1e-10})))\n            ]\n        )\n    ],\n    slicing_specs=[\n        # An empty slice spec means the overall slice, i.e. the whole dataset.\n        tfma.SlicingSpec(),\n        # Data can be sliced along a feature column. In this case, data is\n        # sliced along feature column trip_start_hour.\n        tfma.SlicingSpec(\n            feature_keys=['trip_start_hour'])\n    ])\n\n\"\"\"\nExplanation: Evaluator\nEvaluator コンポーネントは、評価セットに対してモデル パフォーマンス指標を計算します。TensorFlow Model Analysisライブラリを使用します。Evaluatorは、オプションで、新しくトレーニングされたモデルが以前のモデルよりも優れていることを検証できます。これは、モデルを毎日自動的にトレーニングおよび検証する実稼働環境のパイプライン設定で役立ちます。このノートブックでは 1 つのモデルのみをトレーニングするため、Evaluatorはモデルに自動的に「good」というラベルを付けます。\nEvaluatorは、ExampleGenからのデータ、Trainerからのトレーニング済みモデル、およびスライス構成を入力として受け取ります。スライス構成により、特徴値に関する指標をスライスすることができます (たとえば、午前 8 時から午後 8 時までのタクシー乗車でモデルがどのように動作するかなど)。 この構成の例は、以下を参照してください。\nEnd of explanation\n\"\"\"\n\n\n# Use TFMA to compute a evaluation statistics over features of a model and\n# validate them against a baseline.\n\n# The model resolver is only required if performing model validation in addition\n# to evaluation. In this case we validate against the latest blessed model. If\n# no model has been blessed before (as in this case) the evaluator will make our\n# candidate the first blessed model.\nmodel_resolver = tfx.dsl.Resolver(\n      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n      model_blessing=tfx.dsl.Channel(\n          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n              'latest_blessed_model_resolver')\ncontext.run(model_resolver, enable_cache=True)\n\nevaluator = tfx.components.Evaluator(\n    examples=example_gen.outputs['examples'],\n    model=trainer.outputs['model'],\n    baseline_model=model_resolver.outputs['model'],\n    eval_config=eval_config)\ncontext.run(evaluator, enable_cache=True)\n\n\"\"\"\nExplanation: 次に、この構成を Evaluatorに渡して実行します。\nEnd of explanation\n\"\"\"\n\n\nevaluator.outputs\n\n\"\"\"\nExplanation: Evaluator の出力アーティファクトを調べてみましょう。\nEnd of explanation\n\"\"\"\n\n\ncontext.show(evaluator.outputs['evaluation'])\n\n\"\"\"\nExplanation: evaluation出力を使用すると、評価セット全体のグローバル指標のデフォルトの視覚化を表示できます。\nEnd of explanation\n\"\"\"\n\n\nimport tensorflow_model_analysis as tfma\n\n# Get the TFMA output result path and load the result.\nPATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\ntfma_result = tfma.load_eval_result(PATH_TO_RESULT)\n\n# Show data sliced along feature column trip_start_hour.\ntfma.view.render_slicing_metrics(\n    tfma_result, slicing_column='trip_start_hour')\n\n\"\"\"\nExplanation: スライスされた評価メトリクスの視覚化を表示するには、TensorFlow Model Analysis ライブラリを直接呼び出します。\nEnd of explanation\n\"\"\"\n\n\nblessing_uri = evaluator.outputs['blessing'].get()[0].uri\n!ls -l {blessing_uri}\n\n\"\"\"\nExplanation: この視覚化は同じ指標を示していますが、評価セット全体ではなく、trip_start_hourのすべての特徴値で計算されています。\nTensorFlow モデル分析は、公平性インジケーターやモデル パフォーマンスの時系列のプロットなど、他の多くの視覚化をサポートしています。 詳細については、チュートリアルを参照してください。\n構成にしきい値を追加したため、検証出力も利用できます。{code 0}blessing{/code 0} アーティファクトの存在は、モデルが検証に合格したことを示しています。これは実行される最初の検証であるため、候補は自動的に bless されます。\nEnd of explanation\n\"\"\"\n\n\nPATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\nprint(tfma.load_validation_result(PATH_TO_RESULT))\n\n\"\"\"\nExplanation: 検証結果レコードを読み込み、成功を確認することもできます。\nEnd of explanation\n\"\"\"\n\n\npusher = tfx.components.Pusher(\n    model=trainer.outputs['model'],\n    model_blessing=evaluator.outputs['blessing'],\n    push_destination=tfx.proto.PushDestination(\n        filesystem=tfx.proto.PushDestination.Filesystem(\n            base_directory=_serving_model_dir)))\ncontext.run(pusher, enable_cache=True)\n\n\"\"\"\nExplanation: Pusher\nPusher コンポーネントは通常、TFX パイプラインの最後にあります。このコンポーネントはモデルが検証に合格したかどうかをチェックし、合格した場合はモデルを _serving_model_dirにエクスポートします。\nEnd of explanation\n\"\"\"\n\n\npusher.outputs\n\n\"\"\"\nExplanation: 次にPusherの出力アーティファクトを調べてみましょう。\nEnd of explanation\n\"\"\"\n\n\npush_uri = pusher.outputs['pushed_model'].get()[0].uri\nmodel = tf.saved_model.load(push_uri)\n\nfor item in model.signatures.items():\n  pp.pprint(item)\n\n\"\"\"\nExplanation: 特に、Pusher はモデルを次のような SavedModel 形式でエクスポートします。\nEnd of explanation\n\"\"\"\n\n_, _, data = twpca.datasets.jittered_neuron()\nmodel = TWPCA(data, n_components=1, warpinit='identity')\n\nnp.all(np.isclose(model.params['warp'], np.arange(model.shared_length), atol=1e-5, rtol=2))\n\nnp.nanmax(np.abs(model.transform() - data)) < 1e-5\n\n\"\"\"\nExplanation: check identity warp does not change data appreciably\nEnd of explanation\n\"\"\"\n\n\nmodel = TWPCA(data, n_components=1, warpinit='shift')\n\nplt.imshow(np.squeeze(model.transform()))\n\n\"\"\"\nExplanation: check that shift initialization for warp solves the simple toy problem\nEnd of explanation\n\"\"\"\n\nfrom __future__ import print_function, division, unicode_literals\n\nimport oddt\nfrom oddt.datasets import dude\nprint(oddt.__version__)\n\n\"\"\"\nExplanation: <h1>DUD-E: A Database of Useful Decoys: Enhanced</h1>\nEnd of explanation\n\"\"\"\n\n\n%%bash\nmkdir -p ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/ampc/ampc.tar.gz | tar xz -C ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/cxcr4/cxcr4.tar.gz | tar xz -C ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/pur2/pur2.tar.gz | tar xz -C ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/pygm/pygm.tar.gz | tar xz -C ./DUD-E_targets/\nwget -qO- http://dude.docking.org/targets/sahh/sahh.tar.gz | tar xz -C ./DUD-E_targets/\n\ndirectory = './DUD-E_targets'\n\n\"\"\"\nExplanation: We'd like to read files from DUD-E.<br/>\nYou can download different targets and different numbers of targets, but I used only these five:\nampc, \ncxcr4, \npur2, \npygm, \nsahh.<br/>\nEnd of explanation\n\"\"\"\n\n\ndude_database = dude(home=directory)\n\n\"\"\"\nExplanation: We will use the dude class.\nEnd of explanation\n\"\"\"\n\n\ntarget = dude_database['cxcr4']\n\n\"\"\"\nExplanation: Now we can get one target or iterate over all targets in our directory.\nLet's choose one target.\nEnd of explanation\n\"\"\"\n\n\ntarget.ligand\n\n\"\"\"\nExplanation: target has four properties: protein, ligand, actives and decoys:<br/>\nprotein - protein molecule<br/>\nligand - ligand molecule<br/>\nactives - generator containing actives<br/>\ndecoys - generator containing decoys\nEnd of explanation\n\"\"\"\n\n\nfor target in dude_database:\n    actives = list(target.actives)\n    decoys = list(target.decoys)\n    print('Target: ' + target.dude_id, \n          'Number of actives: ' + str(len(actives)), \n          'Number of decoys: ' + str(len(decoys)), \n          sep='\\t\\t')\n\n\"\"\"\nExplanation: Let's see which target has the most actives and decoys.\nEnd of explanation\n\"\"\"\n\nmax_steps = 3000\nbatch_size = 128\ndata_dir = 'data/cifar10/cifar-10-batches-bin/'\nmodel_dir = 'model/_cifar10_v2/'\n\n\"\"\"\nExplanation: 全局参数\nEnd of explanation\n\"\"\"\n\n\nX_train, y_train = cifar10_input.distorted_inputs(data_dir, batch_size)\n\nX_test, y_test = cifar10_input.inputs(eval_data=True, data_dir=data_dir, batch_size=batch_size)\n\nimage_holder = tf.placeholder(tf.float32, [batch_size, 24, 24, 3])\nlabel_holder = tf.placeholder(tf.int32, [batch_size])\n\n\"\"\"\nExplanation: 初始化权重\n如果需要，会给权重加上L2 loss。为了在后面计算神经网络的总体loss的时候被用上，需要统一存到一个collection。\n加载数据\n使用cifa10_input来获取数据，这个文件来自tensorflow github，可以下载下来直接使用。如果使用distorted_input方法，那么得到的数据是经过增强处理的。会对图片随机做出切片、翻转、修改亮度、修改对比度等操作。这样就能多样化我们的训练数据。\n得到一个tensor，batch_size大小的batch。并且可以迭代的读取下一个batch。\nEnd of explanation\n\"\"\"\n\n\nweight1 = variable_with_weight_loss([5, 5, 3, 64], stddev=0.05, lambda_value=0)\nkernel1 = tf.nn.conv2d(image_holder, weight1, [1, 1, 1, 1], padding='SAME')\nbias1 = tf.Variable(tf.constant(0.0, shape=[64]))\nconv1 = tf.nn.relu(tf.nn.bias_add(kernel1, bias1))\npool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\nnorm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n\n\"\"\"\nExplanation: 第一个卷积层\n同样的，我们使用5x5卷积核，3个通道（input_channel），64个output_channel。不对第一层的参数做正则化，所以将lambda_value设定为0。其中涉及到一个小技巧，就是在pool层，使用了3x3大小的ksize，但是使用2x2的stride，这样增加数据的丰富性。最后使用LRN。LRN最早见于Alex参见ImageNet的竞赛的那篇CNN论文中，Alex在论文中解释了LRN层模仿了生物神经系统的“侧抑制”机制，对局部神经元的活动创建竞争环境，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增加了模型的泛化能力。不过在之后的VGGNet论文中，对比了使用和不使用LRN两种模型，结果表明LRN并不能提高模型的性能。不过这里还是基于AlexNet的设计将其加上。\nEnd of explanation\n\"\"\"\n\n\nweight2 = variable_with_weight_loss(shape=[5, 5, 64, 64], stddev=5e-2, lambda_value=0.0)\nkernel2 = tf.nn.conv2d(norm1, weight2, strides=[1, 1, 1, 1], padding='SAME')\nbias2 = tf.Variable(tf.constant(0.1, shape=[64]))\nconv2 = tf.nn.relu(tf.nn.bias_add(kernel2, bias2))\nnorm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\npool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n\n\"\"\"\nExplanation: 第二个卷积层\n\n输入64个channel，输出依然是64个channel\n设定bias的大小为0.1\n调换最大池化层和LRN的顺序，先进行LRN然后再最大池化层\n\n但是为什么要这么做，完全不知道？\n多看论文。\nEnd of explanation\n\"\"\"\n\n\nflattern = tf.reshape(pool2, [batch_size, -1])\ndim = flattern.get_shape()[1].value\nweight3 = variable_with_weight_loss(shape=[dim, 384], stddev=0.04, lambda_value=0.04)\nbias3 = tf.Variable(tf.constant(0.1, shape=[384]))\nlocal3 = tf.nn.relu(tf.matmul(flattern, weight3) + bias3)\n\n\"\"\"\nExplanation: 第一个全连接层\n\n要将卷积层拉伸\n全连接到新的隐藏层，设定为384个节点\n正态分布设定为0.04，bias设定为0.1\n重点是，在这里我们还设定weight loss的lambda数值为0.04\nEnd of explanation\n\"\"\"\n\n\nweight4 = variable_with_weight_loss(shape=[384, 192], stddev=0.04, lambda_value=0.04)\nbias4 = tf.Variable(tf.constant(0.1, shape=[192]))\nlocal4 = tf.nn.relu(tf.matmul(local3, weight4) + bias4)\n\n\"\"\"\nExplanation: 第二个全连接层\n\n下降为192个节点，减少了一半\nEnd of explanation\n\"\"\"\n\n\nweight5 = variable_with_weight_loss(shape=[192, 10], stddev=1/192.0, lambda_value=0.0)\nbias5 = tf.Variable(tf.constant(0.0, shape=[10]))\nlogits = tf.add(tf.matmul(local4, weight5), bias5)\n\ndef loss(logits, labels):\n    labels = tf.cast(labels, tf.int64)\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=logits, labels=labels,\n        name = 'cross_entropy_per_example'\n    )\n    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n    tf.add_to_collection('losses', cross_entropy_mean)\n    \n    return tf.add_n(tf.get_collection('losses'), name='total_loss')\n\nloss = loss(logits, label_holder)\n\ntrain_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\n\n\"\"\"\nExplanation: 输出层\n\n最后有10个类别\nEnd of explanation\n\"\"\"\n\n\ntop_k_op = tf.nn.in_top_k(logits, label_holder, 1)\n\nsess = tf.InteractiveSession()\n\nsaver = tf.train.Saver()\n\ntf.global_variables_initializer().run()\n\n\"\"\"\nExplanation: 使用in_top_k来输出top k的准确率，默认使用top 1。常用的可以是top 5。\nEnd of explanation\n\"\"\"\n\n\ntf.train.start_queue_runners()\n\n\"\"\"\nExplanation: 启动caifar_input中需要用的线程队列。主要用途是图片数据增强。这里总共使用了16个线程来处理图片。\nEnd of explanation\n\"\"\"\n\n\nfor step in range(max_steps):\n    start_time = time.time()\n    image_batch, label_batch = sess.run([X_train, y_train])\n    _, loss_value = sess.run([train_op, loss], \n                             feed_dict={image_holder: image_batch, label_holder: label_batch})\n    duration = time.time() - start_time\n    if step % 10 == 0:\n        examples_per_sec = batch_size / duration\n        sec_this_batch = float(duration)\n        \n        format_str = ('step %d, loss = %.2f (%.1f examples/sec; %.3f sec/batch)')\n        print(format_str % (step, loss_value, examples_per_sec, sec_this_batch))\n\nsaver.save(sess, save_path=os.path.join(model_dir, 'model.chpt'), global_step=max_steps)\n\nnum_examples = 10000\nnum_iter = int(math.ceil(num_examples / batch_size))\nture_count = 0\ntotal_sample_count = num_iter * batch_size\nstep = 0\nwhile step < num_iter:\n    image_batch, label_batch = sess.run([X_test, y_test])\n    predictions = sess.run([top_k_op], \n                           feed_dict={image_holder: image_batch, label_holder: label_batch})\n    true_count += np.sum(predictions)\n    step += 1\n\nprecision = ture_count / total_sample_count\nprint(\"Precision @ 1 = %.3f\" % precision)\n\nsess.close()\n\n\"\"\"\nExplanation: 每次在计算之前，先执行image_train,label_train来获取一个batch_size大小的训练数据。然后，feed到train_op和loss中，训练样本。每10次迭代计算就会输出一些必要的信息。\nEnd of explanation\n\"\"\"\n\n!python -m spacy download en_core_web_sm\n\n\"\"\"\nExplanation: Versioning Example (Part 1/3)\nIn this example, we'll train an NLP model for sentiment analysis of tweets using spaCy.\nThrough this series, we'll take advantage of ModelDB's versioning system to keep track of changes.\nThis workflow requires verta&gt;=0.14.4 and spaCy&gt;=2.0.0.\n\nSetup\nDownload a spaCy model to train.\nEnd of explanation\n\"\"\"\n\n\nfrom __future__ import unicode_literals, print_function\n\nimport boto3\nimport json\nimport numpy as np\nimport pandas as pd\nimport spacy\n\n\"\"\"\nExplanation: Import libraries we'll need.\nEnd of explanation\n\"\"\"\n\n\nfrom verta import Client\n\nclient = Client('http://localhost:3000/')\nproj = client.set_project('Tweet Classification')\nexpt = client.set_experiment('SpaCy')\n\n\"\"\"\nExplanation: Bring in Verta's ModelDB client to organize our work, and log and version metadata.\nEnd of explanation\n\"\"\"\n\n\nS3_BUCKET = \"verta-starter\"\nS3_KEY = \"english-tweets.csv\"\nFILENAME = S3_KEY\n\nboto3.client('s3').download_file(S3_BUCKET, S3_KEY, FILENAME)\n\n\"\"\"\nExplanation: Prepare Data\nDownload a dataset of English tweets from S3 for us to train with.\nEnd of explanation\n\"\"\"\n\n\nimport utils\n\ndata = pd.read_csv(FILENAME).sample(frac=1).reset_index(drop=True)\nutils.clean_data(data)\n\ndata.head()\n\n\"\"\"\nExplanation: Then we'll load and clean the data.\nEnd of explanation\n\"\"\"\n\n\nfrom verta.code import Notebook\nfrom verta.configuration import Hyperparameters\nfrom verta.dataset import S3\nfrom verta.environment import Python\n\ncode_ver = Notebook()  # Notebook & git environment\nconfig_ver = Hyperparameters({'n_iter': 20})\ndataset_ver = S3(\"s3://{}/{}\".format(S3_BUCKET, S3_KEY))\nenv_ver = Python(Python.read_pip_environment())  # pip environment and Python version\n\n\"\"\"\nExplanation: Capture and Version Model Ingredients\nWe'll first capture metadata about our code, configuration, dataset, and environment using utilities from the verta library.\nEnd of explanation\n\"\"\"\n\n\nrepo = client.set_repository('Tweet Classification')\ncommit = repo.get_commit(branch='master')\n\n\"\"\"\nExplanation: Then, to log them, we'll use a ModelDB repository to prepare a commit.\nEnd of explanation\n\"\"\"\n\n\ncommit.update(\"notebooks/tweet-analysis\", code_ver)\ncommit.update(\"config/hyperparams\", config_ver)\ncommit.update(\"data/tweets\", dataset_ver)\ncommit.update(\"env/python\", env_ver)\n\ncommit.save(\"Initial model\")\n\ncommit\n\n\"\"\"\nExplanation: Now we'll add these versioned components to the commit and save it to ModelDB.\nEnd of explanation\n\"\"\"\n\n\nnlp = spacy.load('en_core_web_sm')\n\n\"\"\"\nExplanation: Train and Log Model\nWe'll use the pre-trained spaCy model we downloaded earlier...\nEnd of explanation\n\"\"\"\n\n\nimport training\n\ntraining.train(nlp, data, n_iter=20)\n\n\"\"\"\nExplanation: ...and fine-tune it with our dataset.\nEnd of explanation\n\"\"\"\n\n\nrun = client.set_experiment_run()\n\nrun.log_model(nlp)\n\n\"\"\"\nExplanation: Now that our model is good to go, we'll log it to ModelDB so our progress is never lost.\nUsing Verta's ModelDB Client, we'll create an Experiment Run to encapsulate our work, and log our model as an artifact.\nEnd of explanation\n\"\"\"\n\n\nrun.log_commit(\n    commit,\n    {\n        'notebook': \"notebooks/tweet-analysis\",\n        'hyperparameters': \"config/hyperparams\",\n        'training_data': \"data/tweets\",\n        'python_env': \"env/python\",\n    },\n)\n\n\"\"\"\nExplanation: And finally, we'll link the commit we created earlier to the Experiment Run to complete our logged model version.\nEnd of explanation\n\"\"\"\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport yaml\n\n\"\"\"\nExplanation: The following script extracts the (more) helpful reviews from the swiss reviews and saves them locally.\nFrom the extracted reviews it also saves a list with their asin identifiers.\nThe list of asin identifiers will be later used to to find the average review rating for the respective products.\nEnd of explanation\n\"\"\"\n\n\nwith open(\"data/swiss-reviews.txt\", 'r') as fp:\n    swiss_rev = fp.readlines()\n\nlen(swiss_rev)\n\nswiss_rev[2]\n\n\"\"\"\nExplanation: Load the swiss reviews\nEnd of explanation\n\"\"\"\n\n\ndef filter_helpful(line):\n    l = line.rstrip('\\n')\n    l = yaml.load(l)\n    if('helpful' in l.keys()):\n        if(l['helpful'][1] >= 5):\n            return True\n        else:\n            return False\n    else:\n        print(\"Review does not have helpful score key: \"+line)\n        return False\n\n\"\"\"\nExplanation: The filter_helpful function keeps only the reviews which had at least 5 flags/votes in the helpfulness field.\nThis amounts to a subset of around 23000 reviews. A smaller subset of around 10000 reviews was obtained as well by only keeping reviews with 10 flags/votes. The main advantage of the smaller subset is that it contains better quality reviews while its drawback is, of course, the reduced size.\n1) Extract the helpful reviews\nEnd of explanation\n\"\"\"\n\n\ndef get_helpful(data):\n    res = []\n    counter = 1\n    i = 0\n    for line in data:\n        i += 1\n        if(filter_helpful(line)):\n            if(counter % 1000 == 0):\n                print(\"Count \"+str(counter)+\" / \"+str(i))\n            counter += 1\n            res.append(line)\n    return res\n\nswiss_reviews_helpful = get_helpful(swiss_rev)\n\nlen(swiss_reviews_helpful)\n\n\"\"\"\nExplanation: Apply the filter_helpful to each swiss product review\nEnd of explanation\n\"\"\"\n\n\nwrite_file = open('data/swiss-reviews-helpful-correct-bigger.txt', 'w')\nfor item in swiss_reviews_helpful:\n  write_file.write(item)\nwrite_file.close()\n\n\"\"\"\nExplanation: Save the subset with helpful swiss product reviews\nEnd of explanation\n\"\"\"\n\n\nwith open('data/swiss-reviews-helpful-correct-bigger.txt', 'r') as fp:\n    swiss_reviews_helpful = fp.readlines()\n\n\"\"\"\nExplanation: 2) Extract the asins of the products which the helpful reviews correspond to\nEnd of explanation\n\"\"\"\n\n\ndef filter_asin(line):\n    l = line.rstrip('\\n')\n    l = yaml.load(l)\n    if('asin' in l.keys()):\n        return l['asin']\n    else:\n        return ''\n\nhelpful_asins = []\ncounter = 1\nfor item in swiss_reviews_helpful:\n    if(counter%500 == 0):\n        print(counter)\n    counter += 1\n    x = filter_asin(item)\n    if(len(x) > 0):\n        helpful_asins.append(x)\n\n\"\"\"\nExplanation: The following function simply extracts the 'asin' from the helpful reviews.\nRepetitions of the asins are of no consequence, as the list is just meant to be a check up.\nEnd of explanation\n\"\"\"\n\n\nimport pickle\n\nwith open('data/helpful_asins_bigger.pickle', 'wb') as fp:\n    pickle.dump(helpful_asins, fp)\n\n\"\"\"\nExplanation: Save the list of asins.\nEnd of explanation\n\"\"\"\n\nget_ipython().magic('load_ext autoreload')\nget_ipython().magic('autoreload 2')\n\nimport glob\nimport logging\nimport numpy as np\nimport os\n\nlogging.basicConfig(format=\n                          \"%(relativeCreated)12d [%(filename)s:%(funcName)20s():%(lineno)s] [%(process)d] %(message)s\",\n                    # filename=\"/tmp/caiman.log\",\n                    level=logging.WARNING)\n\nimport caiman as cm\nfrom caiman.source_extraction import cnmf as cnmf\nfrom caiman.utils.utils import download_demo\nimport matplotlib.pyplot as plt\n\nimport bokeh.plotting as bpl\nbpl.output_notebook()\n\n\"\"\"\nExplanation: Example of 1p online analysis using a Ring CNN + OnACID\nThe demo shows how to perform online analysis on one photon data using a Ring CNN for extracting the background followed by processing using the OnACID algorithm. The algorithm relies on the usage a GPU to efficiently estimate and apply the background model so it is recommended to have access to a GPU when running this notebook.\nEnd of explanation\n\"\"\"\n\n\nfnames=download_demo('blood_vessel_10Hz.mat')\n\n\"\"\"\nExplanation: First specify the data file(s) to be analyzed\nThe download_demo method will download the file (if not already present) and store it inside your caiman_data/example_movies folder. You can specify any path to files you want to analyze.\nEnd of explanation\n\"\"\"\n\n\nreuse_model = False                                                 # set to True to re-use an existing ring model\npath_to_model = None                                                # specify a pre-trained model here if needed \ngSig = (7, 7)                                                       # expected half size of neurons\ngnb = 2                                                             # number of background components for OnACID\ninit_batch = 500                                                    # number of frames for initialization and training\n\nparams_dict = {'fnames': fnames,\n               'var_name_hdf5': 'Y',                                # name of variable inside mat file where the data is stored\n               'fr': 10,                                            # frame rate (Hz)\n               'decay_time': 0.5,                                   # approximate length of transient event in seconds\n               'gSig': gSig,\n               'p': 0,                                              # order of AR indicator dynamics\n               'ring_CNN': True,                                    # SET TO TRUE TO USE RING CNN \n               'min_SNR': 2.65,                                     # minimum SNR for accepting new components\n               'SNR_lowest': 0.75,                                  # reject components with SNR below this value\n               'use_cnn': False,                                    # do not use CNN based test for components\n               'use_ecc': True,                                     # test eccentricity\n               'max_ecc': 2.625,                                    # reject components with eccentricity above this value\n               'rval_thr': 0.70,                                    # correlation threshold for new component inclusion\n               'rval_lowest': 0.25,                                 # reject components with corr below that value\n               'ds_factor': 1,                                      # spatial downsampling factor (increases speed but may lose some fine structure)\n               'nb': gnb,\n               'motion_correct': False,                             # Flag for motion correction\n               'init_batch': init_batch,                            # number of frames for initialization (presumably from the first file)\n               'init_method': 'bare',\n               'normalize': False,\n               'expected_comps': 1100,                               # maximum number of expected components used for memory pre-allocation (exaggerate here)\n               'sniper_mode': False,                                 # flag using a CNN to detect new neurons (o/w space correlation is used)\n               'dist_shape_update' : True,                           # flag for updating shapes in a distributed way\n               'min_num_trial': 5,                                   # number of candidate components per frame\n               'epochs': 3,                                          # number of total passes over the data\n               'stop_detection': True,                               # Run a last epoch without detecting new neurons  \n               'K': 50,                                              # initial number of components\n               'lr': 6e-4,\n               'lr_scheduler': [0.9, 6000, 10000],\n               'pct': 0.01,\n               'path_to_model': path_to_model,                       # where the ring CNN model is saved/loaded\n               'reuse_model': reuse_model                            # flag for re-using a ring CNN model          \n              }\nopts = cnmf.params.CNMFParams(params_dict=params_dict)\n\n\"\"\"\nExplanation: Set up some parameters\nHere we set up some parameters for specifying the ring model and running OnACID. We use the same params object as in batch processing with CNMF.\nEnd of explanation\n\"\"\"\n\n\nrun_onacid = True\n\nif run_onacid:\n    cnm = cnmf.online_cnmf.OnACID(params=opts)\n    cnm.fit_online()\n    fld_name = os.path.dirname(cnm.params.ring_CNN['path_to_model'])\n    res_name_nm = os.path.join(fld_name, 'onacid_results_nm.hdf5')\n    cnm.save(res_name_nm)                # save initial results (without any postprocessing)\nelse:\n    fld_name = os.path.dirname(path_to_model)\n    res_name = os.path.join(fld_name, 'onacid_results.hdf5')\n    cnm = cnmf.online_cnmf.load_OnlineCNMF(res_name)\n    cnm.params.data['fnames'] = fnames\n\n\"\"\"\nExplanation: Now run the Ring-CNN + CaImAn online algorithm (OnACID).\nThe first initbatch frames are used for training the ring-CNN model. Once the model is trained the background is subtracted and the different is used for initialization purposes. The initialization method chosen here bare will only search for a small number of neurons and is mostly used to initialize the background components. Initialization with the full CNMF can also be used by choosing cnmf.\nWe first create an OnACID object located in the module online_cnmf and we pass the parameters similarly to the case of batch processing. We then run the algorithm using the fit_online method. We then save the results inside\nthe folder where the Ring_CNN model is saved.\nEnd of explanation\n\"\"\"\n\n\nds = 10             # plot every ds frames to make more manageable figures\ninit_batch = 500\ndims, T = cnmf.utilities.get_file_size(fnames, var_name_hdf5='Y')\nT = np.array(T).sum()\nn_epochs = cnm.params.online['epochs']\nT_detect = 1e3*np.hstack((np.zeros(init_batch), cnm.t_detect))\nT_shapes = 1e3*np.hstack((np.zeros(init_batch), cnm.t_shapes))\nT_online = 1e3*np.hstack((np.zeros(init_batch), cnm.t_online)) - T_detect - T_shapes\nplt.figure()\nplt.stackplot(np.arange(len(T_detect))[::ds], T_online[::ds], T_detect[::ds], T_shapes[::ds],\n              colors=['tab:red', 'tab:purple', 'tab:brown'])\nplt.legend(labels=['process', 'detect', 'shapes'], loc=2)\nplt.title('Processing time allocation')\nplt.xlabel('Frame #')\nplt.ylabel('Processing time [ms]')\nmax_val = 80\nplt.ylim([0, max_val]);\nplt.plot([init_batch, init_batch], [0, max_val], '--k')\nfor i in range(n_epochs - 1):\n    plt.plot([(i+1)*T, (i+1)*T], [0, max_val], '--k')\nplt.xlim([0, n_epochs*T]);\nplt.savefig(os.path.join(fld_name, 'time_per_frame_ds.pdf'), bbox_inches='tight', pad_inches=0)\n\ninit_batch = 500\nplt.figure()\ntc_init = cnm.t_init*np.ones(T*n_epochs)\nds = 10\n#tc_mot = np.hstack((np.zeros(init_batch), np.cumsum(T_motion)/1000))\ntc_prc = np.cumsum(T_online)/1000#np.hstack((np.zeros(init_batch), ))\ntc_det = np.cumsum(T_detect)/1000#np.hstack((np.zeros(init_batch), ))\ntc_shp = np.cumsum(T_shapes)/1000#np.hstack((np.zeros(init_batch), ))\nplt.stackplot(np.arange(len(tc_init))[::ds], tc_init[::ds], tc_prc[::ds], tc_det[::ds], tc_shp[::ds],\n              colors=['g', 'tab:red', 'tab:purple', 'tab:brown'])\nplt.legend(labels=['initialize', 'process', 'detect', 'shapes'], loc=2)\nplt.title('Processing time allocation')\nplt.xlabel('Frame #')\nplt.ylabel('Processing time [s]')\nmax_val = (tc_prc[-1] + tc_det[-1] + tc_shp[-1] + cnm.t_init)*1.05\nfor i in range(n_epochs - 1):\n    plt.plot([(i+1)*T, (i+1)*T], [0, max_val], '--k')\nplt.xlim([0, n_epochs*T]);\nplt.ylim([0, max_val])\nplt.savefig(os.path.join(fld_name, 'time_cumulative_ds.pdf'), bbox_inches='tight', pad_inches=0)\n\nprint('Cost of estimating model and running first epoch: {:.2f}s'.format(tc_prc[T] + tc_det[T] + tc_shp[T] + tc_init[T]))\n\n\"\"\"\nExplanation: Check speed\nCreate some plots that show the speed per frame and cumulatively\nEnd of explanation\n\"\"\"\n\n\n# first compute background summary images\nimages = cm.load(fnames, var_name_hdf5='Y', subindices=slice(None, None, 2))\ncn_filter, pnr = cm.summary_images.correlation_pnr(images, gSig=3, swap_dim=False) # change swap dim if output looks weird, it is a problem with tiffile\n\nplt.figure(figsize=(15, 7))\nplt.subplot(1,2,1); plt.imshow(cn_filter); plt.colorbar()\nplt.subplot(1,2,2); plt.imshow(pnr); plt.colorbar()\n\ncnm.estimates.plot_contours_nb(img=cn_filter, idx=cnm.estimates.idx_components, line_color='white', thr=0.3)\n\n\"\"\"\nExplanation: Do some initial plotting\nEnd of explanation\n\"\"\"\n\n\ncnm.estimates.nb_view_components(img=cn_filter, denoised_color='red')\n\n\"\"\"\nExplanation: View components\nNow inspect the components extracted by OnACID. Note that if single pass was used then several components would be non-zero only for the part of the time interval indicating that they were detected online by OnACID.\nNote that if you get data rate error you can start Jupyter notebooks using:\n'jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10'\nEnd of explanation\n\"\"\"\n\n\nsave_file = True\nif save_file:\n    from caiman.utils.nn_models import create_LN_model\n    model_LN = create_LN_model(images, shape=opts.data['dims'] + (1,), n_channels=opts.ring_CNN['n_channels'],\n                               width=opts.ring_CNN['width'], use_bias=opts.ring_CNN['use_bias'], gSig=gSig[0],\n                               use_add=opts.ring_CNN['use_add'])\n    model_LN.load_weights(cnm.params.ring_CNN['path_to_model'])\n\n    # Load the data in batches and save them\n\n    m = []\n    saved_files = []\n    batch_length = 256\n    for i in range(0, T, batch_length):\n        images = cm.load(fnames, var_name_hdf5='Y', subindices=slice(i, i + batch_length))\n        images_filt = np.squeeze(model_LN.predict(np.expand_dims(images, axis=-1)))\n        temp_file = os.path.join(fld_name, 'pfc_back_removed_' + format(i, '05d') + '.h5')\n        saved_files.append(temp_file)\n        m = cm.movie(np.maximum(images - images_filt, 0))\n        m.save(temp_file)\nelse:\n    saved_files = glob.glob(os.path.join(fld_name, 'pfc_back_removed_*'))\n    saved_files.sort()\n\nfname_mmap = cm.save_memmap([saved_files], order='C', border_to_0=0)\nYr, dims, T = cm.load_memmap(fname_mmap)\nimages_mmap = Yr.T.reshape((T,) + dims, order='F')\n\n\"\"\"\nExplanation: Load ring model to filter the data\nFilter the data with the learned Ring CNN model and a create memory mapped file with the background subtracted data. We will use this to run the quality tests and screen for false positive components.\nEnd of explanation\n\"\"\"\n\n\ncnm.params.merging['merge_thr'] = 0.7\ncnm.estimates.c1 = np.zeros(cnm.estimates.A.shape[-1])\ncnm.estimates.bl = np.zeros(cnm.estimates.A.shape[-1])\ncnm.estimates.neurons_sn = np.zeros(cnm.estimates.A.shape[-1])\ncnm.estimates.g = None #np.ones((cnm.estimates.A.shape[-1], 1))*.9\ncnm.estimates.merge_components(Yr, cnm.params)\n\n\"\"\"\nExplanation: Merge components\nEnd of explanation\n\"\"\"\n\n\ncnm.params.quality\n\ncnm.estimates.evaluate_components(imgs=images_mmap, params=cnm.params)\n\ncnm.estimates.plot_contours_nb(img=cn_filter, idx=cnm.estimates.idx_components, line_color='white')\n\ncnm.estimates.nb_view_components(idx=cnm.estimates.idx_components, img=cn_filter)\n\n\"\"\"\nExplanation: Evaluate components and compare again\nWe run the component evaluation tests to screen for false positive components.\nEnd of explanation\n\"\"\"\n\n\ncnmfe_results = download_demo('online_vs_offline.npz')\nlocals().update(np.load(cnmfe_results, allow_pickle=True))\nA_patch_good = A_patch_good.item()\nestimates_gt = cnmf.estimates.Estimates(A=A_patch_good, C=C_patch_good, dims=dims)\n\n\nmaxthr=0.01\ncnm.estimates.A_thr=None\ncnm.estimates.threshold_spatial_components(maxthr=maxthr)\nestimates_gt.A_thr=None\nestimates_gt.threshold_spatial_components(maxthr=maxthr*10)\nmin_size = np.pi*(gSig[0]/1.5)**2\nmax_size = np.pi*(gSig[0]*1.5)**2\nntk = cnm.estimates.remove_small_large_neurons(min_size_neuro=min_size, max_size_neuro=2*max_size)\ngtk = estimates_gt.remove_small_large_neurons(min_size_neuro=min_size, max_size_neuro=2*max_size)\n\nm1, m2, nm1, nm2, perf = cm.base.rois.register_ROIs(estimates_gt.A_thr[:, estimates_gt.idx_components],\n                                                  cnm.estimates.A_thr[:, cnm.estimates.idx_components],\n                                                  dims, align_flag=False, thresh_cost=.7, plot_results=True,\n                                                  Cn=cn_filter, enclosed_thr=None)[:-1]\n\n\"\"\"\nExplanation: Compare against CNMF-E results\nWe download the results of CNMF-E on the same dataset and compare.\nEnd of explanation\n\"\"\"\n\n\nfor k, v in perf.items():\n    print(k + ':', '%.4f' % v, end='  ')\n\n\"\"\"\nExplanation: Print performance results\nEnd of explanation\n\"\"\"\n\n\nres_name = os.path.join(fld_name, 'onacid_results.hdf5')\ncnm.save(res_name)\n\n\"\"\"\nExplanation: Save the results\nEnd of explanation\n\"\"\"\n\n\nimport matplotlib.lines as mlines\nlp, hp = np.nanpercentile(cn_filter, [5, 98])\nA_onacid = cnm.estimates.A_thr.toarray().copy()\nA_onacid /= A_onacid.max(0)\n\nA_TP = estimates_gt.A[:, m1].toarray() #cnm.estimates.A[:, cnm.estimates.idx_components[m2]].toarray()\nA_TP = A_TP.reshape(dims + (-1,), order='F').transpose(2,0,1)\nA_FN = estimates_gt.A[:, nm1].toarray()\nA_FN = A_FN.reshape(dims + (-1,), order='F').transpose(2,0,1)\nA_FP = A_onacid[:,cnm.estimates.idx_components[nm2]]\nA_FP = A_FP.reshape(dims + (-1,), order='F').transpose(2,0,1)\n\n\nplt.figure(figsize=(15, 12))\nplt.imshow(cn_filter, vmin=lp, vmax=hp, cmap='viridis')\nplt.colorbar();\n\nfor aa in A_TP:\n    plt.contour(aa, [0.05], colors='k');\nfor aa in A_FN:\n    plt.contour(aa, [0.05], colors='r');\nfor aa in A_FP:\n    plt.contour(aa, [0.25], colors='w');\ncl = ['k', 'r', 'w']\nlb = ['both', 'CNMF-E only', 'ring CNN only']\nday = [mlines.Line2D([], [], color=cl[i], label=lb[i]) for i in range(3)]\nplt.legend(handles=day, loc=3)\nplt.axis('off');\nplt.margins(0, 0);\nplt.savefig(os.path.join(fld_name, 'ring_CNN_contours_gSig_3.pdf'), bbox_inches='tight', pad_inches=0)\n\nA_rej = cnm.estimates.A[:, cnm.estimates.idx_components_bad].toarray()\nA_rej = A_rej.reshape(dims + (-1,), order='F').transpose(2,0,1)\nplt.figure(figsize=(15, 15))\nplt.imshow(cn_filter, vmin=lp, vmax=hp, cmap='viridis')\nplt.title('Rejected Components')\nfor aa in A_rej:\n    plt.contour(aa, [0.05], colors='w');\n\n\"\"\"\nExplanation: Make some plots\nEnd of explanation\n\"\"\"\n\n\nfrom caiman.utils.nn_models import create_LN_model\nmodel_LN = create_LN_model(images, shape=opts.data['dims'] + (1,), n_channels=opts.ring_CNN['n_channels'],\n                           width=opts.ring_CNN['width'], use_bias=opts.ring_CNN['use_bias'], gSig=gSig[0],\n                           use_add=opts.ring_CNN['use_add'])\nmodel_LN.load_weights(cnm.params.ring_CNN['path_to_model'])\n\nW = model_LN.get_weights()\n\nplt.figure(figsize=(10, 10))\nplt.subplot(2,2,1); plt.imshow(np.squeeze(W[0][:,:,:,0])); plt.colorbar(); plt.title('Ring Kernel 1')\nplt.subplot(2,2,2); plt.imshow(np.squeeze(W[0][:,:,:,1])); plt.colorbar(); plt.title('Ring Kernel 2')\nplt.subplot(2,2,3); plt.imshow(np.squeeze(W[-1][:,:,0])); plt.colorbar(); plt.title('Multiplicative Layer 1')\nplt.subplot(2,2,4); plt.imshow(np.squeeze(W[-1][:,:,1])); plt.colorbar(); plt.title('Multiplicative Layer 2');\n\n\"\"\"\nExplanation: Show the learned filters\nEnd of explanation\n\"\"\"\n\n\nm1 = cm.load(fnames, var_name_hdf5='Y')  # original data\nm2 = cm.load(fname_mmap)  # background subtracted data\nm3 = m1 - m2  # estimated background\nm4 = cm.movie(cnm.estimates.A[:,cnm.estimates.idx_components].dot(cnm.estimates.C[cnm.estimates.idx_components])).reshape(dims + (T,)).transpose(2,0,1)\n              # estimated components\n\nnn = 0.01\nmm = 1 - nn/4   # normalize movies by quantiles\nm1 = (m1 - np.quantile(m1[:1000], nn))/(np.quantile(m1[:1000], mm) - np.quantile(m1[:1000], nn))\nm2 = (m2 - np.quantile(m2[:1000], nn))/(np.quantile(m2[:1000], mm) - np.quantile(m2[:1000], nn))\nm3 = (m3 - np.quantile(m3[:1000], nn))/(np.quantile(m3[:1000], mm) - np.quantile(m3[:1000], nn))\nm4 = (m4 - np.quantile(m4[:1000], nn))/(np.quantile(m4[:1000], mm) - np.quantile(m4[:1000], nn))\n\nm = cm.concatenate((cm.concatenate((m1.transpose(0,2,1), m3.transpose(0,2,1)), axis=2),\n                    cm.concatenate((m2.transpose(0,2,1), m4), axis=2)), axis=1)\n\nm[:3000].play(magnification=2, q_min=1, plot_text=True,\n              save_movie=True, movie_name=os.path.join(fld_name, 'movie.avi'))\n\n\"\"\"\nExplanation: Make a movie\nEnd of explanation\n\"\"\"\n\n#$HIDE_INPUT$\nimport pandas as pd\nfrom IPython.display import display\n\nred_wine = pd.read_csv('../input/dl-course-data/red-wine.csv')\n\n# Create training and validation splits\ndf_train = red_wine.sample(frac=0.7, random_state=0)\ndf_valid = red_wine.drop(df_train.index)\ndisplay(df_train.head(4))\n\n# Scale to [0, 1]\nmax_ = df_train.max(axis=0)\nmin_ = df_train.min(axis=0)\ndf_train = (df_train - min_) / (max_ - min_)\ndf_valid = (df_valid - min_) / (max_ - min_)\n\n# Split features and target\nX_train = df_train.drop('quality', axis=1)\nX_valid = df_valid.drop('quality', axis=1)\ny_train = df_train['quality']\ny_valid = df_valid['quality']\n\n\"\"\"\nExplanation: Introduction\nIn the first two lessons, we learned how to build fully-connected networks out of stacks of dense layers. When first created, all of the network's weights are set randomly -- the network doesn't \"know\" anything yet. In this lesson we're going to see how to train a neural network; we're going to see how neural networks learn.\nAs with all machine learning tasks, we begin with a set of training data. Each example in the training data consists of some features (the inputs) together with an expected target (the output). Training the network means adjusting its weights in such a way that it can transform the features into the target. In the 80 Cereals dataset, for instance, we want a network that can take each cereal's 'sugar', 'fiber', and 'protein' content and produce a prediction for that cereal's 'calories'. If we can successfully train a network to do that, its weights must represent in some way the relationship between those features and that target as expressed in the training data.\nIn addition to the training data, we need two more things:\n- A \"loss function\" that measures how good the network's predictions are.\n- An \"optimizer\" that can tell the network how to change its weights.\nThe Loss Function\nWe've seen how to design an architecture for a network, but we haven't seen how to tell a network what problem to solve. This is the job of the loss function.\nThe loss function measures the disparity between the the target's true value and the value the model predicts. \nDifferent problems call for different loss functions. We have been looking at regression problems, where the task is to predict some numerical value -- calories in 80 Cereals, rating in Red Wine Quality. Other regression tasks might be predicting the price of a house or the fuel efficiency of a car.\nA common loss function for regression problems is the mean absolute error or MAE. For each prediction y_pred, MAE measures the disparity from the true target y_true by an absolute difference abs(y_true - y_pred).\nThe total MAE loss on a dataset is the mean of all these absolute differences.\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/VDcvkZN.png\" width=\"500\" alt=\"A graph depicting error bars from data points to the fitted line..\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>The mean absolute error is the average length between the fitted curve and the data points.\n</center></figcaption>\n</figure>\n\nBesides MAE, other loss functions you might see for regression problems are the mean-squared error (MSE) or the Huber loss (both available in Keras).\nDuring training, the model will use the loss function as a guide for finding the correct values of its weights (lower loss is better). In other words, the loss function tells the network its objective.\nThe Optimizer - Stochastic Gradient Descent\nWe've described the problem we want the network to solve, but now we need to say how to solve it. This is the job of the optimizer. The optimizer is an algorithm that adjusts the weights to minimize the loss.\nVirtually all of the optimization algorithms used in deep learning belong to a family called stochastic gradient descent. They are iterative algorithms that train a network in steps. One step of training goes like this:\n1. Sample some training data and run it through the network to make predictions.\n2. Measure the loss between the predictions and the true values.\n3. Finally, adjust the weights in a direction that makes the loss smaller.\nThen just do this over and over until the loss is as small as you like (or until it won't decrease any further.)\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/rFI1tIk.gif\" width=\"1600\" alt=\"Fitting a line batch by batch. The loss decreases and the weights approach their true values.\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>Training a neural network with Stochastic Gradient Descent.\n</center></figcaption>\n</figure>\n\nEach iteration's sample of training data is called a minibatch (or often just \"batch\"), while a complete round of the training data is called an epoch. The number of epochs you train for is how many times the network will see each training example.\nThe animation shows the linear model from Lesson 1 being trained with SGD. The pale red dots depict the entire training set, while the solid red dots are the minibatches. Every time SGD sees a new minibatch, it will shift the weights (w the slope and b the y-intercept) toward their correct values on that batch. Batch after batch, the line eventually converges to its best fit. You can see that the loss gets smaller as the weights get closer to their true values.\nLearning Rate and Batch Size\nNotice that the line only makes a small shift in the direction of each batch (instead of moving all the way). The size of these shifts is determined by the learning rate. A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values.\nThe learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds. Their interaction is often subtle and the right choice for these parameters isn't always obvious. (We'll explore these effects in the exercise.)\nFortunately, for most work it won't be necessary to do an extensive hyperparameter search to get satisfactory results. Adam is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is \"self tuning\", in a sense). Adam is a great general-purpose optimizer.\nAdding the Loss and Optimizer\nAfter defining a model, you can add a loss function and optimizer with the model's compile method:\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"mae\",\n)\nNotice that we are able to specify the loss and optimizer with just a string. You can also access these directly through the Keras API -- if you wanted to tune parameters, for instance -- but for us, the defaults will work fine.\n<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n    <strong>What's In a Name?</strong><br>\nThe <strong>gradient</strong> is a vector that tells us in what direction the weights need to go. More precisely, it tells us how to change the weights to make the loss change <em>fastest</em>. We call our process gradient <strong>descent</strong> because it uses the gradient to <em>descend</em> the loss curve towards a minimum. <strong>Stochastic</strong> means \"determined by chance.\" Our training is <em>stochastic</em> because the minibatches are <em>random samples</em> from the dataset. And that's why it's called SGD!\n</blockquote>\n\nExample - Red Wine Quality\nNow we know everything we need to start training deep learning models. So let's see it in action! We'll use the Red Wine Quality dataset.\nThis dataset consists of physiochemical measurements from about 1600 Portuguese red wines. Also included is a quality rating for each wine from blind taste-tests. How well can we predict a wine's perceived quality from these measurements?\nWe've put all of the data preparation into this next hidden cell. It's not essential to what follows so feel free to skip it. One thing you might note for now though is that we've rescaled each feature to lie in the interval $[0, 1]$. As we'll discuss more in Lesson 5, neural networks tend to perform best when their inputs are on a common scale.\nEnd of explanation\n\"\"\"\n\n\nprint(X_train.shape)\n\n\"\"\"\nExplanation: How many inputs should this network have? We can discover this by looking at the number of columns in the data matrix. Be sure not to include the target ('quality') here -- only the input features.\nEnd of explanation\n\"\"\"\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(512, activation='relu', input_shape=[11]),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(1),\n])\n\n\"\"\"\nExplanation: Eleven columns means eleven inputs.\nWe've chosen a three-layer network with over 1500 neurons. This network should be capable of learning fairly complex relationships in the data.\nEnd of explanation\n\"\"\"\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='mae',\n)\n\n\"\"\"\nExplanation: Deciding the architecture of your model should be part of a process. Start simple and use the validation loss as your guide. You'll learn more about model development in the exercises.\nAfter defining the model, we compile in the optimizer and loss function.\nEnd of explanation\n\"\"\"\n\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=256,\n    epochs=10,\n)\n\n\"\"\"\nExplanation: Now we're ready to start the training! We've told Keras to feed the optimizer 256 rows of the training data at a time (the batch_size) and to do that 10 times all the way through the dataset (the epochs).\nEnd of explanation\n\"\"\"\n\n\nimport pandas as pd\n\n# convert the training history to a dataframe\nhistory_df = pd.DataFrame(history.history)\n# use Pandas native plot method\nhistory_df['loss'].plot();\n\n\"\"\"\nExplanation: You can see that Keras will keep you updated on the loss as the model trains.\nOften, a better way to view the loss though is to plot it. The fit method in fact keeps a record of the loss produced during training in a History object. We'll convert the data to a Pandas dataframe, which makes the plotting easy.\nEnd of explanation\n\"\"\"\n\nimport time\nfrom datetime import datetime\n\nfrom typing import List\n\nimport numpy as np\nimport pandas as pd\n\nimport google.auth\n\nfrom google.cloud import logging_v2\nfrom google.cloud.monitoring_dashboard.v1 import DashboardsServiceClient\nfrom google.cloud.logging_v2 import MetricsServiceV2Client\nfrom google.cloud.monitoring_v3.query import Query\nfrom google.cloud.monitoring_v3 import MetricServiceClient\n\nimport matplotlib.pyplot as plt\n\n\"\"\"\nExplanation: Analyzing Locust Load Testing Results\nThis Notebook demonstrates how to analyze AI Platform Prediction load testing runs using metrics captured in Cloud Monitoring. \nThis Notebook build on the 02-perf-testing.ipynb notebook that shows how to configure and run load tests against AI Platform Prediction using Locust.io. The outlined testing process results in a Pandas dataframe that aggregates the standard AI Platform Prediction metrics with a set of custom, log-based metrics generated from log entries captured by the Locust testing script.\nThe Notebook covers the following steps:\n1. Retrieve and consolidate test results from Cloud Monitoring\n2. Analyze and visualize utilization and latency results\nSetup\nThis notebook was tested on AI Platform Notebooks using the standard TF 2.2 image.\nImport libraries\nEnd of explanation\n\"\"\"\n\n\nPROJECT_ID = '[your-project-id]' # Set your project Id\nMODEL_NAME = 'resnet_classifier'\nMODEL_VERSION = 'v1'\nLOG_NAME = 'locust' # Set your log name\nTEST_ID = 'test-20200829-190943' # Set your test Id\nTEST_START_TIME = datetime.fromisoformat('2020-08-28T21:30:00-00:00') # Set your test start time\nTEST_END_TIME = datetime.fromisoformat('2020-08-29T22:00:00-00:00') # Set your test end time\n\n\n\"\"\"\nExplanation: Configure GCP environment settings\nEnd of explanation\n\"\"\"\n\n\ncreds , _ = google.auth.default()\nclient = MetricServiceClient(credentials=creds)\n\nproject_path = client.project_path(PROJECT_ID)\nfilter = 'metric.type=starts_with(\"ml.googleapis.com/prediction\")'\n\nfor descriptor in client.list_metric_descriptors(project_path, filter_=filter):\n    print(descriptor.type)\n\n\"\"\"\nExplanation: 1. Retrieve and consolidate test results\nLocust's web interface along with a Cloud Monitoring dashboard provide a cursory view into performance of a tested AI Platform Prediction model version. A more thorough analysis can be performed by consolidating metrics collected during a test and using data analytics and visualization tools.\nIn this section, you will retrieve the metrics captured in Cloud Monitoring and consolidate them into a single Pandas dataframe.\n1.1 List available AI Platform Prediction metrics\nEnd of explanation\n\"\"\"\n\n\nfilter = 'metric.type=starts_with(\"logging.googleapis.com/user\")'\n\nfor descriptor in client.list_metric_descriptors(project_path, filter_=filter):\n    print(descriptor.type)\n\n\"\"\"\nExplanation: 1.2. List custom log based metrics\nEnd of explanation\n\"\"\"\n\n\ndef retrieve_metrics(client, project_id, start_time, end_time, model, model_version, test_id, log_name):\n    \"\"\"\n    Retrieves test metrics from Cloud Monitoring.\n    \"\"\"\n    def _get_aipp_metric(metric_type: str, labels: List[str]=[], metric_name=None)-> pd.DataFrame:\n        \"\"\"\n        Retrieves a specified AIPP metric.\n        \"\"\"\n        query = Query(client, project_id, metric_type=metric_type)\n        query = query.select_interval(end_time, start_time)\n        query = query.select_resources(model_id=model)\n        query = query.select_resources(version_id=model_version)\n        \n        if metric_name:\n            labels = ['metric'] + labels \n        df = query.as_dataframe(labels=labels)\n        \n        if not df.empty:\n            if metric_name:\n                df.columns.set_levels([metric_name], level=0, inplace=True)\n            df = df.set_index(df.index.round('T'))\n        \n        return df\n    \n    def _get_locust_metric(metric_type: str, labels: List[str]=[], metric_name=None)-> pd.DataFrame:\n        \"\"\"\n        Retrieves a specified custom log-based metric.\n        \"\"\"\n        query = Query(client, project_id, metric_type=metric_type)\n        query = query.select_interval(end_time, start_time)\n        query = query.select_metrics(log=log_name)\n        query = query.select_metrics(test_id=test_id)\n        \n        if metric_name:\n            labels = ['metric'] + labels \n        df = query.as_dataframe(labels=labels)\n        \n        if not df.empty: \n            if metric_name:\n                df.columns.set_levels([metric_name], level=0, inplace=True)\n            df = df.apply(lambda row: [metric.mean for metric in row])\n            df = df.set_index(df.index.round('T'))\n        \n        return df\n    \n    # Retrieve GPU duty cycle\n    metric_type = 'ml.googleapis.com/prediction/online/accelerator/duty_cycle'\n    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'duty_cycle')\n    df = metric\n\n    # Retrieve CPU utilization\n    metric_type = 'ml.googleapis.com/prediction/online/cpu/utilization'\n    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'cpu_utilization')\n    if not metric.empty:\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve prediction count\n    metric_type = 'ml.googleapis.com/prediction/prediction_count'\n    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'prediction_count')\n    if not metric.empty:\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve responses per second\n    metric_type = 'ml.googleapis.com/prediction/response_count'\n    metric = _get_aipp_metric(metric_type, ['replica_id', 'signature'], 'response_rate')\n    if not metric.empty:\n        metric = (metric/60).round(2)\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve backend latencies\n    metric_type = 'ml.googleapis.com/prediction/latencies'\n    metric = _get_aipp_metric(metric_type, ['latency_type', 'replica_id', 'signature'])\n    if not metric.empty:\n        metric = metric.apply(lambda row: [round(latency.mean/1000,1) for latency in row])\n        metric.columns.set_names(['metric', 'replica_id', 'signature'], inplace=True)\n        level_values = ['Latency: ' + value for value in metric.columns.get_level_values(level=0)]\n        metric.columns.set_levels(level_values, level=0, inplace=True)\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve Locust latency\n    metric_type = 'logging.googleapis.com/user/locust_latency'\n    metric = _get_locust_metric(metric_type, ['replica_id', 'signature'], 'Latency: client')\n    if not metric.empty:\n        metric = metric.round(2).replace([0], np.nan)\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve Locust user count\n    metric_type = 'logging.googleapis.com/user/locust_users'\n    metric = _get_locust_metric(metric_type, ['replica_id', 'signature'], 'User count')\n    if not metric.empty:\n        metric = metric.round()\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve Locust num_failures\n    metric_type = 'logging.googleapis.com/user/num_failures'\n    metric = _get_locust_metric(metric_type, ['replica_id', 'signature'], 'Num of failures')\n    if not metric.empty:\n        metric = metric.round()\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n    \n    # Retrieve Locust num_failures\n    metric_type = 'logging.googleapis.com/user/num_requests'\n    metric = _get_locust_metric(metric_type, ['replica_id', 'signature'], 'Num of requests')\n    if not metric.empty:\n        metric = metric.round()\n        df = df.merge(metric, how='outer', right_index=True, left_index=True)\n\n    return df\n    \n\ntest_result = retrieve_metrics(\n    client, \n    PROJECT_ID, \n    TEST_START_TIME, \n    TEST_END_TIME, \n    MODEL_NAME, \n    MODEL_VERSION,\n    TEST_ID, \n    LOG_NAME\n)\n\ntest_result.head().T\n\n\"\"\"\nExplanation: 1.3. Retrieve test metrics\nDefine a helper function that retrieves test metrics from Cloud Monitoring\nEnd of explanation\n\"\"\"\n\n\ngpu_utilization_results = test_result['duty_cycle']\ngpu_utilization_results.columns = gpu_utilization_results.columns.get_level_values(0)\nax = gpu_utilization_results.plot(figsize=(14, 9), legend=True)\nax.set_xlabel('Time', fontsize=16)\nax.set_ylabel('Utilization ratio', fontsize=16)\n_ = ax.set_title(\"GPU Utilization\", fontsize=20)\n\n\"\"\"\nExplanation: The retrieved dataframe uses hierarchical indexing for column names. The reason is that some metrics contain multiple time series. For example, the GPU duty_cycle metric includes a time series of measures per each GPU used in the deployment (denoted as replica_id). The top level of the column index is a metric name. The second level is a replica_id. The third level is a signature of a model.\nAll metrics are aligned on the same timeline. \n2. Analyzing and Visualizing test results\nIn the context of our scenario the key concern is GPU utilization at various levels of throughput and latency. The primary metric exposed by AI Platform Prediction to monitor GPU utilization is duty cycle. This metric captures an average fraction of time over the 60 second period during which the accelerator(s) were actively processing.\n2.1. GPU utilization\nEnd of explanation\n\"\"\"\n\n\ncpu_utilization_results = test_result['cpu_utilization']\ncpu_utilization_results.columns = cpu_utilization_results.columns.get_level_values(0)\nax = cpu_utilization_results.plot(figsize=(14, 9), legend=True)\nax.set_xlabel('Time', fontsize=16)\nax.set_ylabel('Utilization ratio', fontsize=16)\n_ = ax.set_title(\"CPU Utilization\", fontsize=20)\n\n\"\"\"\nExplanation: 2.2. CPU utilization\nEnd of explanation\n\"\"\"\n\n\nlatency_results = test_result[['Latency: model', 'Latency: client']]\nlatency_results.columns = latency_results.columns.get_level_values(0)\nax = latency_results.plot(figsize=(14, 9), legend=True)\nax.set_xlabel('Time', fontsize=16)\nax.set_ylabel('milisecond', fontsize=16)\n_ = ax.set_title(\"Latency\", fontsize=20)\n\n\"\"\"\nExplanation: 2.3. Latency\nEnd of explanation\n\"\"\"\n\n\nthroughput_results = test_result[['response_rate', 'User count']]\nthroughput_results.columns = throughput_results.columns.get_level_values(0)\nax = throughput_results.plot(figsize=(14, 9), legend=True)\nax.set_xlabel('Time', fontsize=16)\nax.set_ylabel('Count', fontsize=16)\n_ = ax.set_title(\"Response Rate vs User Count\", fontsize=20)\n\n\"\"\"\nExplanation: 2.4. Request throughput\nWe are going to use the response_rate metric, which tracks a number of responses returned by AI Platform Prediction over a 1 minute interval.\nEnd of explanation\n\"\"\"\n\n\nlogging_client = MetricsServiceV2Client(credentials=creds)\nparent = logging_client.project_path(PROJECT_ID)\n\nfor element in logging_client.list_log_metrics(parent):\n    metric_path = logging_client.metric_path(PROJECT_ID, element.name)\n    logging_client.delete_log_metric(metric_path)\n    print(\"Deleted metric: \", metric_path)\n\ndisplay_name = 'AI Platform Prediction and Locust'\ndashboard_service_client = DashboardsServiceClient(credentials=creds)\nparent = 'projects/{}'.format(PROJECT_ID)\nfor dashboard in dashboard_service_client.list_dashboards(parent):\n    if dashboard.display_name == display_name:\n        dashboard_service_client.delete_dashboard(dashboard.name)\n        print(\"Deleted dashboard:\", dashboard.name)\n\n\"\"\"\nExplanation: Cleaning up: delete the log-based metrics and dasboard\nEnd of explanation\n\"\"\"\n\nimport pylearn2.utils\nimport pylearn2.config\nimport theano\nimport neukrill_net.dense_dataset\nimport neukrill_net.utils\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport holoviews as hl\n%load_ext holoviews.ipython\nimport sklearn.metrics\n\ncd ..\n\nsettings = neukrill_net.utils.Settings(\"settings.json\")\nrun_settings = neukrill_net.utils.load_run_settings(\n    \"run_settings/replicate_8aug.json\", settings, force=True)\n\nmodel = pylearn2.utils.serial.load(run_settings['alt_picklepath'])\n\nc = 'train_objective'\nchannel = model.monitor.channels[c]\n\n\"\"\"\nExplanation: The following are the results we've got from online augmentation so far. Some bugs have been fixed by Scott since then so these might be redundant. If they're not redundant then they are very bad.\nLoading the pickle\nEnd of explanation\n\"\"\"\n\n\nplt.title(c)\nplt.plot(channel.example_record,channel.val_record)\n\nc = 'train_y_nll'\nchannel = model.monitor.channels[c]\nplt.title(c)\nplt.plot(channel.example_record,channel.val_record)\n\ndef plot_monitor(c = 'valid_y_nll'):\n    channel = model.monitor.channels[c]\n    plt.title(c)\n    plt.plot(channel.example_record,channel.val_record)\n    return None\nplot_monitor()\n\nplot_monitor(c=\"valid_objective\")\n\n\"\"\"\nExplanation: Replicating 8aug\nThe DensePNGDataset run with 8 augmentations got us most of the way to our best score in one go. If we can replicate that results with online augmentation then we can be pretty confident that online augmentation is a good idea. Unfortunately, it looks like we can't:\nEnd of explanation\n\"\"\"\n\n\n%run check_test_score.py run_settings/replicate_8aug.json\n\n\"\"\"\nExplanation: Would actually like to know what kind of score this model gets on the check_test_score script.\nEnd of explanation\n\"\"\"\n\n\nrun_settings = neukrill_net.utils.load_run_settings(\n    \"run_settings/online_manyaug.json\", settings, force=True)\n\nmodel = pylearn2.utils.serial.load(run_settings['alt_picklepath'])\n\nplot_monitor(c=\"valid_objective\")\n\n\"\"\"\nExplanation: So we can guess that the log loss score we're seeing is in fact correct. There are definitely some bugs in the ListDataset code.\nMany Augmentations\nWe want to be able to use online augmentations to run large combinations of different augmentations on the images. This model had almost everything turned on, a little:\nEnd of explanation\n\"\"\"\n\n\nsettings = neukrill_net.utils.Settings(\"settings.json\")\nrun_settings = neukrill_net.utils.load_run_settings(\n    \"run_settings/alexnet_based_onlineaug.json\", settings, force=True)\n\nmodel = pylearn2.utils.serial.load(run_settings['pickle abspath'])\n\nplot_monitor(c=\"train_y_nll\")\n\nplot_monitor(c=\"valid_y_nll\")\n\nplot_monitor(c=\"train_objective\")\n\nplot_monitor(c=\"valid_objective\")\n\n\"\"\"\nExplanation: Looks like it's completely incapable of learning.\nThese problems suggest that the augmentation might be garbling the images; making them useless for learning from. Or worse, garbling the order so each image doesn't correspond to its label.\nTransformer Results\nWe also have results from a network trained using a Transformer dataset, which is how online augmentation is supposed to be supported in Pylearn2.\nEnd of explanation\n\"\"\"\n\nfrom pprint import pprint\nfrom time import sleep\nfrom pynq import PL\nfrom pynq import Overlay\nfrom pynq.drivers import Trace_Buffer\nfrom pynq.iop import Pmod_TMP2\nfrom pynq.iop import PMODA\nfrom pynq.iop import PMODB\nfrom pynq.iop import ARDUINO\n\nol = Overlay(\"base.bit\")\nol.download()\npprint(PL.ip_dict)\n\n\"\"\"\nExplanation: Trace Buffer - Tracing IIC Transactions\nThe Trace_Buffer class can monitor the waveform and transations on PMODA, PMODB, and ARDUINO connectors.\nThis demo shows how to use this class to track IIC transactions. For this demo, users have to connect the Pmod TMP2 sensor to PMODA.\nStep 1: Overlay Management\nUsers have to import all the necessary classes. Make sure to use the right bitstream.\nEnd of explanation\n\"\"\"\n\n\ntmp2 = Pmod_TMP2(PMODA)\ntmp2.set_log_interval_ms(1)\n\n\"\"\"\nExplanation: Step 2: Instantiating Temperature Sensor\nAlthough this demo can also be done on PMODB, we use PMODA in this demo.\nSet the log interval to be 1ms. This means the IO Processor (IOP) will read temperature values every 1ms.\nEnd of explanation\n\"\"\"\n\n\ntr_buf = Trace_Buffer(PMODA,\"i2c\",samplerate=1000000)\n\n# Start the trace buffer\ntr_buf.start()\n\n# Issue reads for 1 second\ntmp2.start_log()\nsleep(1)\ntmp2_log = tmp2.get_log()\n\n# Stop the trace buffer\ntr_buf.stop()\n\n\"\"\"\nExplanation: Step 3: Tracking Transactions\nInstantiating the trace buffer with IIC protocol. The sample rate is set to 1MHz. Although the IIC clock is only 100kHz, we still have to use higher sample rate to keep track of IIC control signals from IOP.\nAfter starting the trace buffer DMA, also start to issue IIC reads for 1 second. Then stop the trace buffer DMA.\nEnd of explanation\n\"\"\"\n\n\n# Configuration for PMODA\nstart = 600\nstop = 10000\ntri_sel=[0x40000,0x80000]\ntri_0=[0x4,0x8]\ntri_1=[0x400,0x800]\nmask = 0x0\n\n# Parsing and decoding\ntr_buf.parse(\"i2c_trace.csv\",\n             start,stop,mask,tri_sel,tri_0,tri_1)\ntr_buf.set_metadata(['SDA','SCL'])\ntr_buf.decode(\"i2c_trace.pd\")\n\n\"\"\"\nExplanation: Step 4: Parsing and Decoding Transactions\nThe trace buffer object is able to parse the transactions into a *.csv file (saved into the same folder as this script). The input arguments for the parsing method is:\n    * start : the starting sample number of the trace.\n    * stop : the stopping sample number of the trace.\n    * tri_sel: masks for tri-state selection bits.\n    * tri_0: masks for pins selected when the corresponding tri_sel = 0.\n    * tri_0: masks for pins selected when the corresponding tri_sel = 1.\n    * mask: mask for pins selected always.\nFor PMODB, the configuration of the masks can be:\n    * tri_sel=[0x40000<<32,0x80000<<32]\n    * tri_0=[0x4<<32,0x8<<32]\n    * tri_1=[0x400<<32,0x800<<32]\n    * mask = 0x0\nThen the trace buffer object can also decode the transactions using the open-source sigrok decoders. The decoded file (*.pd) is saved into the same folder as this script.\nReference:\nhttps://sigrok.org/wiki/Main_Page\nEnd of explanation\n\"\"\"\n\n\ns0 = 1\ns1 = 5000\ntr_buf.display(s0,s1)\n\n\"\"\"\nExplanation: Step 5: Displaying the Result\nThe final waveform and decoded transactions are shown using the open-source wavedrom library. The two input arguments (s0 and s1 ) indicate the starting and stopping location where the waveform is shown. \nThe valid range for s0 and s1 is: 0 &lt; s0 &lt; s1 &lt; (stop-start), where start and stop are defined in the last step.\nReference:\nhttps://www.npmjs.com/package/wavedrom\nEnd of explanation\n\"\"\"\n\nfrom __future__ import division\nimport math, random, re\nfrom collections import defaultdict, Counter, deque\nfrom linear_algebra import dot, get_row, get_column, make_matrix, magnitude, scalar_multiply, shape, distance\nfrom functools import partial\n\nusers = [\n    { \"id\": 0, \"name\": \"Hero\" },\n    { \"id\": 1, \"name\": \"Dunn\" },\n    { \"id\": 2, \"name\": \"Sue\" },\n    { \"id\": 3, \"name\": \"Chi\" },\n    { \"id\": 4, \"name\": \"Thor\" },\n    { \"id\": 5, \"name\": \"Clive\" },\n    { \"id\": 6, \"name\": \"Hicks\" },\n    { \"id\": 7, \"name\": \"Devin\" },\n    { \"id\": 8, \"name\": \"Kate\" },\n    { \"id\": 9, \"name\": \"Klein\" }\n]\n\n\"\"\"\nExplanation: 21장 네트워크 분석\n많은 데이터 문제는 노드(node)와 그 사이를 연결하는 엣지(edge)로 구성된 네트워크(network)의 관점에서 볼 수 있다.\n예를들어, 페이스북에서는 사용자가 노드라면 그들의 친구 관계는 엣지가 된다.\n웹에서는 각 웹페이지가 노드이고 페이지 사이를 연결하는 하이퍼링크가 엣지가 된다.\n페이스북의 친구 관계는 상호적이다.\n내가 당신과 친구라면 당신은 반드시 나와 친구이다.\n즉, 이런 경우를 엣지에 방향이 없다(undirected)고 한다.\n반면 하이퍼링크는 그렇지 않다. \n내 홈페이지에는 대한민국 국회 홈페이지에 대한 링크가 있어도,\n반대로 대한민국 국회 홈페이지에는 내 홈페이지에 대한 링크가 없을 수 있다.\n이런 네트워크에는 방향이 있기 때문에 방향성 네트워크(directed network)라고 한다.\n21.1 매개 중심성\n1장에서 우리는 데이텀 네트워크에서 친구의 수를 셈으로써 중심이 되는 주요 핵심 인물을 찾았다.\n여기서는 몇 가지 추가적인 접근법을 살펴보자.\nEnd of explanation\n\"\"\"\n\n\nfriendships = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4),\n               (4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]\n\n\"\"\"\nExplanation: 네트워크는 사용자와 친구 관계를 나타낸다.\nEnd of explanation\n\"\"\"\n\n\n# give each user a friends list\nfor user in users:\n    user[\"friends\"] = []\n    \n# and populate it\nfor i, j in friendships:\n    # this works because users[i] is the user whose id is i\n    users[i][\"friends\"].append(users[j]) # add i as a friend of j\n    users[j][\"friends\"].append(users[i]) # add j as a friend of i   \n\n\"\"\"\nExplanation: 친구 목록을 각 사용자의 dict에 추가하기도 했다.\nEnd of explanation\n\"\"\"\n\n\n# \n# Betweenness Centrality\n#\n\ndef shortest_paths_from(from_user):\n    \n    # 특정 사용자로부터 다른 사용자까지의 모든 최단 경로를 포함하는 dict\n    shortest_paths_to = { from_user[\"id\"] : [[]] }\n\n    # 확인해야 하는 (이전 사용자, 다음 사용자) 큐\n    # 모든 (from_user, from_user의 친구) 쌍으로 시작\n    frontier = deque((from_user, friend)\n                     for friend in from_user[\"friends\"])\n\n    # 큐가 빌 때까지 반복\n    while frontier: \n\n        prev_user, user = frontier.popleft() # 큐의 첫 번째 사용자를\n        user_id = user[\"id\"] # 제거\n\n        # 큐에 사용자를 추가하는 방법을 고려해 보면\n        # prev_user까지의 최단 경로를 이미 알고 있을 수도 있다.\n        paths_to_prev = shortest_paths_to[prev_user[\"id\"]]\n        paths_via_prev = [path + [user_id] for path in paths_to_prev]\n        \n        # 만약 최단 경로를 이미 알고 있다면\n        old_paths_to_here = shortest_paths_to.get(user_id, [])\n        \n        # 지금까지의 최단 경로는 무엇일까?\n        if old_paths_to_here:\n            min_path_length = len(old_paths_to_here[0])\n        else:\n            min_path_length = float('inf')\n                \n        # 길지 않은 새로운 경로만 저장\n        new_paths_to_here = [path_via_prev\n                             for path_via_prev in paths_via_prev\n                             if len(path_via_prev) <= min_path_length\n                             and path_via_prev not in old_paths_to_here]\n        \n        shortest_paths_to[user_id] = old_paths_to_here + new_paths_to_here\n        \n        # 아직 한번도 보지 못한 이웃을 frontier에 추가\n        frontier.extend((user, friend)\n                        for friend in user[\"friends\"]\n                        if friend[\"id\"] not in shortest_paths_to)\n\n    return shortest_paths_to\n\n\"\"\"\nExplanation: 1장에서 연결 중심성(degree centrality)을 살펴볼 때는, 우리가 직관적으로 생각했던 주요 연결고리들이 선정되지 않아 약간 아쉬웠다.\n대안으로 사용할 수 있는 지수 중 하나는 매개 중심성(betweenness centrality)인데, 이는 두 사람 사이의 최단 경로상에 빈번하게 등장하는 사람들이 큰 값을 가지는 지수이다.\n구체적으로는, 노드 $i$의 매개 중심성은 다른 모든 노드 $j,k$ 쌍의 최단 경로 중에, $i$를 거치는 경로의 비율로 계산한다.\n임의의 두 사람이 주어졌을 때 그들 간의 최단 경로를 구해야 한다.\n이 책에서는 덜 효율적이더라도 훨씬 이해하기 쉬운 'Breadth-first search'라고도 알려진 알고리즘을 사용한다.\nEnd of explanation\n\"\"\"\n\n\nfor user in users:\n    user[\"shortest_paths\"] = shortest_paths_from(user)\n\n\"\"\"\nExplanation: 그리고 각 노드에 대해 생성된 dict들을 저장하자.\nEnd of explanation\n\"\"\"\n\n\nfor user in users:\n    user[\"betweenness_centrality\"] = 0.0\n\nfor source in users:\n    source_id = source[\"id\"]\n    for target_id, paths in source[\"shortest_paths\"].items(): # python2에서는 items 대신 iteritems 사용\n        if source_id < target_id:   # 잘못해서 두 번 세지 않도록 주의하자\n            num_paths = len(paths)  # 최단 경로가 몇 개 존재하는가?\n            contrib = 1 / num_paths # 중심성에 기여하는 값\n            for path in paths:\n                for id in path:\n                    if id not in [source_id, target_id]:\n                        users[id][\"betweenness_centrality\"] += contrib\n\nfor user in users:\n    print(user[\"id\"], user[\"betweenness_centrality\"])\n\n\"\"\"\nExplanation: 그러면 이제 매개 중심성을 구할 준비가 다 되었다.\n이제 각각의 최단 경로에 포함되는 각 노드의 매개 중심성에 $1/n$을 더해 주자.\nEnd of explanation\n\"\"\"\n\n\n#\n# closeness centrality\n#\n\ndef farness(user):\n    \"\"\"모든 사용자와의 최단 거리 합\"\"\"\n    return sum(len(paths[0]) \n               for paths in user[\"shortest_paths\"].values())\n\n\"\"\"\nExplanation: 사용자 0과 9의 최단 경로 사이에는 다른 사용자가 없으므로 매개 중심성이 0이다.\n반면 사용자 3, 4, 5는 최단 경로상에 무척 빈번하게 위치하기 때문에 높은 매개 중심성을 가진다.\n\n대게 중심성의 절댓값 자체는 큰 의미를 가지지 않고, 상대값만이 의미를 가진다.\n\n그 외에 살펴볼 수 있는 중심성 지표 중 하나는 근접 중심성(closeness centrality)이다.\n먼저 각 사용자의 원접성(farness)을 계산한다. 원접성이란 from_user와 다른 모든 사용자의 최단 경로를 합한 값이다.\nEnd of explanation\n\"\"\"\n\n\nfor user in users:\n    user[\"closeness_centrality\"] = 1 / farness(user)\n\nfor user in users:\n    print(user[\"id\"], user[\"closeness_centrality\"])\n\n\"\"\"\nExplanation: 이제 근접 중심성은 간단히 계산할 수 있다.\nEnd of explanation\n\"\"\"\n\n\ndef matrix_product_entry(A, B, i, j):\n    return dot(get_row(A, i), get_column(B, j))\n\ndef matrix_multiply(A, B):\n    n1, k1 = shape(A)\n    n2, k2 = shape(B)\n    if k1 != n2:\n        raise ArithmeticError(\"incompatible shapes!\")\n                \n    return make_matrix(n1, k2, partial(matrix_product_entry, A, B))\n\ndef vector_as_matrix(v):\n    \"\"\"(list 형태의) 벡터 v를 n x 1 행렬로 변환\"\"\"\n    return [[v_i] for v_i in v]\n    \ndef vector_from_matrix(v_as_matrix):\n    \"\"\"n x 1 행렬을 리스트로 변환\"\"\"\n    return [row[0] for row in v_as_matrix]\n\ndef matrix_operate(A, v):\n    v_as_matrix = vector_as_matrix(v)\n    product = matrix_multiply(A, v_as_matrix)\n    return vector_from_matrix(product)\n\n\"\"\"\nExplanation: 계산된 근접 중심성의 편차는 더욱 작다. 네트워크 중심에 있는 노드조차 외곽에 위치한 노드들로부터 멀리 떨어져 있기 때문이다.\n여기서 봤듯이 최단 경로를 계산하는 것은 꽤나 복잡하다. 그렇기 때문에 큰 네트워크에서는 근접 중심성을 자주 사용하지 않는다.\n덜 직관적이지만 보통 더 쉽게 계산할 수 있는 고유벡터 중심성(eigenvector centrality)을 더 자주 사용한다.\n21.2 고유벡터 중심성\n고유벡터 중심성에 대해 알아보기 전에 먼저 고유벡터가 무엇인지 살펴봐야 하고, 고유벡터가 무엇인지 알기 위해서는 먼저 행렬 연산에 대해 알아봐야 한다.\n21.2.1 행렬 연산\nEnd of explanation\n\"\"\"\n\n\ndef find_eigenvector(A, tolerance=0.00001):\n    guess = [1 for __ in A]\n\n    while True:\n        result = matrix_operate(A, guess)\n        length = magnitude(result)\n        next_guess = scalar_multiply(1/length, result)\n        \n        if distance(guess, next_guess) < tolerance:\n            return next_guess, length # eigenvector, eigenvalue\n        \n        guess = next_guess\n\n\"\"\"\nExplanation: 행렬 A의 고유 벡터를 찾기 위해, 임의의 벡터 $v$를 골라 matrix_operate를 수행하고, 결과값의 크기가 1이 되게 재조정하는 과정을 반복 수행한다.\nEnd of explanation\n\"\"\"\n\n\nrotate = [[0, 1],\n          [-1, 0]]\n\n\"\"\"\nExplanation: 결과값으로 반환되는 guess를 matrix_operate를 통해 결과값의 크기가 1인 벡터로 재조정하면, 자기 자신이 반환된다. 즉, 여기서 guess는 고유벡터라는 것을 의미한다.\n모든 실수 행렬에 고유벡터와 고유값이 있는 것은 아니다. 예를 들어 시계 방향으로 90도 회전하는 연산을 하는 다음 행렬에는 곱했을 때 가지 자신이 되는 벡터는 영벡터밖에 없다.\nEnd of explanation\n\"\"\"\n\n\nflip = [[0, 1],\n        [1, 0]]\n\n\"\"\"\nExplanation: 이 행렬로 앞서 구현한 find_eignevector(rotate)를 수행하면, 영원히 끝나지 않을 것이다.\n한편, 고유벡터가 있는 행렬도 때로는 무한루프에 빠질 수 있다.\nEnd of explanation\n\"\"\"\n\n\n#\n# eigenvector centrality\n#\n\ndef entry_fn(i, j):\n    return 1 if (i, j) in friendships or (j, i) in friendships else 0\n\nn = len(users)\nadjacency_matrix = make_matrix(n, n, entry_fn)\n\nadjacency_matrix\n\n\"\"\"\nExplanation: 이 행렬은 모든 벡터 [x, y]를 [y, x]로 변환한다. 따라서 [1, 1]은 고유값이 1인 고유벡터가 된다.\n하지만 x, y값이 다른 임의의 벡터에서 출발해서 find_eigenvector를 수행하면 x, y값을 바꾸는 연산만 무한히 수행할 것이다.\n(NumPy같은 라이브러리에는 이런 케이스까지 다룰 수 있는 다양한 방법들이 구현되어 있다.)\n이런 사소한 문제에도 불구하고, 어쨌든 find_eigenvector가 결과값을 반환한다면, 그 결과값은 곧 고유벡터이다.\n21.2.2 중심성\n고유벡터가 데이터 네트워크를 이해하는데 어떻게 도움을 줄까?\n얘기를 하기 전에 먼저 네트워크를 인접행렬(adjacency matrix)의 형태로 나타내 보자. 이 행렬은 사용자 i와 사용자 j가 친구인 경우 (i, j)번째 항목에 1이 있고, 친구가 아닌 경우 0이 있는 행렬이다.\nEnd of explanation\n\"\"\"\n\n\neigenvector_centralities, _ = find_eigenvector(adjacency_matrix)\n\nfor user_id, centrality in enumerate(eigenvector_centralities):\n    print(user_id, centrality)\n\n\"\"\"\nExplanation: 각 사용자의 고유벡터 중심성이란 find_eigenvector로 찾은 사용자의 고유벡터가 된다.\nEnd of explanation\n\"\"\"\n\n\n#\n# directed graphs\n#\n\nendorsements = [(0, 1), (1, 0), (0, 2), (2, 0), (1, 2), (2, 1), (1, 3),\n                (2, 3), (3, 4), (5, 4), (5, 6), (7, 5), (6, 8), (8, 7), (8, 9)]\n\nfor user in users:\n    user[\"endorses\"] = []       # add one list to track outgoing endorsements\n    user[\"endorsed_by\"] = []    # and another to track endorsements\n    \nfor source_id, target_id in endorsements:\n    users[source_id][\"endorses\"].append(users[target_id])\n    users[target_id][\"endorsed_by\"].append(users[source_id])\n\n\"\"\"\nExplanation: 연결의 수가 많고, 중심성이 높은 사용자들한테 연결된 사용자들은 고유벡터 중심성이 높다.\n앞의 결과에 따르면 사용자 1, 사용자 2의 중심성이 가장 높은데, 이는 중심성이 높은 사람들과 세번이나 연결되었기 때문이다.\n이들로부터 멀어질수록 사용자들의 중심성은 점차 줄어든다.\n21.3 방향성 그래프(Directed graphs)와 페이지랭크\n데이텀이 인기를 별로 끌지 못하자, 순이익 팀의 부사장은 친구 모델에서 보증(endorsement)모델로 전향하는 것을 고려 중이다.\n알고 보니 사람들은 어떤 데이터 과학자들끼리 친구인지에 대해서는 별로 관심이 없었지만, 헤드헌터들은 다른 데이터 과학자로부터 존경 받는 데이터 과학자가 누구인지에 대해 관심이 많다.\n이 새로운 모델에서 관계는 상호적인 것이 아니라, 한 사람(source)이 다른 멋진 한 사람(target)의 실력에 보증을 서주는 (source, target) 쌍으로 비대칭적인 관계를 표현하게 된다.\nEnd of explanation\n\"\"\"\n\n\nendorsements_by_id = [(user[\"id\"], len(user[\"endorsed_by\"]))\n                      for user in users]\n\nsorted(endorsements_by_id, \n       key=lambda x: x[1], # (user_id, num_endorsements)\n       reverse=True)\n\n\"\"\"\nExplanation: 그리고 가장 보증을 많이 받은 데이터 과학자들의 데이터를 수집해서, 그것을 헤드헌터들한테 팔면 된다.\nEnd of explanation\n\"\"\"\n\n\ndef page_rank(users, damping = 0.85, num_iters = 100):\n    \n    # 먼저 페이지랭크를 모든 노드에 고르게 배당\n    num_users = len(users)\n    pr = { user[\"id\"] : 1 / num_users for user in users }\n\n    # 매 스텝마다 각 노드가 받는\n    # 적은 양의 페이지랭크\n    base_pr = (1 - damping) / num_users\n    \n    for __ in range(num_iters):\n        next_pr = { user[\"id\"] : base_pr for user in users }\n        for user in users:\n            # 페이지랭크를 외부로 향하는 링크에 배당한다.\n            links_pr = pr[user[\"id\"]] * damping\n            for endorsee in user[\"endorses\"]:\n                next_pr[endorsee[\"id\"]] += links_pr / len(user[\"endorses\"])\n\n        pr = next_pr\n        \n    return pr\n\nfor user_id, pr in page_rank(users).items():\n    print(user_id, pr)\n\n\"\"\"\nExplanation: 사실 '보증의 수'와 같은 숫자는 조작하기가 매우 쉽다.\n가장 간단한 방법 중 하나는, 가짜 계정을 여러 개 만들어서 그것들로 내 계정에 대한 보증을 서는 것이다.\n또 다른 방법은, 친구들끼리 짜고 서로가 서로를 보증해 주는 것이다. (아마 사용자 0, 1, 2가 이런 관계일 가능성이 크다.)\n좀 더 나은 지수는, '누가' 보증을 서는지를 고려하는 것이다.\n보증을 많이 받은 사용자가 보증을 설 때는, 보증을 적게 받은 사용자가 보증을 설 때보다 더 중요한 것으로 받아들여지는 것이 타당하다.\n그리고 사실 이것은 유명한 페이지랭크(PageRank) 알고리즘의 기본 철학이기도 하다.\n1. 네트워크 전체에는 1.0(또는 100%)의 페이지랭크가 있다.\n2. 초기에 이 페이지랭크를 모든 노드에 고르게 배당한다.\n3. 각 스텝을 거칠 때마다 각 노드에 배당된 페이지랭크의 대부분은 외부로 향하는 링크에 균등하게 배당한다.\n4. 각 스텝을 거칠 때마다 각 노드에 남아 있는 페이지랭크를 모든 노드에 고르게 배당한다.\nEnd of explanation\n\"\"\"\n\nversion = '2020-08-25'\n\n\"\"\"\nExplanation: <div style=\"width:100%; background-color: #D9EDF7; border: 1px solid #CFCFCF; text-align: left; padding: 10px;\">\n      <b>Renewable power plants: Download and process notebook</b>\n      <ul>\n        <li><a href=\"main.ipynb\">Main notebook</a></li>\n        <li>Download and process notebook</li>\n        <li><a href=\"validation_and_output.ipynb\">Validation and output notebook</a></li>\n      </ul>\n      <br>This notebook is part of the <a href=\"http://data.open-power-system-data.org/renewable_power_plants\"> Renewable power plants Data Package</a> of <a href=\"http://open-power-system-data.org\">Open Power System Data</a>.\n</div>\n\nThis script downlads and extracts the original data of renewable power plant lists from the data sources, processes and merges them. It subsequently adds the geolocation for each power plant. Finally it saves the DataFrames as pickle-files. Make sure you run the download and process Notebook before the validation and output Notebook.\n<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Script-setup\" data-toc-modified-id=\"Script-setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Script setup</a></span></li><li><span><a href=\"#Settings\" data-toc-modified-id=\"Settings-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Settings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Choose-download-option\" data-toc-modified-id=\"Choose-download-option-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Choose download option</a></span></li><li><span><a href=\"#Update-the-download-links\" data-toc-modified-id=\"Update-the-download-links-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Update the download links</a></span></li><li><span><a href=\"#Set-up-the-downloader-for-data-sources\" data-toc-modified-id=\"Set-up-the-downloader-for-data-sources-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Set up the downloader for data sources</a></span></li><li><span><a href=\"#Set-up-the-NUTS-converter\" data-toc-modified-id=\"Set-up-the-NUTS-converter-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Set up the NUTS converter</a></span></li><li><span><a href=\"#Setup-translation-dictionaries\" data-toc-modified-id=\"Setup-translation-dictionaries-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Setup translation dictionaries</a></span></li></ul></li><li><span><a href=\"#Download-and-process-per-country\" data-toc-modified-id=\"Download-and-process-per-country-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Download and process per country</a></span><ul class=\"toc-item\"><li><span><a href=\"#Germany-DE\" data-toc-modified-id=\"Germany-DE-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Germany DE</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-read\" data-toc-modified-id=\"Download-and-read-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Download and read</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Add-information-and-choose-columns\" data-toc-modified-id=\"Add-information-and-choose-columns-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Add information and choose columns</a></span></li><li><span><a href=\"#Merge-DataFrames\" data-toc-modified-id=\"Merge-DataFrames-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>Merge DataFrames</a></span></li><li><span><a href=\"#Translate-values-and-harmonize-energy-source-level-2\" data-toc-modified-id=\"Translate-values-and-harmonize-energy-source-level-2-3.1.5\"><span class=\"toc-item-num\">3.1.5&nbsp;&nbsp;</span>Translate values and harmonize energy source level 2</a></span></li><li><span><a href=\"#Transform-electrical-capacity-from-kW-to-MW\" data-toc-modified-id=\"Transform-electrical-capacity-from-kW-to-MW-3.1.6\"><span class=\"toc-item-num\">3.1.6&nbsp;&nbsp;</span>Transform electrical capacity from kW to MW</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.1.7\"><span class=\"toc-item-num\">3.1.7&nbsp;&nbsp;</span>Georeferencing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-coordinates-by-postcode\" data-toc-modified-id=\"Get-coordinates-by-postcode-3.1.7.1\"><span class=\"toc-item-num\">3.1.7.1&nbsp;&nbsp;</span>Get coordinates by postcode</a></span></li><li><span><a href=\"#Transform-geoinformation\" data-toc-modified-id=\"Transform-geoinformation-3.1.7.2\"><span class=\"toc-item-num\">3.1.7.2&nbsp;&nbsp;</span>Transform geoinformation</a></span></li></ul></li><li><span><a href=\"#Clean-data\" data-toc-modified-id=\"Clean-data-3.1.8\"><span class=\"toc-item-num\">3.1.8&nbsp;&nbsp;</span>Clean data</a></span></li><li><span><a href=\"#Assign-NUTS-codes\" data-toc-modified-id=\"Assign-NUTS-codes-3.1.9\"><span class=\"toc-item-num\">3.1.9&nbsp;&nbsp;</span>Assign NUTS codes</a></span></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-3.1.10\"><span class=\"toc-item-num\">3.1.10&nbsp;&nbsp;</span>Visualize</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.1.11\"><span class=\"toc-item-num\">3.1.11&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#Denmark-DK\" data-toc-modified-id=\"Denmark-DK-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Denmark DK</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-read\" data-toc-modified-id=\"Download-and-read-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Download and read</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Add-data-source-and-missing-information\" data-toc-modified-id=\"Add-data-source-and-missing-information-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Add data source and missing information</a></span></li><li><span><a href=\"#Correct-the-dates\" data-toc-modified-id=\"Correct-the-dates-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Correct the dates</a></span></li><li><span><a href=\"#Translate-values-and-harmonize-energy-source-level-2\" data-toc-modified-id=\"Translate-values-and-harmonize-energy-source-level-2-3.2.5\"><span class=\"toc-item-num\">3.2.5&nbsp;&nbsp;</span>Translate values and harmonize energy source level 2</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.2.6\"><span class=\"toc-item-num\">3.2.6&nbsp;&nbsp;</span>Georeferencing</a></span></li><li><span><a href=\"#Merge-DataFrames,-add-NUTS-information-and-choose-columns\" data-toc-modified-id=\"Merge-DataFrames,-add-NUTS-information-and-choose-columns-3.2.7\"><span class=\"toc-item-num\">3.2.7&nbsp;&nbsp;</span>Merge DataFrames, add NUTS information and choose columns</a></span></li><li><span><a href=\"#Select-columns\" data-toc-modified-id=\"Select-columns-3.2.8\"><span class=\"toc-item-num\">3.2.8&nbsp;&nbsp;</span>Select columns</a></span></li><li><span><a href=\"#Remove-duplicate-rows\" data-toc-modified-id=\"Remove-duplicate-rows-3.2.9\"><span class=\"toc-item-num\">3.2.9&nbsp;&nbsp;</span>Remove duplicate rows</a></span></li><li><span><a href=\"#Transform-electrical_capacity-from-kW-to-MW\" data-toc-modified-id=\"Transform-electrical_capacity-from-kW-to-MW-3.2.10\"><span class=\"toc-item-num\">3.2.10&nbsp;&nbsp;</span>Transform electrical_capacity from kW to MW</a></span></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-3.2.11\"><span class=\"toc-item-num\">3.2.11&nbsp;&nbsp;</span>Visualize</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.2.12\"><span class=\"toc-item-num\">3.2.12&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#France-FR\" data-toc-modified-id=\"France-FR-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>France FR</a></span><ul class=\"toc-item\"><li><span><a href=\"#ODRE-data\" data-toc-modified-id=\"ODRE-data-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>ODRE data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-the-data\" data-toc-modified-id=\"Load-the-data-3.3.1.1\"><span class=\"toc-item-num\">3.3.1.1&nbsp;&nbsp;</span>Load the data</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.3.1.2\"><span class=\"toc-item-num\">3.3.1.2&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Add-data-source\" data-toc-modified-id=\"Add-data-source-3.3.1.3\"><span class=\"toc-item-num\">3.3.1.3&nbsp;&nbsp;</span>Add data source</a></span></li><li><span><a href=\"#Translate-values\" data-toc-modified-id=\"Translate-values-3.3.1.4\"><span class=\"toc-item-num\">3.3.1.4&nbsp;&nbsp;</span>Translate values</a></span></li><li><span><a href=\"#Correct-site-names\" data-toc-modified-id=\"Correct-site-names-3.3.1.5\"><span class=\"toc-item-num\">3.3.1.5&nbsp;&nbsp;</span>Correct site names</a></span></li><li><span><a href=\"#Replace-suspicious-dates-with-N/A\" data-toc-modified-id=\"Replace-suspicious-dates-with-N/A-3.3.1.6\"><span class=\"toc-item-num\">3.3.1.6&nbsp;&nbsp;</span>Replace suspicious dates with N/A</a></span></li><li><span><a href=\"#Check-missing-values\" data-toc-modified-id=\"Check-missing-values-3.3.1.7\"><span class=\"toc-item-num\">3.3.1.7&nbsp;&nbsp;</span>Check missing values</a></span></li><li><span><a href=\"#Standardize-the-energy-types-and-technologies\" data-toc-modified-id=\"Standardize-the-energy-types-and-technologies-3.3.1.8\"><span class=\"toc-item-num\">3.3.1.8&nbsp;&nbsp;</span>Standardize the energy types and technologies</a></span></li><li><span><a href=\"#Standardize-source-levels-1-3-and-technology\" data-toc-modified-id=\"Standardize-source-levels-1-3-and-technology-3.3.1.9\"><span class=\"toc-item-num\">3.3.1.9&nbsp;&nbsp;</span>Standardize source levels 1-3 and technology</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.3.1.10\"><span class=\"toc-item-num\">3.3.1.10&nbsp;&nbsp;</span>Georeferencing</a></span></li><li><span><a href=\"#Convert-electrical-capacity-to-MW\" data-toc-modified-id=\"Convert-electrical-capacity-to-MW-3.3.1.11\"><span class=\"toc-item-num\">3.3.1.11&nbsp;&nbsp;</span>Convert electrical capacity to MW</a></span></li></ul></li><li><span><a href=\"#Old-data\" data-toc-modified-id=\"Old-data-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Old data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Add-data-source\" data-toc-modified-id=\"Add-data-source-3.3.2.1\"><span class=\"toc-item-num\">3.3.2.1&nbsp;&nbsp;</span>Add data source</a></span></li><li><span><a href=\"#Translate-values-and-harmonize-energy-source-level-2\" data-toc-modified-id=\"Translate-values-and-harmonize-energy-source-level-2-3.3.2.2\"><span class=\"toc-item-num\">3.3.2.2&nbsp;&nbsp;</span>Translate values and harmonize energy source level 2</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.3.2.3\"><span class=\"toc-item-num\">3.3.2.3&nbsp;&nbsp;</span>Georeferencing</a></span></li></ul></li><li><span><a href=\"#Integrate-old-and-new-data\" data-toc-modified-id=\"Integrate-old-and-new-data-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Integrate old and new data</a></span></li><li><span><a href=\"#Select-the-columns\" data-toc-modified-id=\"Select-the-columns-3.3.4\"><span class=\"toc-item-num\">3.3.4&nbsp;&nbsp;</span>Select the columns</a></span></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-3.3.5\"><span class=\"toc-item-num\">3.3.5&nbsp;&nbsp;</span>Visualize</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.3.6\"><span class=\"toc-item-num\">3.3.6&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#Poland-PL\" data-toc-modified-id=\"Poland-PL-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Poland PL</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download\" data-toc-modified-id=\"Download-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>Download</a></span></li><li><span><a href=\"#Load-and-explore-the-data\" data-toc-modified-id=\"Load-and-explore-the-data-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>Load and explore the data</a></span></li><li><span><a href=\"#Inspect-the-data\" data-toc-modified-id=\"Inspect-the-data-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>Inspect the data</a></span></li><li><span><a href=\"#Harmonising-energy-levels\" data-toc-modified-id=\"Harmonising-energy-levels-3.4.4\"><span class=\"toc-item-num\">3.4.4&nbsp;&nbsp;</span>Harmonising energy levels</a></span></li><li><span><a href=\"#Georeferencing-(NUTS-classification)\" data-toc-modified-id=\"Georeferencing-(NUTS-classification)-3.4.5\"><span class=\"toc-item-num\">3.4.5&nbsp;&nbsp;</span>Georeferencing (NUTS classification)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Add-NUTS-information\" data-toc-modified-id=\"Add-NUTS-information-3.4.5.1\"><span class=\"toc-item-num\">3.4.5.1&nbsp;&nbsp;</span>Add NUTS information</a></span></li></ul></li><li><span><a href=\"#Add-data-source-and-year\" data-toc-modified-id=\"Add-data-source-and-year-3.4.6\"><span class=\"toc-item-num\">3.4.6&nbsp;&nbsp;</span>Add data source and year</a></span></li><li><span><a href=\"#Select-columns\" data-toc-modified-id=\"Select-columns-3.4.7\"><span class=\"toc-item-num\">3.4.7&nbsp;&nbsp;</span>Select columns</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.4.8\"><span class=\"toc-item-num\">3.4.8&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#Switzerland-CH\" data-toc-modified-id=\"Switzerland-CH-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Switzerland CH</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-read\" data-toc-modified-id=\"Download-and-read-3.5.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Download and read</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.5.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Add-data-source\" data-toc-modified-id=\"Add-data-source-3.5.3\"><span class=\"toc-item-num\">3.5.3&nbsp;&nbsp;</span>Add data source</a></span></li><li><span><a href=\"#Harmonize-energy-source-hierarchy-and-translate-values\" data-toc-modified-id=\"Harmonize-energy-source-hierarchy-and-translate-values-3.5.4\"><span class=\"toc-item-num\">3.5.4&nbsp;&nbsp;</span>Harmonize energy source hierarchy and translate values</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.5.5\"><span class=\"toc-item-num\">3.5.5&nbsp;&nbsp;</span>Georeferencing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Postcode-to-lat/lon-(WGS84)\" data-toc-modified-id=\"Postcode-to-lat/lon-(WGS84)-3.5.5.1\"><span class=\"toc-item-num\">3.5.5.1&nbsp;&nbsp;</span>Postcode to lat/lon (WGS84)</a></span></li><li><span><a href=\"#Add-NUTS-information\" data-toc-modified-id=\"Add-NUTS-information-3.5.5.2\"><span class=\"toc-item-num\">3.5.5.2&nbsp;&nbsp;</span>Add NUTS information</a></span></li></ul></li><li><span><a href=\"#Transform-electrical_capacity-from-kW-to-MW\" data-toc-modified-id=\"Transform-electrical_capacity-from-kW-to-MW-3.5.6\"><span class=\"toc-item-num\">3.5.6&nbsp;&nbsp;</span>Transform electrical_capacity from kW to MW</a></span></li><li><span><a href=\"#Select-columns-to-keep\" data-toc-modified-id=\"Select-columns-to-keep-3.5.7\"><span class=\"toc-item-num\">3.5.7&nbsp;&nbsp;</span>Select columns to keep</a></span></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-3.5.8\"><span class=\"toc-item-num\">3.5.8&nbsp;&nbsp;</span>Visualize</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.5.9\"><span class=\"toc-item-num\">3.5.9&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#United-Kingdom-UK\" data-toc-modified-id=\"United-Kingdom-UK-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>United Kingdom UK</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-Read\" data-toc-modified-id=\"Download-and-Read-3.6.1\"><span class=\"toc-item-num\">3.6.1&nbsp;&nbsp;</span>Download and Read</a></span></li><li><span><a href=\"#Clean-the-data\" data-toc-modified-id=\"Clean-the-data-3.6.2\"><span class=\"toc-item-num\">3.6.2&nbsp;&nbsp;</span>Clean the data</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.6.3\"><span class=\"toc-item-num\">3.6.3&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Add-data-source\" data-toc-modified-id=\"Add-data-source-3.6.4\"><span class=\"toc-item-num\">3.6.4&nbsp;&nbsp;</span>Add data source</a></span></li><li><span><a href=\"#Translate-values-and-harmonise-energy-source-levels-1-3-and-technology\" data-toc-modified-id=\"Translate-values-and-harmonise-energy-source-levels-1-3-and-technology-3.6.5\"><span class=\"toc-item-num\">3.6.5&nbsp;&nbsp;</span>Translate values and harmonise energy source levels 1-3 and technology</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.6.6\"><span class=\"toc-item-num\">3.6.6&nbsp;&nbsp;</span>Georeferencing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cases-with-unknown-Easting-and-Northing-coordinates\" data-toc-modified-id=\"Cases-with-unknown-Easting-and-Northing-coordinates-3.6.6.1\"><span class=\"toc-item-num\">3.6.6.1&nbsp;&nbsp;</span>Cases with unknown Easting and Northing coordinates</a></span></li><li><span><a href=\"#Cases-for-approximation\" data-toc-modified-id=\"Cases-for-approximation-3.6.6.2\"><span class=\"toc-item-num\">3.6.6.2&nbsp;&nbsp;</span>Cases for approximation</a></span></li><li><span><a href=\"#Add-NUTS-information\" data-toc-modified-id=\"Add-NUTS-information-3.6.6.3\"><span class=\"toc-item-num\">3.6.6.3&nbsp;&nbsp;</span>Add NUTS information</a></span></li><li><span><a href=\"#Visualize-the-data\" data-toc-modified-id=\"Visualize-the-data-3.6.6.4\"><span class=\"toc-item-num\">3.6.6.4&nbsp;&nbsp;</span>Visualize the data</a></span></li></ul></li><li><span><a href=\"#Keep-only-the-columns-of-interest\" data-toc-modified-id=\"Keep-only-the-columns-of-interest-3.6.7\"><span class=\"toc-item-num\">3.6.7&nbsp;&nbsp;</span>Keep only the columns of interest</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.6.8\"><span class=\"toc-item-num\">3.6.8&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#Sweden\" data-toc-modified-id=\"Sweden-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Sweden</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-the-data\" data-toc-modified-id=\"Load-the-data-3.7.1\"><span class=\"toc-item-num\">3.7.1&nbsp;&nbsp;</span>Load the data</a></span></li><li><span><a href=\"#Clean-the-data\" data-toc-modified-id=\"Clean-the-data-3.7.2\"><span class=\"toc-item-num\">3.7.2&nbsp;&nbsp;</span>Clean the data</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.7.3\"><span class=\"toc-item-num\">3.7.3&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Correct-the-dates\" data-toc-modified-id=\"Correct-the-dates-3.7.4\"><span class=\"toc-item-num\">3.7.4&nbsp;&nbsp;</span>Correct the dates</a></span></li><li><span><a href=\"#Add-source\" data-toc-modified-id=\"Add-source-3.7.5\"><span class=\"toc-item-num\">3.7.5&nbsp;&nbsp;</span>Add source</a></span></li><li><span><a href=\"#Translate-values-and-harmonize-energy-source-levels\" data-toc-modified-id=\"Translate-values-and-harmonize-energy-source-levels-3.7.6\"><span class=\"toc-item-num\">3.7.6&nbsp;&nbsp;</span>Translate values and harmonize energy source levels</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.7.7\"><span class=\"toc-item-num\">3.7.7&nbsp;&nbsp;</span>Georeferencing</a></span></li><li><span><a href=\"#Assigning-NUTS-codes\" data-toc-modified-id=\"Assigning-NUTS-codes-3.7.8\"><span class=\"toc-item-num\">3.7.8&nbsp;&nbsp;</span>Assigning NUTS codes</a></span></li><li><span><a href=\"#Select-the-columns-to-keep\" data-toc-modified-id=\"Select-the-columns-to-keep-3.7.9\"><span class=\"toc-item-num\">3.7.9&nbsp;&nbsp;</span>Select the columns to keep</a></span></li><li><span><a href=\"#Visualize\" data-toc-modified-id=\"Visualize-3.7.10\"><span class=\"toc-item-num\">3.7.10&nbsp;&nbsp;</span>Visualize</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.7.11\"><span class=\"toc-item-num\">3.7.11&nbsp;&nbsp;</span>Save</a></span></li></ul></li><li><span><a href=\"#Czech-Republic\" data-toc-modified-id=\"Czech-Republic-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Czech Republic</a></span><ul class=\"toc-item\"><li><span><a href=\"#Download-and-read-the-data\" data-toc-modified-id=\"Download-and-read-the-data-3.8.1\"><span class=\"toc-item-num\">3.8.1&nbsp;&nbsp;</span>Download and read the data</a></span></li><li><span><a href=\"#Clean-the-data\" data-toc-modified-id=\"Clean-the-data-3.8.2\"><span class=\"toc-item-num\">3.8.2&nbsp;&nbsp;</span>Clean the data</a></span></li><li><span><a href=\"#Reformat-the-data\" data-toc-modified-id=\"Reformat-the-data-3.8.3\"><span class=\"toc-item-num\">3.8.3&nbsp;&nbsp;</span>Reformat the data</a></span></li><li><span><a href=\"#Translate-column-names\" data-toc-modified-id=\"Translate-column-names-3.8.4\"><span class=\"toc-item-num\">3.8.4&nbsp;&nbsp;</span>Translate column names</a></span></li><li><span><a href=\"#Translate-values-and-harmonize-energy-levels\" data-toc-modified-id=\"Translate-values-and-harmonize-energy-levels-3.8.5\"><span class=\"toc-item-num\">3.8.5&nbsp;&nbsp;</span>Translate values and harmonize energy levels</a></span></li><li><span><a href=\"#Add-data-source\" data-toc-modified-id=\"Add-data-source-3.8.6\"><span class=\"toc-item-num\">3.8.6&nbsp;&nbsp;</span>Add data source</a></span></li><li><span><a href=\"#Georeferencing\" data-toc-modified-id=\"Georeferencing-3.8.7\"><span class=\"toc-item-num\">3.8.7&nbsp;&nbsp;</span>Georeferencing</a></span></li><li><span><a href=\"#Assign-NUTS-codes\" data-toc-modified-id=\"Assign-NUTS-codes-3.8.8\"><span class=\"toc-item-num\">3.8.8&nbsp;&nbsp;</span>Assign NUTS codes</a></span></li><li><span><a href=\"#Select-the-columns-to-keep\" data-toc-modified-id=\"Select-the-columns-to-keep-3.8.9\"><span class=\"toc-item-num\">3.8.9&nbsp;&nbsp;</span>Select the columns to keep</a></span></li><li><span><a href=\"#Drop-duplicates\" data-toc-modified-id=\"Drop-duplicates-3.8.10\"><span class=\"toc-item-num\">3.8.10&nbsp;&nbsp;</span>Drop duplicates</a></span></li><li><span><a href=\"#Visualuze\" data-toc-modified-id=\"Visualuze-3.8.11\"><span class=\"toc-item-num\">3.8.11&nbsp;&nbsp;</span>Visualuze</a></span></li><li><span><a href=\"#Save\" data-toc-modified-id=\"Save-3.8.12\"><span class=\"toc-item-num\">3.8.12&nbsp;&nbsp;</span>Save</a></span></li></ul></li></ul></li><li><span><a href=\"#Zip-the-raw-data\" data-toc-modified-id=\"Zip-the-raw-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Zip the raw data</a></span></li></ul></div>\nEnd of explanation\n\"\"\"\n\n\nimport logging\nimport os\nimport posixpath\nimport urllib.parse\nimport urllib.request\nimport re\nimport zipfile\nimport pickle\nimport urllib\nimport shutil\nimport datetime\n\nimport numpy as np\nimport pandas as pd\nimport utm  # for transforming geoinformation in the utm format\nimport requests\nimport fake_useragent\nfrom string import Template\nfrom IPython.display import display\nimport xlrd\nimport bs4\nimport bng_to_latlon\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n\n# for visualizing locations on maps\nimport cartopy.crs as ccrs \nimport cartopy.feature as cfeature\nfrom cartopy.io import shapereader\nimport geopandas\nimport shapely\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    datefmt='%d %b %Y %H:%M:%S'\n)\n\nlogger = logging.getLogger()\n\n# Create input, intermediate and output folders if they don't exist.\n# If the paths are relative, the correspoding folders will be created\n# inside the current working directory.\ninput_directory_path = os.path.join('input', 'original_data')\nintermediate_directory_path = 'intermediate'\noutput_directory_path = os.path.join('output', 'renewable_power_plants')\n\nos.makedirs(input_directory_path, exist_ok=True)\nos.makedirs(intermediate_directory_path, exist_ok=True)\nos.makedirs(output_directory_path, exist_ok=True)\n\n# Create the folder to which the Eurostat files with data at the level of the whole EU/Europe\n#are going to be downloaded\neurostat_eu_directory_path = os.path.join('input', 'eurostat_eu')\nos.makedirs(eurostat_eu_directory_path, exist_ok=True)\n\n# Define the path of the file with the list of sources.\nsource_list_filepath = os.path.join('input', 'sources.csv')\n\n# Import the utility functions and classes from the util package\nimport util.helper\nfrom util.visualizer import visualize_points\n\n\"\"\"\nExplanation: Script setup\nEnd of explanation\n\"\"\"\n\n\ndownload_from = 'original_sources'\n#download_from = 'opsd_server' \n\n\"\"\"\nExplanation: Settings\nChoose download option\nThe original data can either be downloaded from the original data sources as specified below or from the opsd-Server. Default option is to download from the original sources as the aim of the project is to stay as close to original sources as possible. However, if problems with downloads e.g. due to changing urls occur, you can still run the script with the original data from the opsd_server.\nEnd of explanation\n\"\"\"\n\n\nsource_df = pd.read_csv(source_list_filepath)\nuk_main_page = 'https://www.gov.uk/government/publications/renewable-energy-planning-database-monthly-extract'\ncurrent_link = util.helper.get_beis_link(uk_main_page)\ncurrent_filename = current_link.split('/')[-1]\n\nsource_df.loc[(source_df['country'] == 'UK') & (source_df['source'] == 'BEIS'), 'url'] = current_link\nsource_df.loc[(source_df['country'] == 'UK') & (source_df['source'] == 'BEIS'), 'filename'] = current_filename\nsource_df.to_csv(source_list_filepath, index=False, header=True)\n\nsource_df.fillna('')\n\n\"\"\"\nExplanation: Update the download links\nThe download link for the UK is updated at the end of each quarter by the source provider, BEIS. We keep up with those changes by extracting the download link automatically from the web page it is on. That way, the link does not have to be updated manually.\nNote: you must be connected to the Internet if you want to execute this step.\nEnd of explanation\n\"\"\"\n\n\nimport util.downloader\nfrom util.downloader import Downloader\ndownloader = Downloader(version, input_directory_path, source_list_filepath, download_from)\n\n\"\"\"\nExplanation: Note that, as of August 25, 2020, the following sources are available only from the OPSD server and the data will be downloaded from it even if download_from is set to 'original_sources':\n- Energinet (DK)\n- Eurostat files which contain correspondence tables between postal codes and NUTS.\nThe original links which should be downloaded from OPSD are marked as inactive in the column active in the above dataframe.\nSet up the downloader for data sources\nThe Downloader class in the util package is responsible for downloading the original files to appropriate folders. In order to access its functionality, we have to instantiate it first.\nEnd of explanation\n\"\"\"\n\n\n#import importlib\n#importlib.reload(util.nuts_converter)\n#importlib.reload(util.downloader)\n#from util.downloader import Downloader\n#downloader = Downloader(version, input_directory_path, source_list_filepath, download_from)\nfrom util.nuts_converter import NUTSConverter\nnuts_converter = NUTSConverter(downloader, eurostat_eu_directory_path)\n\n\"\"\"\nExplanation: Set up the NUTS converter\nThe NUTSConverter class in the util package uses the information on each facility's  postcode, municipalty name, municipality code, longitude, and latitude to assign it correct NUTS 2016 level 1, 2, and 3 codes.\nHere, we instantiate the converter so that we can use it later.\nEnd of explanation\n\"\"\"\n\n\n# Get column translation list\ncolumnnames = pd.read_csv(os.path.join('input', 'column_translation_list.csv'))\ncolumnnames.head(2)\n\n# Get value translation list\nvaluenames = pd.read_csv(os.path.join('input', 'value_translation_list.csv'))\nvaluenames.head(2)\n\n\"\"\"\nExplanation: Setup translation dictionaries\nColumn and value names of the original data sources will be translated to English and standardized across different sources. Standardized column names, e.g. \"electrical_capacity\" are required to merge data in one DataFrame.<br>\nThe column and the value translation lists are provided in the input folder of the Data Package.\nEnd of explanation\n\"\"\"\n\n\n# Define the lists of source names\ndownloader = Downloader(version, input_directory_path, source_list_filepath, download_from)\n\ntsos = ['50Hertz', 'Amprion', 'TenneT', 'TransnetBW']\ndatasets = ['50Hertz', 'Amprion', 'TenneT', 'TransnetBW','bnetza','bnetza_pv','bnetza_pv_historic']\n\n# Download the files and get the local file paths indexed by source names\nfilepaths = downloader.download_data_for_country('DE')\n\n# Remove the Eurostat NUTS file as it's a geoinformation source\nDE_postcode2nuts_filepath = filepaths.pop('Eurostat')\n\n# Open all data sets before processing.\nfilenames = {}\n\nfor source in filepaths:\n    filepath = filepaths[source]\n    print(source, filepath)\n    if os.path.splitext(filepath)[1] != '.xlsx' and zipfile.is_zipfile(filepath):\n        filenames[source] = zipfile.ZipFile(filepath)\n    else:\n        filenames[source] = filepath\n\n# Read TSO data from the zip files\ndfs = {}\n\nbasenames_by_tso = {\n    '50Hertz': '50Hertz Transmission GmbH EEG-Zahlungen Stammdaten 2019',\n    'Amprion': 'Amprion GmbH EEG-Zahlungen Anlagenstammdaten 2019',\n    'TenneT': 'TenneT TSO GmbH Anlagenstammdaten 2019',\n    'TransnetBW': 'TransnetBW GmbH Anlagenstammdaten 2019',\n}\n        \nfor tso in tsos:\n    filename = basenames_by_tso[tso]+'.csv'\n    print('Reading', filename)\n    #print(filenames[tso].namelist())\n    dfs[tso] = pd.read_csv(\n        filenames[tso].open(filename),\n        sep=';',\n        thousands='.',\n        decimal=',',\n\n        # Headers have to have the same order for all TSOs. Therefore just define headers here.\n        # Remove the following three lines if for next version, headers should be read out initially \n        # to then check if order is the same everywhere.\n        names=['EEG-Anlagenschlüssel', 'MASTR_Nr_EEG','Netzbetreiber Betriebsnummer','Netzbetreiber Name',\n               'Strasse_flurstueck','PLZ','Ort / Gemarkung','Gemeindeschlüssel','Bundesland',\n               'Installierte Leistung','Energieträger','Spannungsebene','Leistungsmessung','Regelbarkeit',\n               'Inbetriebnahme','Außerbetriebnahme','Netzzugang','Netzabgang'],\n        header=None,\n        skiprows=1,\n        parse_dates=[14, 15, 16, 17], #[11, 12, 13, 14]\n        #infer_datetime_format=True,\n        date_parser = lambda x: pd.to_datetime(x, errors='coerce', format='%d.%m.%Y'),\n        encoding='iso-8859-1',\n        dayfirst=True,\n        low_memory=False\n    )\n    print('Done reading ' + filename)\n\nfor filename in filenames.values():\n    if(isinstance(filename, zipfile.ZipFile)):\n        #print(filename)\n        filename.close()\n\n# define the date parser\ndef date_parser(x):\n    if type(x) == str:\n        return datetime.datetime.strptime(x, '%D.%M.%Y')\n    elif type(x) == float and pd.isnull(x):\n        return pd.NaT\n    \ndef inspect(x):\n    try:\n        converted = datetime.datetime.strptime(x, '%d.%m.%Y')\n        return False\n    except:\n        return True\n\n# Read BNetzA register\nprint('Reading bnetza: '+filenames['bnetza'])\ndfs['bnetza'] = pd.read_excel(filenames['bnetza'],\n                          sheet_name='Gesamtübersicht',\n                          header=0,\n                          converters={'4.9 Postleit-zahl': str, 'Gemeinde-Schlüssel': str}\n)\n\nskiprows = {'bnetza_pv_historic': 10, 'bnetza_pv': 9}\n\nfor dataset in ['bnetza_pv', 'bnetza_pv_historic']:\n    print(dataset)\n    print('Reading ' + dataset + ': ' + filenames[dataset])\n    xls_handle = pd.ExcelFile(filenames[dataset])\n    print('Concatenating all '+dataset+' sheets into one dataframe')\n    dfs[dataset] = pd.concat(\n        (xls_handle.parse(\n            sheet,\n            skiprows=skiprows[dataset],\n            converters={'Anlage \\nPLZ': str}\n        ) for sheet in xls_handle.sheet_names),\n        sort=True\n    )\n\n# Make sure that the column `Inbetriebnahme-datum *)` (commissioning date) in the bnetza_pv set is datetime.\nmask = dfs['bnetza_pv']['Inbetriebnahme-datum *)'].apply(lambda x: type(x) == int)\n\ndfs['bnetza_pv']['Inbetriebnahme-datum *)'] = pd.to_datetime(dfs['bnetza_pv']['Inbetriebnahme-datum *)'],\n                                                             errors='coerce',\n                                                             dayfirst=True,\n                                                             infer_datetime_format=True)\ndfs['bnetza_pv']['Inbetriebnahme-datum *)'] = dfs['bnetza_pv']['Inbetriebnahme-datum *)'].apply(\n    lambda x: x.to_datetime64()\n)\n\ndfs['bnetza_pv_historic'] = dfs['bnetza_pv_historic'].drop(['Unnamed: 7'], axis=1)\n\npickle.dump( dfs, open( \"intermediate/temp_dfs_DE_after_reading.pickle\", \"wb\" ) )\n\ndfs = pickle.load( open( \"intermediate/temp_dfs_DE_after_reading.pickle\", \"rb\" ) )\n\n\"\"\"\nExplanation: Download and process per country\nFor one country after the other, the original data is downloaded, read, processed, translated, eventually georeferenced and saved. If respective files are already in the local folder, these will be utilized.\nTo process the provided data pandas DataFrame is applied.<br>\nGermany DE\nDownload and read\nThe data which will be processed below is provided by the following data sources:\nNetztransparenz.de - Official grid transparency platform from the German Transmission System Operators (TSOs): 50Hertz, Amprion, TenneT and TransnetBW.\nBundesnetzagentur (BNetzA) - German Federal Network Agency for Electricity, Gas, Telecommunications, Posts and Railway (In separate files for data for roof-mounted PV power plants and for all other renewable energy power plants.)\nData URL for BNetzA gets updated every few month. To be sure, always check if the links (url_bnetza; url_bnetza_pv) are up to date.\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for Germany, create dictionary and show dictionary\ncolumnnames = pd.read_csv(os.path.join('input', 'column_translation_list.csv'))\nidx_DE = columnnames[columnnames['country'] == 'DE'].index\ncolumn_dict_DE = columnnames.loc[idx_DE].set_index('original_name')['opsd_name'].to_dict()\ncolumn_dict_DE\n\n# Start the column translation process for each original data source\nprint('Translation...')\nfor dataset in dfs:\n    # Remove newlines and any other duplicate whitespaces in column names:\n    dfs[dataset] = dfs[dataset].rename(columns={col: re.sub(r\"\\s+\", ' ', col) for col in dfs[dataset].columns})\n    # Do column name translations\n    print(dataset)\n    #print(list(dfs[dataset].columns))\n    dfs[dataset].rename(columns=column_dict_DE, inplace=True)\n    #print(list(dfs[dataset].columns).index('decommissioning_date'))\n    #print('--------------------------------------------')\n\nprint('done.')\n\n\"\"\"\nExplanation: Translate column names\nTo standardise the DataFrame the original column names from the German TSOs and the BNetzA wil be translated and new English column names wil be assigned to the DataFrame. The unique column names are required to merge the DataFrame.<br>\nThe column_translation_list is provided here as csv in the input folder. It is loaded in 2.3 Setup of translation dictionaries.\nEnd of explanation\n\"\"\"\n\n\n# Add data source names to the DataFrames\nfor tso in tsos:\n    dfs[tso]['data_source'] = tso\n    dfs[tso]['tso'] = tso\n\ndfs['bnetza']['data_source'] = 'BNetzA'\ndfs['bnetza_pv']['data_source'] = 'BNetzA_PV'\ndfs['bnetza_pv_historic']['data_source'] = 'BNetzA_PV_historic'\n\n# Add for the BNetzA PV data the energy source level 2\ndfs['bnetza_pv']['energy_source_level_2'] = 'Photovoltaics'\ndfs['bnetza_pv_historic']['energy_source_level_2'] = 'Photovoltaics'\n\n# Select those columns of the original data which are utilised further\ndfs['bnetza'] = dfs['bnetza'].loc[:, ('commissioning_date', 'decommissioning_date',\n                              'notification_reason', 'energy_source_level_2',\n                              'electrical_capacity_kW', 'thermal_capacity_kW',\n                              'voltage_level', 'dso', 'eeg_id', 'bnetza_id',\n                              'federal_state', 'postcode', 'municipality_code',\n                              'municipality', 'address', 'address_number',\n                              'utm_zone', 'utm_east', 'utm_north',\n                              'data_source')]\n\nfor dataset in datasets: print(dataset+':'); display(dfs[dataset].tail(2))\n\n\"\"\"\nExplanation: Add information and choose columns\nAll data source names and for the BNetzA-PV data the energy source level 2 will added.\nEnd of explanation\n\"\"\"\n\n\n# Merge DataFrames of each original source into a common DataFrame DE_renewables\ndfs_list = []\n\nfor dataset in datasets:\n    dfs_list.append(dfs[dataset])\n\nDE_renewables = pd.concat(dfs_list, sort=True)\nDE_renewables.head(2)\n\nDE_renewables.reset_index(drop=True, inplace=True)\nDE_renewables.head(2)\n\n\"\"\"\nExplanation: Merge DataFrames\nThe individual DataFrames from the TSOs (Netztransparenz.de) and BNetzA are merged.\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for Germany, create dictionary and show dictionary\nvaluenames = pd.read_csv(os.path.join('input', 'value_translation_list.csv'))\nidx_DE = valuenames[valuenames['country'] == 'DE'].index\nvalue_dict_DE = valuenames.loc[idx_DE].set_index('original_name')['opsd_name'].to_dict()\nvalue_dict_DE\n\nprint('replacing...')\n# Replace all original value names by the OPSD value names. \n# Running time: some minutes.\nDE_renewables.replace(value_dict_DE, inplace=True)\nprint('Done!')\n\nDE_renewables['postcode'] = DE_renewables['postcode'].apply(pd.to_numeric, errors='ignore')\n\n\"\"\"\nExplanation: Translate values and harmonize energy source level 2\nDifferent German terms for energy source level 2, energy source level 3, technology and voltage levels are translated and harmonized across the individual data sources. The value_translation_list is provided here as csv in the input folder. It is loaded in 2.3 Setup of translation dictionaries.\nEnd of explanation\n\"\"\"\n\n\n# Create dictionary in order to assign energy_source to its subtype\nenergy_source_dict_DE = valuenames.loc[idx_DE].set_index(\n    'opsd_name')['energy_source_level_2'].to_dict()\n\n# Column energy_source partly contains energy source level 3 and technology information,\n# thus this column is copied to new column technology...\nDE_renewables['technology'] = DE_renewables['energy_source_level_2']\n\n# ...and the energy source level 2 values are replaced by the higher level classification\nDE_renewables['energy_source_level_2'].replace(energy_source_dict_DE, inplace=True)\n\n# Choose energy source level 2 entries where energy_source is \"Bioenergy\" in order to \n# separate Bioenergy subtypes to \"energy_source_level_3\" and subtypes for the rest to \"technology\"\nidx_DE_Bioenergy = DE_renewables[DE_renewables['energy_source_level_2'] == 'Bioenergy'].index\n\n# Assign technology to energy source level 3 for all entries where energy source level 2 is \n# Bioenergy and delete those entries from technology\nDE_renewables[['energy_source_level_3']] = DE_renewables.iloc[idx_DE_Bioenergy][['technology']]\nDE_renewables.loc[idx_DE_Bioenergy]['technology'] = np.nan\n\n# Assign energy source level 1 to the dataframe\nDE_renewables['energy_source_level_1'] = 'Renewable energy'\n\n# Show the hierarchy of the energy types present in the frame\nenergy_columns = ['energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3', 'technology']\nDE_renewables[energy_columns].drop_duplicates().sort_values(by='energy_source_level_2')\n\n\"\"\"\nExplanation: Separate and assign energy source level 1 - 3 and technology\nEnd of explanation\n\"\"\"\n\n\ndrop_mask = DE_renewables['energy_source_level_2'].isin(['Other fossil fuels', 'Storage'])\nDE_renewables.drop(DE_renewables.index[drop_mask], axis=0, inplace=True)\n\n\"\"\"\nExplanation: According to the OPSD energy hierarchy, the power plants whose energy_source_level_2 is either Storage or Other fossil fuels do not belong to the class of renewable-energy facilities. Therefore, we can remove them.\nEnd of explanation\n\"\"\"\n\n\n# Electrical capacity per energy source level 2 (in MW)\nDE_renewables.groupby(['energy_source_level_2'])['electrical_capacity_kW'].sum() / 1000\n\n\"\"\"\nExplanation: Summary of DataFrame\nEnd of explanation\n\"\"\"\n\n\n# kW to MW\nDE_renewables[['electrical_capacity_kW', 'thermal_capacity_kW']] /= 1000\n\n# adapt column name\nDE_renewables.rename(columns={'electrical_capacity_kW': 'electrical_capacity',\n                              'thermal_capacity_kW': 'thermal_capacity'}, inplace=True)\n\n\"\"\"\nExplanation: Transform electrical capacity from kW to MW\nEnd of explanation\n\"\"\"\n\n\n# Read generated postcode/location file\npostcode = pd.read_csv(os.path.join('input', 'de_tso_postcode_full.csv'))\n\n# Drop possible duplicates in postcodes\npostcode.drop_duplicates('postcode', keep='last', inplace=True)\n\n# Show first entries\npostcode.head(2)\n\n\"\"\"\nExplanation: Georeferencing\nGet coordinates by postcode\n(for data with no existing geocoordinates)\nThe available post code in the original data provides a first approximation for the geocoordinates of the RE power plants.<br>\nThe BNetzA data provides the full zip code whereas due to data privacy the TSOs only report the first three digits of the power plant's post code (e.g. 024xx) and no address. Subsequently a centroid of the post code region polygon is used to find the coordinates.\nWith data from\n*  http://www.suche-postleitzahl.org/downloads?download=plz-gebiete.shp.zip\n*  http://www.suche-postleitzahl.org/downloads?download_file=plz-3stellig.shp.zip\n*  http://www.suche-postleitzahl.org/downloads\na CSV-file for all existing German post codes with matching geocoordinates has been compiled. The latitude and longitude coordinates were generated by running a PostgreSQL + PostGIS database. Additionally the respective TSO has been added to each post code. (A Link to the SQL script will follow here later)\n(License: http://www.suche-postleitzahl.org/downloads, Open Database Licence for free use. Source of data: © OpenStreetMap contributors)\nEnd of explanation\n\"\"\"\n\n\n# Take postcode and longitude/latitude information\npostcode = postcode[['postcode', 'lon', 'lat']]\n\n# Cast DE_renewables['postcode'] to int64 in order to do the natural join of the dataframes\nDE_renewables['postcode'] = pd.to_numeric(DE_renewables['postcode'], errors='coerce')\n\n# Join two dataframes\nDE_renewables = DE_renewables.merge(postcode, on=['postcode'],  how='left')\n\n\"\"\"\nExplanation: Merge geometry information by using the postcode\nEnd of explanation\n\"\"\"\n\n\nDE_renewables.groupby(['utm_zone'])['utm_zone'].count()\n\n\"\"\"\nExplanation: Transform geoinformation\n(for data with already existing geoinformation)\nIn this section the existing geoinformation (in UTM-format) will be transformed into latidude and longitude coordiates as a uniform standard for geoinformation. \nThe BNetzA data set offers UTM Geoinformation with the columns utm_zone (UTM-Zonenwert), utm_east and utm_north. Most of utm_east-values include the utm_zone-value 32 at the beginning of the number. In order to properly standardize and transform this geoinformation into latitude and longitude it is necessary to remove this utm_zone value. For all UTM entries the utm_zone 32 is used by the BNetzA.\n|utm_zone|   utm_east|   utm_north| comment|\n|---|---|---| ----|\n|32|    413151.72|  6027467.73| proper coordinates|\n|32|    32912159.6008|  5692423.9664| caused error by 32|\nHow many different utm_zone values are in the data set?\nEnd of explanation\n\"\"\"\n\n\n# Find entries with 32 value at the beginning\nidx_32 = (DE_renewables['utm_east'].astype(str).str[:2] == '32')\nidx_notnull = DE_renewables['utm_east'].notnull()\n\n# Remove 32 from utm_east entries\nDE_renewables.loc[idx_32, 'utm_east'] = DE_renewables.loc[idx_32,\n                                                          'utm_east'].astype(str).str[2:].astype(float)\n\ndef convert_to_latlon(utm_east, utm_north, utm_zone):\n    try:\n        return utm.to_latlon(utm_east, utm_north, utm_zone, 'U')\n    except:\n        return ''\n\nDE_renewables['latlon'] = DE_renewables.loc[idx_notnull, ['utm_east', 'utm_north', 'utm_zone']].apply(\n        lambda x: convert_to_latlon(x[0], x[1], x[2]), axis=1).astype(str)\n\n\"\"\"\nExplanation: Remove the utm_zone \"32\" from the utm_east value\nEnd of explanation\n\"\"\"\n\n\nlat = []\nlon = []\n\nfor row in DE_renewables['latlon']:\n    try:\n        # Split tuple format into the column lat and lon\n        row = row.lstrip('(').rstrip(')')\n        parts = row.split(',')\n        if(len(parts)<2):\n            raise Exception('This is not a proper tuple. So go to exception block.')\n        lat.append(parts[0])\n        lon.append(parts[1])\n    except:\n        # set NaN\n        lat.append(np.NaN)\n        lon.append(np.NaN)\n\nDE_renewables['latitude'] = pd.to_numeric(lat)\nDE_renewables['longitude'] = pd.to_numeric(lon)\n\n# Add new values to DataFrame lon and lat\nDE_renewables['lat'] = DE_renewables[['lat', 'latitude']].apply(\n    lambda x: x[1] if pd.isnull(x[0]) else x[0],\n    axis=1)\n\nDE_renewables['lon'] = DE_renewables[['lon', 'longitude']].apply(\n    lambda x: x[1] if pd.isnull(x[0]) else x[0],\n    axis=1)\n\n\"\"\"\nExplanation: Conversion UTM to latitude and longitude\nEnd of explanation\n\"\"\"\n\n\n#DE_renewables[DE_renewables['data_source'] == '50Hertz'].to_excel('test.xlsx')\n\nprint('Missing coordinates ', DE_renewables.lat.isnull().sum())\n\ndisplay(\n    DE_renewables[DE_renewables.lat.isnull()].groupby(\n        ['energy_source_level_2','data_source']\n    )['data_source'].count()\n)\n\nprint('Share of missing coordinates (note that NaN can mean it\\'s all fine):')\n\nDE_renewables[DE_renewables.lat.isnull()].groupby(\n        ['energy_source_level_2','data_source']\n    )['data_source'].count() / DE_renewables.groupby(\n        ['energy_source_level_2','data_source']\n    )['data_source'].count()\n\n\"\"\"\nExplanation: Check: missing coordinates by data source and type\nEnd of explanation\n\"\"\"\n\n\n# drop lonlat column that contains both, latitute and longitude\nDE_renewables.drop(['latlon', 'longitude', 'latitude'], axis=1, inplace=True)\n\n\"\"\"\nExplanation: Remove temporary columns\nEnd of explanation\n\"\"\"\n\n\npickle.dump(DE_renewables, open( \"intermediate/temp_dfs_DE_before_cleaning.pickle\", \"wb\" ) )\n\nDE_renewables = pickle.load( open( \"intermediate/temp_dfs_DE_before_cleaning.pickle\", \"rb\" ) )\n\n\"\"\"\nExplanation: Save temporary Pickle (to have a point to quickly return to if things break after this point):\nEnd of explanation\n\"\"\"\n\n\n# Remove out-of-range dates\n# Keep only values between 1900 and 2100 to rule out outliers / wrong values. \n# Also, Excel doesn't support dates before 1900..\n\nmask = ((DE_renewables['commissioning_date']>pd.Timestamp('1900')) & \n        (DE_renewables['commissioning_date']<pd.Timestamp('2100')))\nDE_renewables = DE_renewables[mask]\n\nDE_renewables['municipality_code'] = DE_renewables['municipality_code'].astype(str)\n\n# Remove spaces from municipality code\nDE_renewables['municipality_code'] = DE_renewables['municipality_code'].str.replace(' ', '', regex=False)\n\nDE_renewables['municipality_code'] = pd.to_numeric(DE_renewables['municipality_code'], errors='coerce', downcast='integer')\n\n# Merge address and address_number\nto_string = lambda x: str(x) if not pd.isnull(x) else ''\nDE_renewables['address'] = DE_renewables['address'].map(to_string) + ' ' + DE_renewables['address_number'].map(to_string)\n\n# Make sure that the column has no whitespaces at the beginning and the end\nDE_renewables['address'] = DE_renewables['address'].str.strip()\n\n# Remove the column with address numbers as it is not needed anymore\ndel DE_renewables['address_number']\n\n\"\"\"\nExplanation: Clean data\nEnd of explanation\n\"\"\"\n\n\n# Set up a temporary postcode column as a string column for joining with the appropriate NUTS correspondence table\nDE_renewables['postcode_str'] = DE_renewables['postcode'].astype(str).str[:-2]\n\nDE_renewables = nuts_converter.add_nuts_information(DE_renewables, 'DE', DE_postcode2nuts_filepath,\n                                                     postcode_column='postcode_str',\n                                                     how=['postcode', 'municipality_code', 'municipality', 'latlon'])\n\n# Drop the temporary column\nDE_renewables.drop('postcode_str', axis='columns', inplace=True)\n\n# Report the number of facilites whose NUTS codes were successfully sudetermined\ndetermined = DE_renewables['nuts_1_region'].notnull().sum()\nprint('NUTS successfully determined for', determined, 'out of', DE_renewables.shape[0], 'facilities in DE.')\n\n# Report the number of facilites whose NUTS codes could not be determined\nnot_determined = DE_renewables['nuts_1_region'].isnull().sum()\nprint('NUTS could not be determined for', not_determined, 'out of', DE_renewables.shape[0], 'facilities in DE.')\n\n\"\"\"\nExplanation: Assign NUTS codes\nEnd of explanation\n\"\"\"\n\n\nvisualize_points(DE_renewables['lat'],\n                 DE_renewables['lon'],\n                'Germany',\n                 categories=DE_renewables['energy_source_level_2']\n)\n\n\"\"\"\nExplanation: Visualize\nEnd of explanation\n\"\"\"\n\n\nDE_renewables.to_pickle('intermediate/DE_renewables.pickle')\n\ndel DE_renewables\n\n\"\"\"\nExplanation: Save\nThe merged, translated, cleaned, DataFrame will be saved temporily as a pickle file, which stores a Python object fast.\nEnd of explanation\n\"\"\"\n\n\n# Download the data for Denmark\nfilepaths = downloader.download_data_for_country('DK')\nprint(filepaths)\n\n\"\"\"\nExplanation: Denmark DK\nDownload and read\nThe data which will be processed below is provided by the following data sources:\nEnergistyrelsen (ens) / Danish Energy Agency - The wind turbines register is released by the Danish Energy Agency. \nEnerginet.dk - The data of solar power plants are released by the leading transmission network operator Denmark.\ngeonames.org - The postcode  data from Denmark is provided by Geonames and licensed under a Creative Commons Attribution 3.0 license.\nEurostat - The data for converting information on municipalities, postcodes and geographic coordinates to NUTS 2016 classification codes.\nEnd of explanation\n\"\"\"\n\n\ndef read_dk_wind_turbines(filepath, sheet_name):\n    # Reads the data on Danish wind turbines\n    # from the sheet of the given name\n    # in the file with the path.\n    # Returns the data as a Pandas dataframe.\n    \n    book = xlrd.open_workbook(filepath)\n    sheet = book.sheet_by_name(sheet_name)\n    \n    # Since the column names are in two rows, not one,\n    # collect them in two parts. The first part is\n    # fixed and contains column names.\n    header = []\n    for i in range(0, 16):\n        # Make sure that strings 1) do not contain the newline sign\n        # and 2) have no trailing blank spaces.\n        column_name = sheet.cell_value(17, i).replace(\"\\n\", \"\").strip()\n        header = header + [column_name]\n    # The second part is variable. It consists of two subparts:\n    # 1) previous years (type float)\n    # 2) the past months of the current year (type date)\n    \n    # Reading previous years as column names\n    i = 16\n    cell = sheet.cell(16, i)\n\n    while cell.ctype == xlrd.XL_CELL_NUMBER:\n        column_name = str(int(cell.value))\n        header = header + [column_name]\n        i = i + 1\n        cell = sheet.cell(16, i)\n    \n    # Reading the months of the current year as column names\n    while cell.ctype == xlrd.XL_CELL_DATE:\n        year, month, _, _, _, _ = xlrd.xldate_as_tuple(cell.value, book.datemode)\n        column_name = str(\"{}-{}\".format(year, month))\n        header = header + [column_name]\n        i = i + 1\n        cell = sheet.cell(16, i)\n        \n    # Add the final column for the total of the current year\n    header += ['{}-total'.format(header[-1].split('-')[0])]\n        \n        \n    # Skip the first 17 rows in the sheet. The rest contains the data.\n    df = pd.read_excel(filepath,\n                       sheet_name=sheet_name,\n                       skiprows=17,\n                       skipfooter=3\n                    )\n    \n    # \n    #df.drop(df.columns[len(df.columns)-1], axis=1, inplace=True)\n    \n    # Set the column names.\n    df.columns = header\n    \n    return df\n\n# Get wind turbines data\nwind_turbines_sheet_name = 'IkkeAfmeldte-Existing turbines'\nDK_wind_filepath = filepaths['Energistyrelsen']\nDK_wind_df = read_dk_wind_turbines(DK_wind_filepath,\n                                   wind_turbines_sheet_name\n                                  )\n\n# Get photovoltaic data\nDK_solar_filepath = filepaths['Energinet']\nDK_solar_df = pd.read_excel(DK_solar_filepath,\n                            sheet_name='Data',\n                            skiprows=[0],\n                            converters={'Postnr': str}\n                           )\n\n# Remove duplicates\nDK_wind_df.drop_duplicates(inplace=True)\nDK_solar_df.drop_duplicates(inplace=True)\n\n\"\"\"\nExplanation: The function for reading the data on the wind turbines.\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for Denmark, create dictionary and show dictionary\nidx_DK = columnnames[columnnames['country'] == 'DK'].index\ncolumn_dict_DK = columnnames.loc[idx_DK].set_index('original_name')['opsd_name'].to_dict()\n\n# Windows has problems reading the csv entry for east and north (DK).\n# The reason might be the difference when opening the csv between linux and\n# windows.\ncolumn_dict_DK_temp = {}\nfor k, v in column_dict_DK.items():\n    column_dict_DK_temp[k] = v\n    if v == 'utm_east' or v == 'utm_north':\n        # merge 2 lines to 1\n        new_key = ''.join(k.splitlines())\n        column_dict_DK_temp[new_key] = v\n\ncolumn_dict_DK = column_dict_DK_temp\n\ncolumn_dict_DK\n\n# Replace column names based on column_dict_DK\nDK_wind_df.rename(columns=column_dict_DK, inplace=True)\nDK_solar_df.rename(columns=column_dict_DK, inplace=True)\n\n\"\"\"\nExplanation: Translate column names\nEnd of explanation\n\"\"\"\n\n\n# Add names of the data sources to the DataFrames\nDK_wind_df['data_source'] = 'Energistyrelsen'\nDK_solar_df['data_source'] = 'Energinet.dk'\n\n# Add energy source level 2 and technology for each of the two DataFrames\nDK_wind_df['energy_source_level_2'] = 'Wind'\nDK_solar_df['energy_source_level_2'] = 'Solar'\nDK_solar_df['technology'] = 'Photovoltaics'\n\n\"\"\"\nExplanation: Add data source and missing information\nEnd of explanation\n\"\"\"\n\n\nmask=DK_solar_df['commissioning_date'] == '1970-01-01'\nDK_solar_df.loc[mask, 'commissioning_date'] = np.nan\n\n\"\"\"\nExplanation: Correct the dates\nSome dates in the Energinet dataset are equal to 1970-01-01, which should be NaN instead\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for Denmark, create dictionary and show dictionary\nidx_DK = valuenames[valuenames['country'] == 'DK'].index\nvalue_dict_DK = valuenames.loc[idx_DK].set_index('original_name')['opsd_name'].to_dict()\n\n# Replace all original value names by the OPSD value names\nDK_wind_df.replace(value_dict_DK, inplace=True)\nDK_solar_df.replace(value_dict_DK, inplace=True)\n\n\"\"\"\nExplanation: Translate values and harmonize energy source level 2\nEnd of explanation\n\"\"\"\n\n\n# Index for all values with utm information\nidx_notnull = DK_wind_df['utm_east'].notnull()\n\n# Convert from UTM values to latitude and longitude coordinates\nDK_wind_df['lonlat'] = DK_wind_df.loc[idx_notnull, ['utm_east', 'utm_north']\n                                      ].apply(lambda x: utm.to_latlon(x[0],\n                                                                      x[1],\n                                                                      32,\n                                                                      'U'), axis=1).astype(str)\n\n# Split latitude and longitude in two columns\nlat = []\nlon = []\n\nfor row in DK_wind_df['lonlat']:\n    try:\n        # Split tuple format\n        # into the column lat and lon\n        row = row.lstrip('(').rstrip(')')\n        lat.append(row.split(',')[0])\n        lon.append(row.split(',')[1])\n    except:\n        # set NAN\n        lat.append(np.NaN)\n        lon.append(np.NaN)\n\nDK_wind_df['lat'] = pd.to_numeric(lat)\nDK_wind_df['lon'] = pd.to_numeric(lon)\n\n# drop lonlat column that contains both, latitute and longitude\nDK_wind_df.drop('lonlat', axis=1, inplace=True)\n\n\"\"\"\nExplanation: Georeferencing\nUTM32 to latitude and longitude (Data from Energistyrelsen)\nThe Energistyrelsen data set offers UTM Geoinformation with the columns utm_east and utm_north belonging to the UTM zone 32. In this section the existing geoinformation (in UTM-format) will be transformed into latidude and longitude coordiates as a uniform standard for geoinformation.\nEnd of explanation\n\"\"\"\n\n\n# Get geo-information\nzip_DK_geo = zipfile.ZipFile(filepaths['Geonames'])\n\n# Read generated postcode/location file\nDK_geo = pd.read_csv(zip_DK_geo.open('DK.txt'), sep='\\t', header=None)\n\n# add column names as defined in associated readme file\nDK_geo.columns = ['country_code', 'postcode', 'place_name', 'admin_name1',\n                  'admin_code1', 'admin_name2', 'admin_code2', 'admin_name3',\n                  'admin_code3', 'lat', 'lon', 'accuracy']\n\n# Drop rows of possible duplicate postal_code\nDK_geo.drop_duplicates('postcode', keep='last', inplace=True)\nDK_geo['postcode'] = DK_geo['postcode'].astype(str)\n\n# Add longitude/latitude infomation assigned by postcode (for Energinet.dk data)\nDK_solar_df = DK_solar_df.merge(DK_geo[['postcode', 'lon', 'lat']],\n                                on=['postcode'],\n                                how='left')\n\n# Show number of units with missing coordinates separated by wind and solar\nprint('Missing Coordinates DK_wind', DK_wind_df.lat.isnull().sum(), 'out of', len(DK_wind_df.index))\nprint('Missing Coordinates DK_solar', DK_solar_df.lat.isnull().sum(), 'out of', len(DK_solar_df.index))\n\nzip_DK_geo.close()\n\n\"\"\"\nExplanation: Postcode to lat/lon (WGS84)\n(for data from Energinet.dk)\nThe available post code in the original data provides an approximation for the geocoordinates of the solar power plants.<br>\nThe postcode will be assigned to latitude and longitude coordinates with the help of the postcode table.\nEnd of explanation\n\"\"\"\n\n\n# Merge DataFrames for wind and solar into DK_renewables\ndataframes = [DK_wind_df, DK_solar_df]\nDK_renewables = pd.concat(dataframes, sort=False)\nDK_renewables = DK_renewables.reset_index()\n\n# Assign energy source level 1 to the dataframe\nDK_renewables['energy_source_level_1'] = 'Renewable energy'\n\n# Merge the address and address-number columns into one\nto_string = lambda x: str(x) if not pd.isnull(x) else \"\"\nDK_renewables['address'] = DK_renewables['address'].map(to_string) + \" \" + DK_renewables['address_number'].map(to_string)\n\n# Make sure that the column has no whitespaces at the beginning or the end\nDK_renewables['address'] = DK_renewables['address'].str.strip()\n\n# Assign NUTS codes\nDK_postcode2nuts = filepaths['Eurostat']\nDK_renewables = nuts_converter.add_nuts_information(DK_renewables, 'DK', DK_postcode2nuts,\n                                                    how=['latlon', 'postcode', 'municipality_code', 'municipality_name'])\n\n\n# Report the number of facilites whose NUTS codes were successfully sudetermined\ndetermined = DK_renewables['nuts_1_region'].notnull().sum()\nprint('NUTS successfully determined for', determined, 'out of', DK_renewables.shape[0], 'facilities in DK.')\n\n# Report the number of facilites whose NUTS codes could not be determined\nnot_determined = DK_renewables['nuts_1_region'].isnull().sum()\nprint('NUTS could not be determined for', not_determined, 'out of', DK_renewables.shape[0], 'facilities in DK.')\n\n\"\"\"\nExplanation: Merge DataFrames, add NUTS information and choose columns\nEnd of explanation\n\"\"\"\n\n\nDK_renewables[DK_renewables['nuts_1_region'].isnull()][['municipality', 'municipality_code', 'lat', 'lon']]\n\n\"\"\"\nExplanation: Let us check geoinformation on the facilities for which NUTS codes could not be determined.\nEnd of explanation\n\"\"\"\n\n\n# Select those columns of the orignal data which are utilised further\ncolumns_of_interest = ['commissioning_date', 'energy_source_level_1', 'energy_source_level_2',\n                   'technology', 'electrical_capacity_kW', 'dso', 'gsrn_id', 'postcode',\n                   'municipality_code', 'municipality', 'address',\n                   'utm_east', 'utm_north', 'lon', 'lat', 'nuts_1_region', 'nuts_2_region', 'nuts_3_region',\n                   'hub_height', 'rotor_diameter', 'manufacturer', 'model', 'data_source']\n\n# Clean DataFrame from columns other than specified above\nDK_renewables = DK_renewables.loc[:, columns_of_interest]\nDK_renewables.reset_index(drop=True, inplace=True)\n\n\"\"\"\nExplanation: As we see, no information on municipality and latitude/longitude coordinates are present for those power plants, so there was no possibility to assign them their NUTS codes. \nSelect columns\nEnd of explanation\n\"\"\"\n\n\n# Remove duplicates\nDK_renewables.drop_duplicates(inplace=True)\nDK_renewables.reset_index(drop=True, inplace=True)\n\n\"\"\"\nExplanation: Remove duplicate rows\nEnd of explanation\n\"\"\"\n\n\n# kW to MW\nDK_renewables['electrical_capacity_kW'] /= 1000\n\n# adapt column name\nDK_renewables.rename(columns={'electrical_capacity_kW': 'electrical_capacity'},\n                     inplace=True)\n\n\"\"\"\nExplanation: Transform electrical_capacity from kW to MW\nEnd of explanation\n\"\"\"\n\n\nvisualize_points(DK_renewables['lat'],\n                 DK_renewables['lon'],\n                 'Denmark',\n                 categories=DK_renewables['energy_source_level_2']\n)\n\n\"\"\"\nExplanation: Visualize\nEnd of explanation\n\"\"\"\n\n\nDK_renewables.to_pickle('intermediate/DK_renewables.pickle')\ndel DK_renewables\n\n\"\"\"\nExplanation: Save\nEnd of explanation\n\"\"\"\n\n\n# Download the data\nfilepaths = downloader.download_data_for_country('FR')\n\n# Show the local paths\nfilepaths\n\n\"\"\"\nExplanation: France FR\nThe data which will be processed below is provided by the following data sources:\nMinistry for Ecological and Inclusive Transition - Number of installations and installed capacity of the different renewable source for every municipality in France. Data until 31/12/2017. As of 2020, this dataset is no longer maintained by the ministry and we refer to it as the old dataset.\nODRÉ - The Open Data Réseaux Énergies (ODRÉ, Open Data Networks for Energy) platform provides stakeholders with data around the themes of Production, Multi-energy Consumption, Storage, Mobility, Territories and Regions, Infrastructure, Markets and Meteorology. As of 2020, we refer to this dataset as the new dataset. It contains the data up to 31/12/2018.\nOpenDataSoft - a list of French INSEE codes and corresponding coordinates, published under the Licence Ouverte (Etalab).\nEnd of explanation\n\"\"\"\n\n\n# Load the data\nFR_re_filepath = filepaths['ODRE']\nFR_re_df = pd.read_csv(FR_re_filepath,\n                       sep=';',\n                       parse_dates=['dateRaccordement', 'dateDeraccordement',\n                                   'dateMiseEnService', 'dateDebutVersion'],\n                      infer_datetime_format=True)\n\n# Make sure that the column dateDeraccordement is datetime\nFR_re_df['dateDeraccordement'] = pd.to_datetime(FR_re_df['dateDeraccordement'], errors='coerce')\n\n\"\"\"\nExplanation: ODRE data\nLoad the data\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for France, create dictionary and show it\ncolumnnames = pd.read_csv(os.path.join('input', 'column_translation_list.csv'))\nidx_FR = columnnames[(columnnames['country'] == 'FR') & (columnnames['data_source'] == 'ODRE')].index\ncolumn_dict_FR = columnnames.loc[idx_FR].set_index('original_name')['opsd_name'].to_dict()\ncolumn_dict_FR\n\n# Translate column names\nFR_re_df.rename(columns=column_dict_FR, inplace=True)\n\n# Keep only the columns specified in the translation dictionary as we'll need only them\ncolumns_to_keep = list(column_dict_FR.values())\nFR_re_df = FR_re_df.loc[:, columns_to_keep]\nFR_re_df.reset_index(drop=True, inplace=True)\n\n# Show a pair of rows\nFR_re_df.head(2)\n\n\"\"\"\nExplanation: Translate column names\nEnd of explanation\n\"\"\"\n\n\nFR_re_df['data_source'] = 'Open Data Réseaux Énergies'\nFR_re_df['as_of_year'] = 2018 # Year for which the dataset has been compiled by the data source\n\n\"\"\"\nExplanation: Add data source\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for France, create a dictionary and show it\nvaluenames = pd.read_csv(os.path.join('input', 'value_translation_list.csv'))\n\nidx_FR = valuenames[(valuenames['country'] == 'FR') & (valuenames['data_source'] == 'ODRE')].index\nvalue_dict_FR = valuenames.loc[idx_FR].set_index('original_name')['opsd_name'].to_dict()\nvalue_dict_FR\n\n# Replace all original value names by the OPSD value names\nFR_re_df.replace(value_dict_FR, inplace=True)\n\n\"\"\"\nExplanation: Translate values\nEnd of explanation\n\"\"\"\n\n\nno_name_aliases = ['Agrégation des installations de moins de 36KW', 'Confidentiel', 'confidentiel']\nno_name_mask = FR_re_df['site_name'].isin(no_name_aliases)\nFR_re_df.loc[no_name_mask, 'site_name'] = np.nan\n\n\"\"\"\nExplanation: Correct site names\nSome facilites do not come with their names. Instead, strings such as Agrégation des installations de moins de 36KW, Confidentiel and confidentiel are used. Here, we correct this by setting all such names to np.nan.\nEnd of explanation\n\"\"\"\n\n\nmask = (FR_re_df['commissioning_date'].dt.year <= 1900) &\\\n       ((FR_re_df['technology'].isin(['Photovoltaics', 'Onshore']) |\\\n        (FR_re_df['energy_source_level_2'] == 'Solar')))\nFR_re_df.loc[mask, 'commissioning_date'] = np.nan\n\n#for x in FR_re_df[FR_re_df['commissioning_date'].dt.year <= 1980]['technology']:\n#    print(x)\n\n\"\"\"\nExplanation: Replace suspicious dates with N/A\nThe commissioning dates of some solar and wind plants are set in the early 20th and late 19th centuries. We replace those dates with N/A since they do not make sense.\nEnd of explanation\n\"\"\"\n\n\n# Check the columns\nFR_re_df.isnull().all()\n\n\"\"\"\nExplanation: Check missing values\nNow, we will drop all the columns and all the rows which contain only null values.\nEnd of explanation\n\"\"\"\n\n\n# Check the rows\nprint('There is a row containing all the null values?')\nFR_re_df.isnull().all(axis=1).any()\n\n\"\"\"\nExplanation: As we see above, no column contains only the null value, so we do not need to drop any.\nEnd of explanation\n\"\"\"\n\n\nFR_re_df[['energy_source_level_2', 'technology']].drop_duplicates()\n\n\"\"\"\nExplanation: No row contains only the null values, so no need to for filtering on that basis.\nStandardize the energy types and technologies\nNow, we proceed with standardizing the energy types and technologies present in the data according to the OPSD energy hierarchy.\nEnd of explanation\n\"\"\"\n\n\n# Define the mask for selecting rows with unusable info on electrical capacity\nec_mask = (FR_re_df['electrical_capacity'] == 0) | (FR_re_df['electrical_capacity'].isna())\n\n# Define the mask for selecting the rows with non-renewable energy_source_level_2\nnon_renewable_esl2 = ['Non-renewable thermal', 'Non-hydraulic storage', 'Nuclear']\nnon_renewable_esl2_mask = FR_re_df['energy_source_level_2'].isin(non_renewable_esl2)\n\n# Define the mask to select the rows with non-renewable technology\nnon_renewable_technologies = ['Steam turbine', 'Combustion cogeneration', 'Combustion engine',\n                              'Combined cycle', 'Pumped storage', 'Piston motor', 'Nuclear fission']\nnon_renewable_technology_mask = FR_re_df['technology'].isin(non_renewable_technologies)\n\n# Define the mask to select the rows without specified energy type and technology\nother_mask = (FR_re_df['energy_source_level_2'] == 'Other') & \\\n            ((FR_re_df['technology'] == 'Other') | (pd.isnull(FR_re_df['technology'])))\n\n# Combine the masks\ndrop_mask = ec_mask | non_renewable_esl2_mask | non_renewable_technology_mask | other_mask\n\n# Show how many rows are going to be dropped\nprint('Dropping', drop_mask.sum(), 'rows out of', FR_re_df.shape[0])\n\n# Keep all the rows not selected by the drop mask\nkeep_mask = ~drop_mask\nFR_re_df = FR_re_df[keep_mask].reindex()\n\n# Show some rows\nprint(\"A sample of the kept data:\")\nFR_re_df.sample(5)\n\n\"\"\"\nExplanation: In order to facilitate further processing, we can remove the rows that we know for sure we won't need.\nThose are the rows satisfying either of the following conditions:\n* electrical_capacity is 0 or NaN,\n* energy_source_level_2 corresponds to a non-renewable energy type (Non-renewable thermal, Non-hydraulic storage, Nuclear),\n* technology indicates that a non-renewable technology is used at the facility (Steam turbine, Combustion cogeneration, Combustion engine, Combined cycle, Pumped storage, Piston motor, Nuclear fission).\n* energy_source_level_2 is Other and technology is Other or NaN.\nEnd of explanation\n\"\"\"\n\n\nFR_re_df[['energy_source_level_2', 'technology']].drop_duplicates()\n\n\"\"\"\nExplanation: Standardize source levels 1-3 and technology\nLet us see the energy types and technologies present in the filtered data.\nEnd of explanation\n\"\"\"\n\n\n# Make sure that the proper string is used to indicate other or unspecified technology\nFR_re_df['technology'].replace('Other', 'Other or unspecified technology', inplace=True)\n\n# Define a function that will deal with other cases\ndef standardize(row):  \n    level_2 = row['energy_source_level_2']\n    technology = row['technology']\n    \n    if level_2 in ['Marine', 'Geothermal', 'Bioenergy']:\n        technology = np.nan\n    elif level_2 in ['Solar', 'Hydro', 'Other'] and pd.isna(technology):\n        technology = 'Other or unspecified technology'\n    elif level_2 == 'Wind' and (pd.isna(technology) or technology == 'Other or unspecified technology'):\n        technology = 'Onshore'\n    \n    if level_2 == 'Hydro' and technology in ['Lake', 'Closed']:\n        technology = 'Other or unspecified technology'\n    elif level_2 == 'Solar' and technology == 'Thermodynamic':\n        technology = 'Other or unspecified technology'\n    elif level_2 == 'Other' and technology == 'Photovoltaics':\n        level_2 = 'Solar'\n        \n    \n    return [level_2, technology]\n\n# Apply the rules coded in function standardize\nFR_re_df[['energy_source_level_2', 'technology']] = FR_re_df.apply(standardize, axis=1, result_type='expand')\n\n# Show the existing level 2 types and technologies\nFR_re_df[['energy_source_level_2', 'technology']].drop_duplicates()\n\n\"\"\"\nExplanation: First, let us standardize the values for energy source level 2 and technology.\n1. We will use np.nan to indicate that technology should not be specified for the respective kind of sources according to the OPSD hierarchy.\n2. 'Other or unspecified technology' will mean that technology should be specified but it was unclear or missing in the original dataset.\nThat means that we need to apply the following correction rules to the current data:\n- All occurences of Other in the column technology should be replaced with Other or unspecified technology.\n- If energy_source_level_2 is Marine, Geothermal, or Bioenergy, then technology should be set to np.nan regardless of what is specified in the data set.\n- If energy_source_level_2 is Solar or Hydro, and technology is NaN, then technology should be set to Other or unspecified technology.\n- If energy_source_level_2 is Wind and technology is NaN, then technology should be set to Onshore since France has no offshore wind farms.\n- If energy_source_level_2 is Hydro and technology is Lake or Closed, then technology should be set to Other or unspecified technology.\n- If energy_source_level_2 is Solar and technology is Thermodynamic, then technology should be set to Other or unspecified technology.\n- If energy_source_level_2 is Other and technology is Photovoltaics, then energy_source_level_2 should be set to Solar.\nEnd of explanation\n\"\"\"\n\n\nFR_re_df[['energy_source_level_2', 'energy_source_level_3']].drop_duplicates()\n\n\"\"\"\nExplanation: Let us now deal with the third level of the energy hierarchy. Only Bioenergy has the third level. Information on it can be found in the column energy_source_level_3 (whose original name was combustible).\nEnd of explanation\n\"\"\"\n\n\nindex = (pd.isna(FR_re_df['energy_source_level_3']) & \\\n        (FR_re_df['energy_source_level_2'] == 'Bioenergy'))\nFR_re_df.loc[index, 'energy_source_level_3'] = 'Other or unspecified'\n         \nindex = FR_re_df['energy_source_level_3'] == 'Wood'\nFR_re_df.loc[index, 'energy_source_level_3'] = 'Biomass and biogas'\n\n\"\"\"\nExplanation: We see that only the following two corrections are needed:\n- If energy_source_level_3 is Wood, set energy_source_level_3 to Biomass and biogas.\n- If energy_source_level_3 is NaN, and energy_source_level_2 is Bioenergy, set energy_source_level_3 to Other or unspecified.\nEnd of explanation\n\"\"\"\n\n\n# Assign energy_source_level_1 to the dataframe\nFR_re_df['energy_source_level_1'] = 'Renewable energy'\n\n# Show the hierarchy\nenergy_columns = ['energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3', 'technology']\nFR_re_df[energy_columns].drop_duplicates()\n\n\"\"\"\nExplanation: Finally, we declare all the plants as renewable and show the final hierarchy.\nEnd of explanation\n\"\"\"\n\n\n# Get the local path of the downloaded georeferencing data\nFR_geo_filepath = filepaths['Opendatasoft']\n\n# Read INSEE Code Data\nFR_geo = pd.read_csv(FR_geo_filepath,\n                     sep=';',\n                     header=0,\n                     converters={'Code_postal': str})\n\n# Drop possible duplicates of the same INSEE code\nFR_geo.drop_duplicates('INSEE_COM', keep='last', inplace=True)\n\n# create columns for latitude/longitude\nlat = []\nlon = []\n\n# split in latitude/longitude\nfor row in FR_geo['Geo Point']:\n    try:\n        # Split tuple format\n        # into the column lat and lon\n        row = row.lstrip('(').rstrip(')')\n        lat.append(row.split(',')[0])\n        lon.append(row.split(',')[1])\n    except:\n        # set NAN\n        lat.append(np.NaN)\n        lon.append(np.NaN)\n\n# add these columns to the INSEE DataFrame\nFR_geo['lat'] = pd.to_numeric(lat)\nFR_geo['lon'] = pd.to_numeric(lon)\n\n# Column names of merge key have to be named identically\nFR_re_df.rename(columns={'municipality_code': 'INSEE_COM'}, inplace=True)\n\n# Merge longitude and latitude columns by the Code INSEE\nFR_re_df = FR_re_df.merge(FR_geo[['INSEE_COM', 'lat', 'lon']],\n                          on=['INSEE_COM'],\n                          how='left')\n\n# Translate Code INSEE column back to municipality_code\nFR_re_df.rename(columns={'INSEE_COM': 'municipality_code'}, inplace=True)\n\n\"\"\"\nExplanation: Georeferencing\nFirst, we will determine the plants' longitude and latitude coordinates, and then assign them their NUTS codes.\nMunicipality (INSEE) code to lon/lat\nEnd of explanation\n\"\"\"\n\n\n#import importlib\n#importlib.reload(util.nuts_converter)\n#from util.nuts_converter import NUTSConverter\n#nuts_converter = NUTSConverter(downloader, eurostat_eu_directory_path)\n\nFR_postcode2nuts_path = filepaths['Eurostat']\nFR_re_df = nuts_converter.add_nuts_information(FR_re_df, 'FR', FR_postcode2nuts_path,\n                                               lau_name_type='NATIONAL',\n                                               closest_approximation=True,\n                                               how=['municipality_code', 'latlon'])\n\n# Report the number of facilites whose NUTS codes were successfully determined\ndetermined = FR_re_df['nuts_1_region'].notnull().sum()\nprint('NUTS successfully determined for', determined, 'out of', FR_re_df.shape[0], 'facilities in FR.')\n\n# Report the number of facilites whose NUTS codes could not be determined\nnot_determined = FR_re_df['nuts_1_region'].isnull().sum()\nprint('NUTS could not be determined for', not_determined, 'out of', FR_re_df.shape[0], 'facilities in FR.')\n\n\"\"\"\nExplanation: Determine NUTS codes\nEnd of explanation\n\"\"\"\n\n\n# Check the facilities without NUTS classification\nno_nuts = FR_re_df['nuts_1_region'].isnull()\n\n# Find the masks where some information for finding the proper NUTS code is present\nlat_or_lon_present = ~(FR_re_df['lat'].isna() & FR_re_df['lon'].isna())\nmunicipality_code_present = ~(FR_re_df['municipality_code'].isnull())\nmunicipality_name_present = ~(FR_re_df['municipality'].isnull())\n\n# Show the cases where NUTS classification failed even though it shouldn't have\nprint('1. No NUTS code but latitude/longitude info present')\nproblematic_lat_lon = FR_re_df[no_nuts & lat_or_lon_present][['lat', 'lon']]\ndisplay(problematic_lat_lon)\n\nprint('2. No NUTS code but municipality code info present')\nproblematic_municipality_codes = FR_re_df[no_nuts & municipality_code_present]['municipality_code'].unique()\ndisplay(problematic_municipality_codes)\n\nprint('3. No NUTS code but municipality name info present')\nproblematic_municipality_names = FR_re_df[no_nuts & municipality_name_present]['municipality'].unique()\ndisplay(problematic_municipality_names)\n\n\"\"\"\nExplanation: Let us now check the facilities without NUTS classification.\nEnd of explanation\n\"\"\"\n\n\n# Check if the any problematic code is actually present in the translation table\npresent_any = False\nfor code in problematic_municipality_codes:\n    mask = nuts_converter.municipality2nuts_df['municipality_code'].str.match(code)\n    present_any = present_any or mask.any()\n    \nprint(present_any)\n\n\"\"\"\nExplanation: We see that no row with known longitude and latitude was left unclassified. \nWhat we also see is that some municipality codes did not translate to the corresponding NUTS codes. Further inspection shows that those codes are not present in the official NUTS translation tables.\nEnd of explanation\n\"\"\"\n\n\n# Print only the names of those problematic municipalities, which appear in the translation table only once.\nfor name in problematic_municipality_names:\n    mask = nuts_converter.municipality2nuts_df['municipality'].str.match(name)\n    if mask.sum() == 1:\n        print(name)\n\n\"\"\"\nExplanation: We also see that problematic municipality names are either not present in the official translation tables or more than one municipality in the tables bears them.\nEnd of explanation\n\"\"\"\n\n\nFR_re_df['electrical_capacity'] = FR_re_df['electrical_capacity'] / 1000\n\n\"\"\"\nExplanation: Therefore, we can confirm that NUTS classification codes were determined with the highest precision possible.\nConvert electrical capacity to MW\nEnd of explanation\n\"\"\"\n\n\n# Load the data\nFR_re_filepath = filepaths['gouv.fr']\n\nFR_re_df_old = pd.read_excel(FR_re_filepath,\n                         sheet_name='Commune',\n                         encoding='UTF8',\n                         thousands='.',\n                         decimals=',',\n                         header=[3, 4],\n                         skipfooter=9,      # skip the summary rows \n                         index_col=[0, 1],  # required for MultiIndex\n                         converters={'Code officiel géographique': str})\nFR_re_df_old.tail()\n\n\"\"\"\nExplanation: Old data\nEnd of explanation\n\"\"\"\n\n\n# Rearrange data\nFR_re_df_old.index.rename(['insee_com', 'municipality'], inplace=True)\nFR_re_df_old.columns.rename(['energy_source_level_2', None], inplace=True)\nFR_re_df_old = (FR_re_df_old\n                .stack(level='energy_source_level_2', dropna=False)\n                .reset_index(drop=False))\n\n# Choose the translation terms for France, create dictionary and show dictionary\nidx_FR = columnnames[(columnnames['country'] == 'FR') & (columnnames['data_source'] == 'gouv.fr')].index\ncolumn_dict_FR = columnnames.loc[idx_FR].set_index('original_name')['opsd_name'].to_dict()\ncolumn_dict_FR\n\n# Translate columnnames\nFR_re_df_old.rename(columns=column_dict_FR, inplace=True)\n\n# Drop all rows that contain NA \nFR_re_df_old = FR_re_df_old.dropna()\nFR_re_df_old.head(10)\n\n\"\"\"\nExplanation: This French data source contains number of installations and sum of installed capacity per energy source per municipality. The list is limited to the plants which are covered by article 10 of february 2000 by an agreement to a purchase commitment.\nEnd of explanation\n\"\"\"\n\n\nFR_re_df_old['data_source'] = 'Ministry for the Ecological and Inclusive Transition'\nFR_re_df_old['as_of_year'] = 2017 # Year for which the dataset has been compiled by the data source\n\n\"\"\"\nExplanation: Add data source\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for France, create dictionary and show dictionary\nidx_FR = valuenames[(valuenames['country'] == 'FR') & (valuenames['data_source'] == 'gouv.fr')].index\nvalue_dict_FR = valuenames.loc[idx_FR].set_index('original_name')['opsd_name'].to_dict()\nvalue_dict_FR\n\n# Replace all original value names by the OPSD value names\nFR_re_df_old.replace(value_dict_FR, inplace=True)\n\n\"\"\"\nExplanation: Translate values and harmonize energy source level 2\nKept secret if number of installations < 3\nIf the number of installations is less than 3, it is marked with an s instead of the number 1 or 2 due to statistical confidentiality (as explained by the data provider). Here, the s is changed to < 3. This is done in the same step as the other value translations of the energy sources.\nEnd of explanation\n\"\"\"\n\n\nenergy_source_dict_FR = valuenames.loc[idx_FR].set_index(\n    'opsd_name')['energy_source_level_2'].to_dict()\ndisplay(energy_source_dict_FR)\ndisplay(FR_re_df_old[['energy_source_level_2']].drop_duplicates())\n(FR_re_df_old['energy_source_level_2'].replace(energy_source_dict_FR).unique())\n\n# Create dictionnary in order to assign energy_source to its subtype\nenergy_source_dict_FR = valuenames.loc[idx_FR].set_index(\n    'opsd_name')['energy_source_level_2'].to_dict()\n\n# Column energy_source partly contains subtype information, thus this column is copied\n# to new column for energy_source_subtype.\nFR_re_df_old['technology'] = FR_re_df_old['energy_source_level_2']\n\n# Only Photovoltaics should be kept as technology. Hydro should be changed to 'Other or unspecified technology',\n# Geothermal to NaN, and Wind to Onshore.\n# 1. np.nan means that technology should not be specified for the respective kind of sources\n#    according to the hierarchy (http://open-power-system-data.org/2016-10-25-opsd_tree.svg)\n# 2. 'Other or unspecified technology' means that technology should be specified\n#    but it was unclear or missing in the original dataset.\ntechnology_translation_dictionary = {\n    'Solar' : 'Photovoltaics',\n    'Wind': 'Onshore',\n    'Hydro': 'Other or unspecified technology',\n    'Geothermal': np.nan\n}\nFR_re_df_old['technology'].replace(technology_translation_dictionary, inplace=True)\n\n# The energy source subtype values in the energy_source column are replaced by\n# the higher level classification\nFR_re_df_old['energy_source_level_2'].replace(energy_source_dict_FR, inplace=True)\n\n# Assign energy_source_level_1 to the dataframe\nFR_re_df_old['energy_source_level_1'] = 'Renewable energy'\n\nFR_re_df_old.reset_index(drop=True, inplace=True)\n\n# Choose energy source level 2 entries where energy source level 2 is Bioenergy in order to  \n# seperate Bioenergy subtypes to energy source level 3 and subtypes for the rest to technology\nidx_FR_Bioenergy = FR_re_df_old[FR_re_df_old['energy_source_level_2'] == 'Bioenergy'].index\n\n# Assign technology to energy source level 3  for all entries where energy source level 2 is  \n# Bioenergy and delete those entries from  technology\nFR_re_df_old[['energy_source_level_3']] = FR_re_df_old.iloc[idx_FR_Bioenergy][['technology']]\nFR_re_df_old.loc[idx_FR_Bioenergy,'technology'] = np.nan\n\n\"\"\"\nExplanation: Separate and assign energy source level 1-3 and technology\nEnd of explanation\n\"\"\"\n\n\nFR_re_df_old[['energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3', 'technology']].drop_duplicates()\n\n\"\"\"\nExplanation: Show the hierarchy of the energy types present in the data.\nEnd of explanation\n\"\"\"\n\n\n# Column names of merge key have to be named identically\nFR_re_df_old.rename(columns={'municipality_code': 'INSEE_COM'}, inplace=True)\n\n# Merge longitude and latitude columns by the Code INSEE\nFR_re_df_old = FR_re_df_old.merge(FR_geo[['INSEE_COM', 'lat', 'lon']],\n                          on=['INSEE_COM'],\n                          how='left')\n\n# Translate Code INSEE column back to municipality_code\nFR_re_df_old.rename(columns={'INSEE_COM': 'municipality_code'}, inplace=True)\n\n\"\"\"\nExplanation: Georeferencing\nMunicipality (INSEE) code to lat/lon\nEnd of explanation\n\"\"\"\n\n\nFR_postcode2nuts_path = filepaths['Eurostat']\n\nFR_re_df_old = nuts_converter.add_nuts_information(FR_re_df_old, 'FR', FR_postcode2nuts_path,\n                                               how=['municipality_code', 'latlon'])\n# how=['municipality', 'municipality_code', 'latlon']\n# Report the number of facilites whose NUTS codes were successfully determined\ndetermined = FR_re_df_old['nuts_1_region'].notnull().sum()\nprint('NUTS successfully determined for', determined, 'out of', FR_re_df_old.shape[0], 'facilities in FR.')\n\n# Report the number of facilites whose NUTS codes could not be determined\nnot_determined = FR_re_df_old['nuts_1_region'].isnull().sum()\nprint('NUTS could not be determined for', not_determined, 'out of', FR_re_df_old.shape[0], 'facilities in FR.')\n\n# Show the facilities without NUTS classification\nFR_re_df_old[FR_re_df_old['nuts_1_region'].isnull()]\n\n\"\"\"\nExplanation: Determine NUTS codes\nEnd of explanation\n\"\"\"\n\n\n# For each column present in the new data's column space, but not the old,\n# add an empty column to the old data.\n\nfor new_column in FR_re_df.columns:\n    if new_column not in FR_re_df.columns:\n        FR_re_df_old[new_column] = np.nan\n\n# Define the mask to select the municipalities from the old data, that are not covered\n# by the new.\nnot_included = ~(FR_re_df_old['municipality_code'].isin(FR_re_df['municipality_code']))\nFR_re_df_old[not_included]\n\n# Add a dummy column to the new data frame\n# representing the number of power plants (always 1)\nFR_re_df['number_of_installations'] = 1\n\n# Mark the old data rows as aggregations on municipality level.\nFR_re_df_old['site_name'] = 'Aggregated data for ' + FR_re_df_old['municipality']\n\n# Concatenate the new data with the old rows referring to the municipalities\n# not covered by the new.\nFR_re_df = pd.concat([FR_re_df, FR_re_df_old[not_included]], ignore_index=True, axis='index', sort=True)\n\n\"\"\"\nExplanation: As we can see, the NUTS codes were determined successfully for all the facilities in the dataset.\nIntegrate old and new data\nSome municipalities are not covered by the new data set, provided by ODRE. Now, we find those municipalities and integrate them with the new data.\nThe only column present in the old data, but not in the new, is number_of_installations. Since the old data\nwere aggregated on the municipality level, the column in question refers to the numbers of power plants in the \nmunicipalitis. Since the new data covers individual plants, if we set the column number_of_installations to 1\nfor all the plants in the the new data, we will make the two sets consistent with one another and be able\nto concatenate them. \nWe will set site_name to 'Aggregated data for municipality' for all the rows from the old data, where municipality refers to the name of the municipality for which the row has been compiled.\nNote: the electrical capacity in the old data is already in MW, so conversion is not needed.\nEnd of explanation\n\"\"\"\n\n\ncolumns_to_keep = ['EIC_code', 'municipality_group_code', 'IRIS_code', 'as_of_year',\n       'commissioning_date', 'connection_date', 'data_source', 'departement',\n       'departement_code', 'disconnection_date',\n       'electrical_capacity', 'energy_source_level_1', 'energy_source_level_2',\n       'energy_source_level_3', 'lat', 'lon',\n       'municipality', 'municipality_code',\n       'municipality_group', 'number_of_installations', 'nuts_1_region',\n       'nuts_2_region', 'nuts_3_region', 'region', 'region_code', 'site_name',\n       'source_station_code', 'technology']\nFR_re_df = FR_re_df[columns_to_keep]\nFR_re_df.reset_index(drop=True, inplace=True)\n\n\"\"\"\nExplanation: Select the columns\nNow, we select the columns we want to keep.\nEnd of explanation\n\"\"\"\n\n\nvisualize_points(FR_re_df['lat'],\n                 FR_re_df['lon'],\n                 'France',\n                 categories=FR_re_df['energy_source_level_2']\n)\n\n\"\"\"\nExplanation: Visualize\nEnd of explanation\n\"\"\"\n\n\nFR_re_df.to_pickle('intermediate/FR_renewables.pickle')\n\ndel FR_re_df\n\n\"\"\"\nExplanation: Save\nEnd of explanation\n\"\"\"\n\n\n# Download the data\nfilepaths = downloader.download_data_for_country('PL')\n\n# Get the local paths to the data files\nPL_re_filepath = filepaths['Urzad Regulacji Energetyki']\nPL_postcode2nuts_filepath = filepaths['Eurostat']\nPL_geo_filepath = filepaths['Geonames']\n\n\"\"\"\nExplanation: Poland PL\nDownload\nThe data which will be processed below is provided by the following data source:\nUrzad Regulacji Energetyki (URE) / Energy Regulatory Office - Installed capacities of renewable-energy power plants in Poland. The plants are anonymized in the sense that no names, post codes or geographical coordinates are present. They are described by: the energy type their use, installed capacity, województwo (province) and powiat (district) that they are located in.\nEnd of explanation\n\"\"\"\n\n\n# Read the data into a pandas dataframe\nPL_re_df = pd.read_excel(PL_re_filepath,\n                       encoding='latin',\n                       header=2,\n                       skipfooter=14\n                       )\n# Show 5 random rows\nPL_re_df.sample(n=5)\n\n\"\"\"\nExplanation: Load and explore the data\nThe dataset comes in the csv format. Let us open it, inspect its columns and clean it a bit before processing it further.\nEnd of explanation\n\"\"\"\n\n\n# Get the mask for selecting the WS plants\nws_mask = PL_re_df['Rodzaj_OZE'] == 'WS'\n\n# Drop them\nprint('Dropping', ws_mask.sum(), 'out of', PL_re_df.shape[0], 'power plants.')\nPL_re_df.drop(PL_re_df.index[ws_mask], axis=0, inplace=True)\nPL_re_df.reset_index(drop=True, inplace=True)\n\n\"\"\"\nExplanation: There are only five columns:\n- Lp.: the ordinal number of the entry (power plant), effectively serving as its identification number.\n- Województwo: the province (voivodeship) where the plant is located\n- Powiat: the district where the plant is located\n- Rodzaj_OZE: the code of the energy the plants uses. According to the legend in the .xlsx file, the codes are as follows:\n  - BG: biogas\n  - BM: biomass\n  - PVA: solar energy\n  - WIL: wind energy\n  - WO: hydroenergy\n  - WS: using the technology of co-firing biomass, biogas or bioliquids with other fuels (fossil fuels and biomass / biogas / bioliquids)\n- Moc zainstalowana [MW]: installed capacity (in MWs).\nThe type corresponding to WS does not fit into the OPSD energy hiearchy, so we can drop such plants.\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for Poland, create and show the dictionary\ncolumnnames = pd.read_csv(os.path.join('input', 'column_translation_list.csv'))\nidx_PL = columnnames[(columnnames['country'] == 'PL') &\n                     (columnnames['data_source'] == 'Urzad Regulacji Energetyki')].index\ncolumn_dict_PL = columnnames.loc[idx_PL].set_index('original_name')['opsd_name'].to_dict()\ncolumn_dict_PL\n\n# Translate column names\nPL_re_df.rename(columns=column_dict_PL, inplace=True)\n\n# Show a couple of rows\nPL_re_df.head(2)\n\n\"\"\"\nExplanation: To ease the work, we can translate the columns' names to English using the OPSD translation tables.\nEnd of explanation\n\"\"\"\n\n\nprint('The number of missing values in the data:', PL_re_df.isna().sum().sum())\n\nprint('Are all capacities proper numbers?', PL_re_df['electrical_capacity'].dtype == 'float64')\n\nprint('What about the energy codes?', PL_re_df['energy_type'].unique())\n\n# Check the voivodeships\nprint('Show the names of the voivodeships.')\nPL_re_df['region'].unique()\n\n\"\"\"\nExplanation: Inspect the data\nLet us do few quick checks to see state of the data:\n- Are there any NA values?\n- Are all the values in the column electrical_capacity proper numbers?\n- Are all the values in the column energy_type (codes of energy types) consistent strings? Here we check if all the codes appear in one and only one form. For example, PVA is the code for solar energy and we want to make sure that only PVA appears in the column, not other variations such as pva, Pva etc.\n- What is the form of the geographical data? Are some districts represented by different strings in different rows? What about the regions (provinces, województwa, voivodeships)?\nWe will need the answers to those questions to know how to proceed with processing.\nEnd of explanation\n\"\"\"\n\n\nPL_re_df['region'] = PL_re_df['region'].str.strip().str.capitalize()\nPL_re_df['region'].unique()\n\n\"\"\"\nExplanation: We can see that each name comes in two forms: (1) with the first letter capital and (2) with the first letter lowercase. One province is referred to by three different strings: 'Śląskie', 'śląskie', and 'śląskie ' (the last with a trailing white space). In order to standardize this column, we trim and capitalize all the strings appearing in it.\nEnd of explanation\n\"\"\"\n\n\ndistricts = PL_re_df['district'].unique()\ndistricts.sort()\ndistricts\n\n\"\"\"\nExplanation: Now, let us check the strings for districts (powiats).\nEnd of explanation\n\"\"\"\n\n\n# Correct the typos\nPL_re_df.loc[PL_re_df['district'] == 'lipowski', 'district'] = 'lipnowski'\nPL_re_df.loc[PL_re_df['district'] == 'hojnowski', 'district'] = 'hajnowski'\n\n\"\"\"\nExplanation: As we see in the list, the same district can be referred to by more than one string. We identify the following ways a district is referred to in the dataset:\n1. by using the noun in the nominative case, capitalized (e.g. Kraków),\n2. by prepending m. or m. st. to the form 1 (e.g. m. Kraków or m. st. Warszawy) and\n3. by the possesive adjective, lowercase (e.g. krakowski).\nSome districts, such as Krakow, appear in all the three forms, but there are those which appear in two (e.g. Bytom and m. Bytom). This will pose a problem when we later try to assign the plants their NUTS codes. Furthermore, the NUTS translation tables do not map districts to the codes, but lower administrative units (municipalities) and postcodes to NUTS. We solve this issue at a later point in the notebook, Section Georeferencing (NUTS classification), and not here as it requires heavier processing than warranted during initial explorative analysis and lightweight cleaning of the data.\nWe note that the districts lipowski and hojnowski are misspelled, as they should actually be lipnowski and hajnowski, so we can correct the typos now.\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for Poland, create dictionary \nidx_PL = valuenames[valuenames['country'] == 'PL'].index\nvalue_dict_PL = valuenames.loc[idx_PL].set_index('original_name')['opsd_name'].to_dict()\n\n# Set energy source level 3\nPL_re_df['energy_source_level_3'] = PL_re_df['energy_type'].replace(value_dict_PL)\n\n# Create dictionnary in order to assign energy_source_level_2 to its subtype\nidx_PL = valuenames[valuenames['country'] == 'PL'].index\nenergy_source_dict_PL = valuenames.loc[idx_PL].set_index('original_name')['energy_source_level_2'].to_dict()\n\n# Add energy_source_level_2\nPL_re_df['energy_source_level_2'] = PL_re_df['energy_type'].replace(energy_source_dict_PL)\n\n\n# Standardize the values for technology\n# 1. np.nan means that technology should not be specified for the respective kind of sources\n#    according to the hierarchy (http://open-power-system-data.org/2016-10-25-opsd_tree.svg)\n# 2. 'Other or unspecified technology' means that technology should be specified\n#    but it was unclear or missing in the original dataset.\ntechnology_translation_dictionary = {\n    'BG': np.nan,\n    'BM': np.nan,\n    'PVA': 'Other or unspecified technology', # Photovoltaics?\n    'WIL': 'Other or unspecified technology', # Onshore?\n    'WO': 'Other or unspecified technology', # Run-of-river\n}\n\nPL_re_df['technology'] = PL_re_df['energy_type'].replace(technology_translation_dictionary)\n\n# Add energy_source_level_1\nPL_re_df['energy_source_level_1'] = 'Renewable energy'\n\n# Show the hierarchy of sources present in the dataset\nPL_re_df[['energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3', 'technology']].drop_duplicates().sort_values(by='energy_source_level_2')\n\n\"\"\"\nExplanation: Harmonising energy levels\nEnd of explanation\n\"\"\"\n\n\n# Define the function to standardize district names from the original data\ndef standardize_districts(original_string):\n    if original_string[-1] == ',': # there is one district whose name ends with ','; that's a typo in the data\n        original_string = original_string[:-1]\n        \n    if original_string.startswith('m. st. '):\n        return original_string[7:]\n    elif original_string.startswith('m. '):\n        return original_string[3:]\n    elif any([original_string.endswith(suffix) for suffix in ['ski', 'cki', 'zki']]):\n        return 'Powiat ' + original_string\n    else:\n        return original_string\n\n# Get geo-information\nzip_PL_geo = zipfile.ZipFile(PL_geo_filepath)\n\n# Read generated postcode/location file\nPL_geo = pd.read_csv(zip_PL_geo.open('PL.txt'), sep='\\t', header=None)\n\n# add column names as defined in associated readme file\nPL_geo.columns = ['country_code', 'postcode', 'place_name', 'admin_name1',\n                  'admin_code1', 'admin_name2', 'admin_code2', 'admin_name3',\n                  'admin_code3', 'lat', 'lon', 'accuracy']\n\n# Drop rows of possible duplicate postal_code\nPL_geo.drop_duplicates('postcode', keep='last', inplace=True)\nPL_geo['postcode'] = PL_geo['postcode'].astype(str)\n\n# Get the names\ngeonames_districts = PL_geo['admin_name2'].unique()\n\n# Show them\ngeonames_districts\n\n# Standardize the district names from the original data\nPL_re_df['standardized_district'] = PL_re_df['district'].apply(standardize_districts)\nstandardized_districts = PL_re_df['standardized_district'].unique()\n\n# Check which districts could not be found in the GeoNames data\n#print(len([x for x in semi if x in geopowiats]), len([x for x in semi if x not in geopowiats]))\nnot_found = set(standardized_districts).difference(set(geonames_districts))\nnumber_of_not_found = len(not_found)\ntotal = len(standardized_districts)\nprint('{}/{} names could not be found. Those are:'.format(number_of_not_found, total))\nprint(not_found)\n\n\"\"\"\nExplanation: Georeferencing (NUTS classification)\nWe have already seen that the district names are not standardized and observed that we cannot use them directly to get the corresponding NUTS codes.\nThere is a way to get around this issue. We can do it as folows:\n1. First, we find a postcode in the GeoNames zip for Poland that corresponds to each district in the URE data. To do so, we must standardize all the district names to the forms that appear in the GeoNames zip file.\n2. Then, we can easily map a postcode to the appropriate NUTS codes using nuts_converter.\nBy inspection, we observe that all the district names in the zip have one of the following two forms:\n- Noun in the nominative case, capitalized.\n- Powiat * where * is a possessive adjective.\nSo, we standardize all the strings in the district column as follows:\n- Remove all the trailing whitespaces and characters other than letters.\n- If the string starts with m. or m. st., remove m. (or m. st.) from the beginning of the string.\n- If the string ends with a possessive suffix ski, cki or zki, prepend the string Powiat (note the ending whitespace) to it.\nEnd of explanation\n\"\"\"\n\n\n# We define the similarity between two strings, string1 and string2,\n# as the length of the longest prefix of string1 that appears in string2.\n# Note 1: this measure of similarity is not necessarily symmetrical.\n# Note 2: a prefix of a string is its substring that starts from the beginning of the string.\ndef calculate_similarity(string1, string2):\n    for n in range(len(string1), 1, -1):\n        prefix = string1[0:(n-1)] \n        if prefix in string2:\n            return len(prefix)\n    return 0\n\n# Define a function to find, among a group of candidate strings,\n# the most similar string to the one given as the reference string.\ndef find_the_most_similar(reference_string, candidate_strings):\n    the_most_similar = None\n    maximal_similarity = 0\n    for candidate_string in candidate_strings:\n        similarity = calculate_similarity(reference_string, candidate_string)\n        if similarity > maximal_similarity:\n            maximal_similarity = similarity\n            the_most_similar = candidate_string\n    return the_most_similar, maximal_similarity\n\nalready_mapped = PL_re_df[['district', 'standardized_district']].drop_duplicates().to_dict(orient='records')\nalready_mapped = {mapping['district'] : mapping['standardized_district'] for mapping in already_mapped \n                 if mapping['standardized_district'] in geonames_districts}\n\n# Make a dictionary to map each district from the original data to its GeoNames equivalent.\n# The districts whose standardized versions have been found in the GeoNames data to their standardizations.\n# The mappings for other districts will be found using the previously defined similarity measures.\ndistricts_map = PL_re_df[['district', 'standardized_district']].drop_duplicates().to_dict(orient='records')\ndistricts_map = {mapping['district'] : mapping['standardized_district'] for mapping in districts_map}\n\n\n# Override the mappings for the 49 districts whose standardized names have not been found in the GeoNames data.\nfor district, standardized_district in districts_map.items():\n    #standardized_district = ['standardized_district']\n    if standardized_district not in geonames_districts:\n        #print('---------')\n        if standardized_district.startswith('Powiat'):\n            standardized_district = standardized_district[7:]\n        #print(district)\n        capitalized = standardized_district.capitalize()\n        lowercase = standardized_district.lower()\n        candidate1, similarity1 = find_the_most_similar(capitalized, geonames_districts)\n        candidate2, similarity2 = find_the_most_similar(lowercase, geonames_districts)\n        if similarity1 > similarity2:\n            districts_map[district] = candidate1\n            #print('\\t', candidate1, similarity1)\n        elif similarity2 > similarity1:\n            districts_map[district] = candidate2\n            #print('\\t', candidate2, similarity2)\n        else:\n            # Break the ties by mapping to the shorter string\n            if len(candidate1) < len(candidate2):\n                districts_map[district] = candidate1\n                #print('\\t', candidate1, '|', candidate2, similarity1)\n            else:\n                districts_map[district] = candidate2\n                #print('\\t', candidate2, '|', candidate1, similarity2)\n\n# Apply the override to PL_re_df\nPL_re_df['standardized_district'] = PL_re_df['district'].apply(lambda district: districts_map[district])\n\n# Show the results\nPL_re_df[['district', 'standardized_district']].drop_duplicates()\n\n\"\"\"\nExplanation: We can now apply a heuristic method for finding the corresponding name in the GeoNames data. It is based on similarity between strings. It turns out that it works fine, except for a couple of cases, which we deal with manually.\nEnd of explanation\n\"\"\"\n\n\n# Clear the mappings for wołowski, Nowy Sącz, rzeszowski, hojnowski.\nfor district in ['wołowski', 'm. Nowy Sącz', 'rzeszowski', 'hojnowski']:\n    districts_map[district] = ''\n    PL_re_df.loc[PL_re_df['district'] == district, 'standardized_district'] = ''\n\n# For each mapping, select a postcode from the GeoNames data\ndf_dict = {'original' : [], 'geonames' : []}\n\nfor original_name in districts_map:\n    geonames_name = districts_map[original_name]\n    df_dict['original'].append(original_name)\n    df_dict['geonames'].append(geonames_name)\n    \nmapping_df = pd.DataFrame.from_dict(df_dict)\n\n# To make sure that the selected postcodes do appear in the NUTS table,\n# we drop, from PL_geo, all rows with the postcodes not in the postcode-to-NUTS table for Poland.\nPL_table = nuts_converter.open_postcode2nuts(filepaths['Eurostat'])['CODE']\n\nPL_geo = pd.merge(PL_geo, PL_table, how='inner', left_on='postcode', right_on='CODE')\nPL_geo.drop(['CODE'], axis='columns', inplace=True)\n\n#\nmerged = pd.merge(mapping_df,\n                  PL_geo[['admin_name2', 'postcode']],\n                  how='left',\n                  left_on='geonames',\n                  right_on='admin_name2')\n\n# Rename the column postcode to make its meaning straightforward\nmerged.rename(columns={'postcode' : 'random_postcode'}, inplace=True)\nmerged = merged.drop_duplicates(['geonames'])\n\nprint(PL_re_df.shape)\nPL_re_df = pd.merge(PL_re_df,\n                     merged[['geonames', 'random_postcode']],\n                     how='left',\n                     left_on='standardized_district',\n                     right_on='geonames')\n\n# Show results\nPL_re_df.head(2)\n\n\"\"\"\nExplanation: The following districts have not been mapped correctly: wołowski, m. Nowy Sącz and rzeszowski. Let us clear their mappings so that we can assign them their NUTS codes manually later.\nEnd of explanation\n\"\"\"\n\n\ndisplay(PL_re_df[PL_re_df['random_postcode'].isnull()])\nPL_re_df['random_postcode'].isnull().sum()\n\n\"\"\"\nExplanation: Show the rows for which we could not find postcodes.\nEnd of explanation\n\"\"\"\n\n\nPL_postcode2nuts_path = filepaths['Eurostat']\n\nPL_re_df = nuts_converter.add_nuts_information(PL_re_df, 'PL', PL_postcode2nuts_path,\n                                               postcode_column='random_postcode', how=['postcode'])\n\n# Report the number of facilites whose NUTS codes were successfully sudetermined\ndetermined = PL_re_df['nuts_1_region'].notnull().sum()\nprint('NUTS successfully determined for', determined, 'out of', PL_re_df.shape[0], 'facilities in PL.')\n\n# Manual assignments\nmanual_nuts3_map = {\n    'wołowski' : 'PL518',\n    'm. Nowy Sącz' : 'PL218',\n    'rzeszowski' : 'PL325'\n}\n\nfor district in manual_nuts3_map:\n    nuts3 = manual_nuts3_map[district]\n    nuts2 = nuts3[:-1]\n    nuts1 = nuts3[:-2]\n    mask = (PL_re_df['district'] == district)\n    PL_re_df.loc[mask, ['nuts_1_region', 'nuts_2_region', 'nuts_3_region']] = [nuts1, nuts2, nuts3]\n\n# Report the number of facilites whose NUTS codes could not be determined\nnot_determined = PL_re_df['nuts_1_region'].isnull().sum()\nprint('NUTS could not be determined for', not_determined, 'out of', PL_re_df.shape[0], 'facilities in PL.')\n\n\"\"\"\nExplanation: There are only 17 such power plants and all of them are placed in the districts which we deliberately left out for manual classification.\nAdd NUTS information\nWe add the NUTS information as usual, using the converter. After that, we manually add the codes for the left-out districts as follows:\n| District | NUTS_1 | NUTS_2 | NUTS_3 |\n|----------|--------|--------|--------|\n| wołowski | PL5 | PL51 | PL518 |\n| m. Nowy Sącz | PL2 | PL21 | PL218 |\n| rzeszowski | PL3 | PL32 | PL325 |\nEnd of explanation\n\"\"\"\n\n\nPL_re_df['data_source'] = 'Urzad Regulacji Energetyki'\nPL_re_df['as_of_year'] = 2019 # The year for which the dataset has been compiled by the data source\n\n\"\"\"\nExplanation: Add data source and year\nEnd of explanation\n\"\"\"\n\n\n# Choose which column to keep\nPL_re_df = PL_re_df.loc[:, [ 'URE_id', 'region', 'district', \n                             'nuts_1_region', 'nuts_2_region', 'nuts_3_region',\n                             'electrical_capacity', \n                             'energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3', \n                             'technology',\n                             'data_source', 'as_of_year']]\n\n\"\"\"\nExplanation: Select columns\nEnd of explanation\n\"\"\"\n\n\nPL_re_df.to_pickle('intermediate/PL_renewables.pickle')\ndel PL_re_df\n\n\"\"\"\nExplanation: Save\nEnd of explanation\n\"\"\"\n\n\n# Download the data and get the local paths of the downloaded files\nfilepaths = downloader.download_data_for_country('CH')\nCH_re_filepath = filepaths['BFE']\nCH_geo_filepath = filepaths['Geonames']   \nCH_postcode2nuts_filepath = filepaths['Eurostat']\n\n# Get data of renewables per municipality\nCH_re_df = pd.read_excel(CH_re_filepath,\n                         sheet_name='KEV Bezüger 2018',\n                         encoding='UTF8',\n                         thousands='.',\n                         decimals=','\n                         #header=[0]\n                         #skipfooter=9,  # contains summarized values\n                         #index_col=[0, 1], # required for MultiIndex\n                         #converters={'Code officiel géographique':str}\n                         )\n\n\"\"\"\nExplanation: Switzerland CH\nDownload and read\nThe data which will be processed below is provided by the following data sources:\nSwiss Federal Office of Energy - Data of all renewable power plants receiving \"Kostendeckende Einspeisevergütung\" (KEV) which is the Swiss feed in tarif for renewable power plants. \nGeodata is based on municipality codes.\nThe available municipality code in the original data provides an approximation for the geocoordinates of the renewable power plants. The postcode will be assigned to latitude and longitude coordinates with the help of the postcode table.\ngeonames.org - The postcode  data from Switzerland is provided by Geonames and licensed under a Creative Commons Attribution 3.0 license.\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for Switzerland, create dictionary and show dictionary\nidx_CH = columnnames[columnnames['country'] == 'CH'].index\ncolumn_dict_CH = columnnames.loc[idx_CH].set_index('original_name')['opsd_name'].to_dict()\ncolumn_dict_CH\n\n# Translate columnnames\nCH_re_df.columns = [column_name.replace(\"\\n\", \"\") for column_name in CH_re_df.columns]\nCH_re_df.rename(columns=column_dict_CH, inplace=True)\n\n\"\"\"\nExplanation: Translate column names\nEnd of explanation\n\"\"\"\n\n\nCH_re_df['data_source'] = 'BFE'\n\n\"\"\"\nExplanation: Add data source\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for Switzerland, create dictionary \nidx_CH = valuenames[valuenames['country'] == 'CH'].index\nvalue_dict_CH = valuenames.loc[idx_CH].set_index('original_name')['opsd_name'].to_dict()\n\n\"\"\"\nExplanation: Harmonize energy source hierarchy and translate values\nEnd of explanation\n\"\"\"\n\n\n# Assign energy_source_level_1 to the dataframe\nCH_re_df['energy_source_level_1'] = 'Renewable energy'\n\n# Create dictionnary in order to assign energy_source to its subtype\n#energy_source_dict_CH = valuenames.loc[idx_CH].set_index('opsd_name')['energy_source_level_2'].to_dict()\n#\n# ...and the energy source subtype values in the energy_source column are replaced by \n# the higher level classification\n#CH_re_df['energy_source_level_2'].replace(energy_source_dict_CH, inplace=True)\n\nCH_re_df['energy_source_level_3'] = CH_re_df['technology']\n\n# Create dictionnary in order to assign energy_source_level_2 to its subtype\nidx_CH = valuenames[valuenames['country'] == 'CH'].index\nenergy_source_dict_CH = valuenames.loc[idx_CH].set_index('original_name')['energy_source_level_2'].to_dict()\n\n# Add energy_source_level_2\nCH_re_df['energy_source_level_2'] = CH_re_df['energy_source_level_2'].replace(energy_source_dict_CH)\n\n# Translate values in order to standardize energy_source_level_3\nvalue_dict_CH = valuenames.loc[idx_CH].set_index('original_name')['opsd_name'].to_dict()\n\n\nCH_re_df['energy_source_level_3'].replace(value_dict_CH, inplace=True)\n\n# Standardize the values for technology\n# 1. np.nan means that technology should not be specified for the respective kind of sources\n#    according to the hierarchy (http://open-power-system-data.org/2016-10-25-opsd_tree.svg)\n# 2. 'Other or unspecified technology' means that technology should be specified\n#    but it was unclear or missing in the original dataset.\ntechnology_translation_dictionary = {\n    'Klärgasanlage': np.nan,\n    'Dampfprozess': 'Steam turbine',\n    'übrige Biomasse - WKK-Anlage': 'Other or unspecified technology',\n    'übrige Biomasse - Dampfprozess': 'Steam turbine',\n    'Schlammverbrennungsanlage': 'Combustion engine',\n    'WKK-Prozess': 'Other or unspecified technology',\n    'Kehrrichtverbrennungsanlage': 'Combustion engine',\n    'Integrierte Anlage': 'Photovoltaics',\n    'Angebaute Anlage': 'Photovoltaics',\n    'Freistehende Anlage': 'Photovoltaics',\n    'Trinkwasserkraftwerk': 'Other or unspecified technology',\n    'Durchlaufkraftwerk': 'Run-of-river',\n    'Dotierwasserkraftwerk': 'Other or unspecified technology',\n    'Ausleitkraftwerk': 'Other or unspecified technology',\n    'Wind Offshore': 'Other or unspecified technology',\n    'Abwasserkraftwerk': 'Other or unspecified technology',\n    'Unbekannt': 'Other or unspecified technology',\n    np.nan: 'Onshore',\n    None: 'Onshore'\n}\n\nCH_re_df['technology'].replace(technology_translation_dictionary, inplace=True)\n\n# Add energy_source_level_1\nCH_re_df['energy_source_level_1'] = 'Renewable energy'\n\n# Show the hierarchy of sources present in the dataset\nenergy_columns = ['energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3', 'technology']\nCH_re_df[energy_columns].drop_duplicates().sort_values(by='energy_source_level_2')\n\n\"\"\"\nExplanation: Separate and assign energy source level 1-3 and technology\nEnd of explanation\n\"\"\"\n\n\ndrop_mask = (CH_re_df['energy_source_level_3'] == 'Biomass and biogas') & \\\n            (CH_re_df['technology'] == 'Steam turbine')\ndrop_indices = drop_mask[drop_mask].index\nCH_re_df.drop(drop_indices, axis='index', inplace=True)\n\nCH_re_df.reset_index(drop=True, inplace=True)\n\n\"\"\"\nExplanation: The power plants with energy_source_level_3=Biomass and biogas and technology=Steam turbine do not belong to the renewable energy power plants, so we can remove them.\nEnd of explanation\n\"\"\"\n\n\nCH_re_df.replace(value_dict_CH, inplace=True)\n\n\"\"\"\nExplanation: Replace the rest of the original terms with their OPSD equivalents\nEnd of explanation\n\"\"\"\n\n\n# Get geo-information\nzip_CH_geo = zipfile.ZipFile(CH_geo_filepath)\n\n# Read generated postcode/location file\nCH_geo = pd.read_csv(zip_CH_geo.open('CH.txt'), sep='\\t', header=None)\n\n# add column names as defined in associated readme file\nCH_geo.columns = ['country_code', 'postcode', 'place_name', 'admin_name1',\n                  'admin_code1', 'admin_name2', 'admin_code2', 'admin_name3',\n                  'admin_code3', 'lat', 'lon', 'accuracy']\n\n# Drop rows of possible duplicate postal_code\nCH_geo.drop_duplicates('postcode', keep='last', inplace=True)\nCH_geo['postcode'] = CH_geo['postcode'].astype(str)\n\n# harmonise data class \nCH_geo.postcode = CH_geo.postcode.astype(int)\n\n# Add longitude/latitude infomation assigned by municipality code\nCH_re_df = pd.merge(CH_re_df,\n                    CH_geo[['lat', 'lon', 'postcode']],\n                    left_on='municipality_code',\n                    right_on='postcode',\n                    how='left'\n                   )\n\nzip_CH_geo.close()\n\n\"\"\"\nExplanation: Georeferencing\nPostcode to lat/lon (WGS84)\nEnd of explanation\n\"\"\"\n\n\nCH_postcode2nuts_path = filepaths['Eurostat']\n\n# Use the string versions of postcode and municipality code columns\nCH_re_df['postcode_str'] = CH_re_df['postcode'].astype(str).str[:-2]\nCH_re_df['municipality_code_str'] = CH_re_df['municipality_code'].astype(str)\n\nCH_re_df = nuts_converter.add_nuts_information(CH_re_df, 'CH', CH_postcode2nuts_path, \n                                                postcode_column='postcode_str',\n                                                municipality_code_column='municipality_code_str',\n                                                lau_name_type='NATIONAL', how=['postcode', 'municipality'])\n\n# Report the number of facilites whose NUTS codes were successfully sudetermined\ndetermined = CH_re_df['nuts_1_region'].notnull().sum()\nprint('NUTS successfully determined for', determined, 'out of', CH_re_df.shape[0], 'facilities in CH.')\n\n# Report the number of facilites whose NUTS codes could not be determined\nnot_determined = CH_re_df['nuts_1_region'].isnull().sum()\nprint('NUTS could not be determined for', not_determined, 'out of', CH_re_df.shape[0], 'facilities in CH.')\n\n\"\"\"\nExplanation: Add NUTS information\nEnd of explanation\n\"\"\"\n\n\nCH_re_df[CH_re_df['nuts_1_region'].isnull()][['postcode', 'municipality']]\n\n# Check the facilities without NUTS classification\nno_nuts = CH_re_df['nuts_1_region'].isnull()\n\n# Find the masks where some information for finding the proper NUTS code is present\nmunicipality_name_present = ~(CH_re_df['municipality'].isnull())\n\n# Show the cases where NUTS classification failed even though it shouldn't have\nproblematic_municipality_names = CH_re_df[no_nuts & municipality_name_present]['municipality'].unique()\nprint('Problematic municipalities:', ', '.join(list(problematic_municipality_names)) + '.')\n\nprint('Are those names present in the official NUTS tables for CH?')\nif nuts_converter.municipality2nuts_df['municipality'].isin(problematic_municipality_names).any():\n    print('At least one is.')\nelse:\n    print('No, none is.')\n\n\"\"\"\nExplanation: Let us check the stations for which NUTS codes could not be determined.\nEnd of explanation\n\"\"\"\n\n\n# kW to MW\nCH_re_df['electrical_capacity'] /= 1000\n\n# kWh to MWh\nCH_re_df['production'] /= 1000\n\n\"\"\"\nExplanation: We see that the municipalities of only plants for which we could not determine the NUTS codes cannot be found in the official translation tables, so there was no possibility to assign them their NUTS classification codes.\nTransform electrical_capacity from kW to MW\nEnd of explanation\n\"\"\"\n\n\ncolumns_to_keep = ['project_name', 'energy_source_level_2','energy_source_level_3', 'technology', \n                   'electrical_capacity', 'production', 'tariff', 'commissioning_date', 'contract_period_end',\n                   'address', 'municipality_code', 'municipality', 'nuts_1_region', 'nuts_2_region',\n                   'nuts_3_region', 'canton', 'company', 'title', 'surname', 'first_name', 'data_source',\n                   'energy_source_level_1', 'lat', 'lon', 'postcode']\nCH_re_df = CH_re_df.loc[:, columns_to_keep]\nCH_re_df.reset_index(drop=True, inplace=True)\n\n\"\"\"\nExplanation: Select columns to keep\nEnd of explanation\n\"\"\"\n\n\nvisualize_points(CH_re_df['lat'],\n                 CH_re_df['lon'],\n                 'Switzerland',\n                 categories=CH_re_df['energy_source_level_2']\n)\n\n\"\"\"\nExplanation: Visualize\nEnd of explanation\n\"\"\"\n\n\nCH_re_df.to_pickle('intermediate/CH_renewables.pickle')\ndel CH_re_df\n\n\"\"\"\nExplanation: Save\nEnd of explanation\n\"\"\"\n\n\n# Download the data and get the local paths to the corresponding files\nfilepaths = downloader.download_data_for_country('UK')\nUK_re_filepath = filepaths['BEIS']\nUK_geo_filepath = filepaths['Geonames']\nUK_postcode2nuts_filepath = filepaths['Eurostat']\n\n# Read the renewable powerplants data into a dataframe\nUK_re_df = pd.read_csv(UK_re_filepath,\n                       header=2,\n                       encoding='latin1',\n                       parse_dates=['Record Last Updated (dd/mm/yyyy)','Operational'],\n                       infer_datetime_format=True,\n                       thousands=','\n                       )\n\n# Drop empty columns and rows\nUK_re_df.dropna(axis='index', how='all', inplace=True)\nUK_re_df.dropna(axis='columns', how='all', inplace=True)\n\n\"\"\"\nExplanation: Check and validation of the renewable power plants list as well as the creation of CSV/XLSX/SQLite files can be found in Part 2 of this script. It also generates a daily time series of cumulated installed capacities by energy source.\nUnited Kingdom UK\nThe data for the UK are provided by the following sources:\nUK Government Department of Business, Energy & Industrial Strategy (BEIS) - the data contain information on the UK renewable energy sources and are updated at the end of each quarter.\ngeonames.org - the data about latitued and longitudes of  the UK postcodes.\nDownload and Read\nEnd of explanation\n\"\"\"\n\n\n# Keep only operational facilities in the dataset\nUK_re_df = UK_re_df.loc[UK_re_df[\"Development Status\"] == \"Operational\"]\nUK_re_df.reset_index(inplace=True, drop=True)\n\n# Standardize string columns\nstrip_and_lower = ['CHP Enabled']\nstrip_only = ['Country', 'County', 'Operator (or Applicant)', 'Mounting Type for Solar']\n\nfor column in strip_and_lower:\n    util.helper.standardize_column(UK_re_df, column, lower=True)\n\nfor column in strip_only:\n    util.helper.standardize_column(UK_re_df, column, lower=False)\n\n# Drop Flywheels, Battery and Liquid Air Energy Storage\nUK_re_df = UK_re_df[~UK_re_df['Technology Type'].isin(['Flywheels', 'Battery', 'Liquid Air Energy Storage'])]\nUK_re_df.reset_index(drop=True, inplace=True)\n\n# Copy the column \"Technology Type\" to a new column named \"technology\"\nUK_re_df['technology'] = UK_re_df['Technology Type']\n\n\"\"\"\nExplanation: Clean the data\nThe downloaded dataset has to be cleaned:\n  - Both operational and nonoperational facilities are present in the set. However, only operational facilities are of the interest, so the dataset has to be filtered on this condition. \n  - Some columns don't have standardized values. For example, CHP Enabled contains five different strings: \"No\", \"Yes\", \"no\", \"yes\", and \"No \" with a trailing white space, even though they represent only two distinct values. So, we have to ensure a 1-to-1 mapping between the true values of a feature and their representations for all the features present in the set.\n  - The technologies Battery, Flywheels and Liquid Air Energy Storage are of no interest, so the facilities using them should be omitted.\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for the UK and create the translation dictionary\nidx_UK = columnnames[columnnames['country'] == 'UK'].index\ncolumn_dict_UK = columnnames.loc[idx_UK].set_index('original_name')['opsd_name'].to_dict()\n\n# Show the dictionary\ncolumn_dict_UK\n\n# Translate column names\nUK_re_df.rename(columns=column_dict_UK, inplace=True)\n\n\"\"\"\nExplanation: Translate column names\nEnd of explanation\n\"\"\"\n\n\nUK_re_df['data_source'] = 'BEIS'\n\n\"\"\"\nExplanation: Add data source\nEnd of explanation\n\"\"\"\n\n\n# Create dictionnary in order to assign energy_source_level_2 to its subtype\nidx_UK = valuenames[valuenames['country'] == 'UK'].index\nenergy_source_dict_UK = valuenames.loc[idx_UK].set_index('original_name')['energy_source_level_2'].to_dict()\n\n# Add energy_source_level_2\nUK_re_df['energy_source_level_2'] = UK_re_df['energy_source_level_3'].replace(energy_source_dict_UK)\n\n# Translate values in order to standardize energy_source_level_3\nvalue_dict_UK = valuenames.loc[idx_UK].set_index('original_name')['opsd_name'].to_dict()\nUK_re_df['energy_source_level_3'].replace(value_dict_UK, inplace=True)\n\n# Standardize the values for technology\n# 1. np.nan means that technology should not be specified for the respective kind of sources\n#    according to the hierarchy (http://open-power-system-data.org/2016-10-25-opsd_tree.svg)\n# 2. 'Other or unspecified technology' means that technology should be specified\n#    but it was unclear or missing in the original dataset.\ntechnology_translation_dictionary = {\n    'Biomass (co-firing)': 'Other or unspecified technology',\n    'Biomass (dedicated)': 'Other or unspecified technology',\n    'Advanced Conversion Technologies': 'Other or unspecified technology',\n    'Anaerobic Digestion': 'Other or unspecified technology',\n    'EfW Incineration': np.nan,\n    'Large Hydro': 'Other or unspecified technology',\n    'Small Hydro': 'Other or unspecified technology',\n    'Landfill Gas': np.nan,\n    'Solar Photovoltaics': 'Photovoltaics',\n    'Sewage Sludge Digestion': np.nan,\n    'Tidal Barrage and Tidal Stream': np.nan,\n    'Shoreline Wave': np.nan,\n    'Wind Offshore': 'Offshore',\n    'Wind Onshore': 'Onshore',\n    'Pumped Storage Hydroelectricity': 'Pumped storage'\n}\nUK_re_df['technology'].replace(technology_translation_dictionary, inplace=True)\n\n# Add energy_source_level_1\nUK_re_df['energy_source_level_1'] = 'Renewable energy'\n\n# Show the hierarchy of sources present in the dataset\nUK_re_df[['energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3', 'technology']].drop_duplicates()\n\n\"\"\"\nExplanation: Translate values and harmonise energy source levels 1-3 and technology\nEnd of explanation\n\"\"\"\n\n\n# Define a wrapper for bng_to_latlon for handling None values \ndef to_lat_lon(easting, northing):\n    if pd.isnull(easting) or pd.isnull(northing):\n        return (None, None)\n    else:\n        return bng_to_latlon.OSGB36toWGS84(easting, northing)\n\n# Convert easting and northing columns to numbers\nUK_re_df['X-coordinate'] = pd.to_numeric(\n                             UK_re_df['X-coordinate'].astype(str).str.replace(',', ''),\n                             errors='coerce'\n                           )\nUK_re_df['Y-coordinate'] = pd.to_numeric(\n                             UK_re_df['Y-coordinate'].astype(str).str.replace(',', ''),\n                             errors='coerce'\n                           )\n\n# Convert easting and northing coordinates to standard latitude and longitude\nlatlon = UK_re_df.apply(lambda row: to_lat_lon(row[\"X-coordinate\"], row[\"Y-coordinate\"]),\n                        axis=1\n                       )\n\n# Split a column of (latitude, longitude) pairs into two separate coordinate columns\nlatitude = latlon.apply(lambda x: x[0])\nlongitude = latlon.apply(lambda x: x[1])\n\n# Add them to the dataframe\nUK_re_df['latitude'] = latitude\nUK_re_df['longitude'] = longitude\n\n\"\"\"\nExplanation: Georeferencing\nThe facilities' location details comprise of the information on the address, county, region, country (England, Scotland, Wales, Northern Ireland), post code, and Easting (X) and Northing (Y) coordinates of each facility in the OSGB georeferencing system. To convert the easting and northing cordinates to standard WG84 latitude and longitude, we use package bng_latlon.\nEnd of explanation\n\"\"\"\n\n\n# Get geo-information\nzip_UK_geo = zipfile.ZipFile(UK_geo_filepath)\n\n# Read generated postcode/location file\nUK_geo = pd.read_csv(zip_UK_geo.open('GB_full.txt'), sep='\\t', header=None)\n\n# add column names as defined in associated readme file\nUK_geo.columns = ['country_code', 'postcode', 'place_name', 'admin_name1',\n                  'admin_code1', 'admin_name2', 'admin_code2', 'admin_name3',\n                  'admin_code3', 'lat', 'lon', 'accuracy']\n\n# Drop rows of possible duplicate postal_code\nUK_geo.drop_duplicates('postcode', keep='last', inplace=True)\nUK_geo['postcode'] = UK_geo['postcode'].astype(str)\n\n# Find the rows where latitude and longitude are unknown\nmissing_latlon_mask = UK_re_df['latitude'].isna() | UK_re_df['longitude'].isna()\nmissing_latlon = UK_re_df[missing_latlon_mask]\n\n# Add longitude/latitude infomation assigned by post code\nupdated_latlon = pd.merge(missing_latlon,\n                  UK_geo[['lat', 'lon', 'postcode']],\n                  left_on='postcode',\n                  right_on='postcode',\n                  how='left'\n                 )\n\n# Return the updated rows to the original frame\nUK_re_df = pd.merge(UK_re_df,\n             updated_latlon[['uk_beis_id', 'lat', 'lon']],\n             on='uk_beis_id',\n             how='left'\n)\n\n# Use the bng_to_latlon coordinates (columns: 'latitude' and 'longitude') if present, \n# otherwise, use those obtained with UK_geo (columns: 'lat' and 'lon').\nUK_re_df['longitude'] = UK_re_df.apply(lambda row: row['longitude'] if not pd.isnull(row['longitude']) \n                                                                    else row['lon'],\n                         axis=1\n                        )\nUK_re_df['latitude'] = UK_re_df.apply(lambda row: row['latitude'] if not pd.isnull(row['latitude']) \n                                                                  else row['lat'],\n                         axis=1\n                        )\n\n# Drop the UK_geo columns (lat/lon)\n# as the information was moved to the 'latitude' and 'longitude' columns.\nUK_re_df.drop(['lat', 'lon'], axis='columns', inplace=True)\n\nzip_UK_geo.close()\n\n\"\"\"\nExplanation: Cases with unknown Easting and Northing coordinates\nIf the Easting and Northing coordinates of a facility are not provided, its latitude and longitude cannot be determined. For such sources, we look up the WGS84 coordinates in the geodataset provided by geonames.org, where the UK postcodes are paired with their latitudes and longitudes.\nEnd of explanation\n\"\"\"\n\n\n# Find the rows where latitude and longitude are unknown\nmissing_latlon_mask = UK_re_df['latitude'].isna() | UK_re_df['longitude'].isna()\nmissing_latlon = UK_re_df[missing_latlon_mask].copy()\nmissing_latlon = missing_latlon.reset_index()\n\n# Determine their post code prefixes\nprefixes = missing_latlon.apply(lambda row: str(row['postcode']).split(' ')[0],\n                                axis=1\n                               )\nmissing_latlon['Prefix'] = prefixes\n\n# Determine the centroids of the areas covered by the prefixes\ngrouped_UK_geo=UK_geo.groupby(by=lambda i: str(UK_geo['postcode'].loc[i]).split(' ')[0])\n\n# Assing the centroid coordinates to the facilities with unknown coordinates\nupdated_latlon = pd.merge(missing_latlon,\n                    grouped_UK_geo.mean(),\n                    left_on=\"Prefix\",\n                    right_index=True,\n                    how=\"left\"\n                   )\n\n# Return the updated rows to the original frame\nUK_re_df = pd.merge(UK_re_df,\n             updated_latlon[['uk_beis_id', 'lat', 'lon']],\n             on='uk_beis_id',\n             how='left'\n)\n\n# Keep the already known coordinates (columns: 'latitude' and 'longitude') if present, \n# otherwise, use those obtained by approximation (columns: 'lat' and 'lon').\nUK_re_df['longitude'] = UK_re_df.apply(lambda row: row['longitude'] if not pd.isnull(row['longitude']) \n                                                                    else row['lon'],\n                         axis=1\n                        )\nUK_re_df['latitude'] = UK_re_df.apply(lambda row: row['latitude'] if not pd.isnull(row['latitude']) \n                                                                  else row['lat'],\n                         axis=1\n                        )\n\n# Drop the UK_geo columns (lat/lon)\n# as the information was moved to the 'latitude' and 'longitude' columns.\nUK_re_df.drop(['lat', 'lon'], axis='columns', inplace=True)\n\n\"\"\"\nExplanation: Cases for approximation\nIn the cases where the full post code was not present in geonames.org, use its prefix to find the latitude / longitude pairs of locations covered by that prefix. Then, approximate those facilities' locations by the centroids of their prefix areas.\nEnd of explanation\n\"\"\"\n\n\nUK_postcode2nuts_filepath = filepaths['Eurostat']\n\nUK_re_df = nuts_converter.add_nuts_information(UK_re_df, 'UK', UK_postcode2nuts_filepath, \n                                                latitude_column='latitude',\n                                                longitude_column='longitude', closest_approximation=True,\n                                                lau_name_type='NATIONAL', how=['latlon', 'municipality'])\n\n# Report the number of facilites whose NUTS codes were successfully sudetermined\ndetermined = UK_re_df['nuts_1_region'].notnull().sum()\nprint('NUTS successfully determined for', determined, 'out of', UK_re_df.shape[0], 'facilities in UK.')\n\n# Report the number of facilites whose NUTS codes could not be determined\nnot_determined = UK_re_df['nuts_1_region'].isnull().sum()\nprint('NUTS could not be determined for', not_determined, 'out of', UK_re_df.shape[0], 'facilities in UK.')\n\n\"\"\"\nExplanation: Add NUTS information\nEnd of explanation\n\"\"\"\n\n\nUK_re_df[UK_re_df['nuts_1_region'].isnull()]\n\n\"\"\"\nExplanation: Let us see the facilities for which the NUTS codes could not be determined.\nEnd of explanation\n\"\"\"\n\n\nvisualize_points(UK_re_df['latitude'],\n                 UK_re_df['longitude'],\n                 'United Kingdom',\n                 categories=UK_re_df['energy_source_level_2']\n)\n\n\"\"\"\nExplanation: There are two such rows only. The langitude and longitude coordinates, as well as municipality codes, are missing from the data set, so NUTS codes could not have been determined.\nVisualize the data\nEnd of explanation\n\"\"\"\n\n\nmax_X = UK_re_df['X-coordinate'].max()\nmin_X = UK_re_df['X-coordinate'].min()\n\nmax_Y = UK_re_df['Y-coordinate'].max()\nmin_Y = UK_re_df['Y-coordinate'].min()\n\nfigure(num=None, figsize=(8, 6), dpi=100, facecolor='w', edgecolor='k')\nax = plt.axes(projection=ccrs.OSGB())\nax.coastlines('10m')\n\nax.scatter(UK_re_df['X-coordinate'], UK_re_df['Y-coordinate'],s=0.5)\nplt.show()\n\n\"\"\"\nExplanation: We see that some facilities appear to be located in the sea. Let us plot the original OSGB coordinates to see if translation to the standard longitude and latitude coordinates failed for some locations.\nEnd of explanation\n\"\"\"\n\n\n# Rename 'longitude' and 'latitude' to 'lon' and 'lat' to conform to the naming convention\n# used for other countries.\nUK_re_df.rename(columns={'longitude': 'lon', 'latitude': 'lat'}, inplace=True)\n\n\n# Define the columns to keep\ncolumns_of_interest = ['commissioning_date', 'uk_beis_id', 'operator', 'site_name',\n                       'energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3', 'technology',\n                       'electrical_capacity', 'chp', 'support_robranding', 'support_fit', 'support_cfd',\n                       'capacity_individual_turbine', 'number_of_turbines', 'solar_mounting_type',\n                       'status', 'address', 'municipality', 'nuts_1_region', 'nuts_2_region', 'nuts_3_region',\n                       'region', 'country', 'postcode', 'lon', 'lat', 'data_source'\n                      ]\n\nfor col in columns_of_interest:\n    if col not in UK_re_df.columns:\n        print(col)\n\n# Clean the dataframe from columns other than those specified above\nUK_re_df = UK_re_df.loc[:, columns_of_interest]\nUK_re_df.reset_index(drop=True, inplace=True)\nUK_re_df.columns\n\n\"\"\"\nExplanation: As we can see, the maps are basically the same, which confirms that translation to the longitude and latitude coordinates is done correctly and that they reflect the positions specified by the original X and Y OSGB coordinates. \nKeep only the columns of interest\nEnd of explanation\n\"\"\"\n\n\nUK_re_df.to_pickle('intermediate/UK_renewables.pickle')\n\n\"\"\"\nExplanation: Save\nEnd of explanation\n\"\"\"\n\n\n# Download the data and get the local paths to the corresponding files\nfilepaths = downloader.download_data_for_country('SE')\nprint(filepaths)\n\nSE_re_filepath = filepaths['Vindbrukskollen']\nSE_geo_filepath = filepaths['Geonames']\nSE_postcode2nuts_filepath = filepaths['Eurostat']\n\n\"\"\"\nExplanation: Sweden\nThe data for Sweden are provided by the following sources:\n\nVindbrukskollen - Wind farms in Sweden.\nEnd of explanation\n\"\"\"\n\n\n# Define the function for converting the column \"Senast sparads\" to date type\n#def from_int_to_date(int_date):\n#    print(int_date)\n#    str_date =str(int_date)\n#    year = str_date[:4]\n#    month = str_date[4:6]\n#    day = str_date[6:8]\n#    str_date = '{}/{}/{}'.format(year, month, day)\n#    return pd.to_datetime(str_date, format='%Y/%m/%d')\n    \n# Read the data\nSE_re_df = pd.read_excel(SE_re_filepath,\n                        sheet_name='Vindkraftverk',\n                        na_values='-',\n                        parse_dates=['Uppfört', 'Senast sparad'],\n                        infer_datetime_format=True,\n                        #converters={'Senast sparad' : from_int_to_date}\n                        )\n\n# Show 5 rows from the beginning\nSE_re_df.head(5)\n\n\"\"\"\nExplanation: Load the data\nEnd of explanation\n\"\"\"\n\n\n# Drop empty rows and columns \nSE_re_df.dropna(axis='index', how='all', inplace=True)\nSE_re_df.dropna(axis='columns', how='all', inplace=True)\n\n# Make sure that the column Uppfört is of the date type and correctly formatted\nSE_re_df['Uppfört'] = pd.to_datetime(SE_re_df['Uppfört'], format='%Y-%m-%d')\n\n# Keep only operational wind farms\nsubset_mask = SE_re_df['Status'].isin(['Beviljat', 'Uppfört'])\nSE_re_df.drop(SE_re_df[~subset_mask].index, axis='index', inplace=True)\n\n# Remove the farms whose capacity is not known.\nsubset_mask = SE_re_df['Maxeffekt (MW)'].isna()\nSE_re_df.drop(SE_re_df[subset_mask].index, axis='index', inplace=True)\n\n# Standardize string columns\nstring_columns = ['Modell', 'Fabrikat', 'Elområde', 'Kommun', 'Län', 'Handlingstyp', 'Placering']\nfor col in string_columns:\n    util.helper.standardize_column(SE_re_df, col, lower=False)\n\n\"\"\"\nExplanation: Clean the data\n\nDrop empty rows and columns.\nMake sure that the column Uppfört is of the date type.\nKeep only operational wind farms (Status is Beviljat (permission granted) or Uppfört (the farm exists)).\nRemove the farms whose capacity is not known.\nStandardize string columns.\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for the UK and create the translation dictionary\nidx_SE = columnnames[columnnames['country'] == 'SE'].index\ncolumn_dict_SE = columnnames.loc[idx_SE].set_index('original_name')['opsd_name'].to_dict()\n\n# Show the dictionary\ndisplay(column_dict_SE)\n\n# Translate column names\nSE_re_df.rename(columns=column_dict_SE, inplace=True)\n\n\"\"\"\nExplanation: Translate column names\nEnd of explanation\n\"\"\"\n\n\nSE_re_df.loc[(SE_re_df['commissioning_date'].dt.year == 1900), 'commissioning_date'] = np.nan\n\n\"\"\"\nExplanation: Correct the dates\nSome wind farms are declared to be commissioned in the year 1900. We set those dates to np.nan.\nEnd of explanation\n\"\"\"\n\n\nSE_re_df['data_source'] = 'Vindbrukskollen'\n\n\"\"\"\nExplanation: Add source\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for Sweden\nidx_SE = valuenames[valuenames['country'] == 'SE'].index\nvalue_dict_SE = valuenames.loc[idx_SE].set_index('original_name')['opsd_name'].to_dict()\nvalue_dict_SE\n\n# Replace all original value names by the OPSD value names\nSE_re_df.replace(value_dict_SE, inplace=True)\n\n# Set nans in the technology column to 'Unknown or unspecified technology'\nSE_re_df['technology'].fillna('Unknown or unspecified technology', inplace=True)\n\n# Add energy level 2\nSE_re_df['energy_source_level_2'] = 'Wind'\n\n# Add energy_source_level_1\nSE_re_df['energy_source_level_1'] = 'Renewable energy'\n\n# Show the hierarchy of sources present in the dataset\nSE_re_df[['energy_source_level_1', 'energy_source_level_2', 'technology']].drop_duplicates()\n\n\"\"\"\nExplanation: Translate values and harmonize energy source levels\nEnd of explanation\n\"\"\"\n\n\n# Get latitude and longitude columns\nlat, lon = util.helper.sweref99tm_latlon_transform(SE_re_df['sweref99tm_north'], SE_re_df['sweref99tm_east'])\n\n# Include them in the dataframe\nSE_re_df['lat'] = lat\nSE_re_df['lon'] = lon\n\n\"\"\"\nExplanation: Georeferencing\nThe coordinates in the columns sweref99tm_north and sweref99tm_east are specified in the SWEREF 99 TM coordinate system, used in Sweden. To convert those coordinates to the usual WGS84 latitudes and longitudes, we use the function sweref99tm_latlon_transform from the module util.helper, provided by Jon Olauson.\nEnd of explanation\n\"\"\"\n\n\nSE_postcode2nuts_filepath = filepaths['Eurostat']\n\nSE_re_df = nuts_converter.add_nuts_information(SE_re_df, 'SE', SE_postcode2nuts_filepath, \n                                                lau_name_type='NATIONAL', how=['municipality', 'latlon'])\n\n# Report the number of facilites whose NUTS codes were successfully sudetermined\ndetermined = SE_re_df['nuts_1_region'].notnull().sum()\nprint('NUTS successfully determined for', determined, 'out of', SE_re_df.shape[0], 'facilities in SE.')\n\n# Report the number of facilites whose NUTS codes could not be determined\nnot_determined = SE_re_df['nuts_1_region'].isnull().sum()\nprint('NUTS could not be determined for', not_determined, 'out of', SE_re_df.shape[0], 'facilities in SE.')\n\n\"\"\"\nExplanation: Assigning NUTS codes\nEnd of explanation\n\"\"\"\n\n\n# Define which columns should be kept\ncolumns_to_keep = ['municipality', 'county', 'nuts_1_region', 'nuts_2_region', 'nuts_3_region', 'lat', 'lon',\n           'energy_source_level_1', 'energy_source_level_2', 'technology', 'se_vindbrukskollen_id',\n            'site_name', 'manufacturer',\n           'electrical_capacity', 'commissioning_date', 'data_source']\n\n# Keep only the selected columns\nSE_re_df = SE_re_df.loc[:, columns_to_keep]\n\n\"\"\"\nExplanation: Select the columns to keep\nEnd of explanation\n\"\"\"\n\n\nvisualize_points(SE_re_df['lat'],\n                 SE_re_df['lon'],\n                 'Sweden',\n                 categories=SE_re_df['technology']\n)\n\n\"\"\"\nExplanation: Visualize\nEnd of explanation\n\"\"\"\n\n\nSE_re_df.reset_index(inplace=True, drop=True)\nSE_re_df.to_pickle('intermediate/SE_renewables.pickle')\n\ndel SE_re_df\n\n\"\"\"\nExplanation: Save\nEnd of explanation\n\"\"\"\n\n\n# Download the data and get the local paths to the corresponding files\nprint('Start:', datetime.datetime.now())\ndownloader = Downloader(version, input_directory_path, source_list_filepath, download_from)\nfilepaths = downloader.download_data_for_country('CZ')\nprint('End:', datetime.datetime.now())\n\nCZ_re_filepath = filepaths['ERU']\nCZ_geo_filepath = filepaths['Geonames']\nCZ_postcode2nuts_filepath = filepaths['Eurostat']\n\n# Define a converter for CZ postcode strings\ndef to_cz_postcode_format(postcode_str):\n    return postcode_str[:3] + ' ' + postcode_str[3:]\n\n# Read the data from the csv file\nCZ_re_df = pd.read_csv(CZ_re_filepath,\n                       escapechar='\\\\',\n                       dtype = {\n                           'number_of_sources' : int,\n                       },\n                       parse_dates=['licence_approval_date'],\n                       infer_datetime_format=True,\n                       converters = {\n                           'site_postcode' : to_cz_postcode_format,\n                           'holder_postcode' : to_cz_postcode_format\n                       }\n                      )\n# Show a few rows\nCZ_re_df.head(5)\n\n\"\"\"\nExplanation: Czech Republic\nThe data for Czech Republic are provided by the following source:\n-  ERU (Energetický regulační úřad, Energy Regulatory Office) - Administrative authority responsible for regulation in the energy sector. Provides the data on renewable energy plants in Czech Republic.\nDownload and read the data\nDownloading the data from the original source may take 1-2 hours because it's done by scraping the information from HTML pages. \nIf downloading fails because of the ERU's server refusing connections:\n- pause and wait for some time;\n- delete the file eru.csv in the CZ input directory;\n- try downloading again.\nAlternatively, you can download the data from the OPSD server.\nEnd of explanation\n\"\"\"\n\n\nCZ_re_df.dtypes\n\n\"\"\"\nExplanation: Let's inspect the dataframe's columns:\nEnd of explanation\n\"\"\"\n\n\nmwe_columns = [col for col in CZ_re_df.columns if 'megawatts_electric' in col and col != 'megawatts_electric_total']\nmwt_columns = [col for col in CZ_re_df.columns if 'megawatts_thermal' in col and col != 'megawatts_thermal_total']\n\ndef count_types(row):\n    global mwe_columns\n    different_types = sum([row[col] > 0 for col in mwe_columns])\n    return different_types\n\nCZ_re_df.apply(count_types, axis=1).value_counts()\n\n\"\"\"\nExplanation: It contains 30 columns:\n- site_name, site_region, site_postcode, site_locality, site_district give us basic information on the site;\n- megawatts_electric_total shows us the total electric capacity of the site;\n- Since each site can use different types of energy, megawatts_electric_hydro, megawatts_electric_solar, megawatts_electric_biogas_and_biomass, megawatts_electric_wind, megawatts_electric_unspecified show us how total capacity breaks down to those renewable types from the OPSD energy hierarchy;\n- The columns beginning with megawatts_thermal_ represent the amiunt of input energy required (and will be equal to zero in most cases);\n- watercourse and watercourse_length_km represent the name and length of the watercourse used by the site (if any);\n- holder_name, holder_region, holder_address, holder_postcode, holder_locality, holder_district, holder_representative give us basic information on the site's owner;\n- licence_number and licence_approval_date show us the licence number given to the holder and its approval date.\n- link points to the ERU page with the site's data in HTML.\nSince some sites use conventional types of energy, it is possible that megawatts_electric_total &gt; megawatts_electric_hydro + megawatts_electric_solar + megawatts_electric_biogas_and_biomass + megawatts_electric_wind + megawatts_electric_unspecified. If the sum of renewable-energy capacities is equal to zero, that means that the correspoding row actually represents a conventional powerplant, so it should be excluded.\nLet us now check how many sites use how many types of renewable energy sources.\nEnd of explanation\n\"\"\"\n\n\n# Drop empty columns and rows\nCZ_re_df.dropna(axis='index', how='all', inplace=True)\nCZ_re_df.dropna(axis='columns', how='all', inplace=True)\n\n# Drop rows with no data on electrical capacity and the rows where total electrical capacity is 0\nempty_mask = (CZ_re_df['megawatts_electric_total'] == 0) | (CZ_re_df['megawatts_electric_total'].isnull())\nCZ_re_df = CZ_re_df.loc[~empty_mask]\nCZ_re_df.reset_index(inplace=True, drop=True)\n\n# Replace NANs with zeroes in mwe and mwt columns\nreplacement_dict = {col : 0 for col in mwe_columns + mwt_columns}\nCZ_re_df.fillna(replacement_dict, inplace=True)\n\n# Drop the rows where renewable-energy share of the total capacity is equal to zero\nconventional_mask = (CZ_re_df['megawatts_electric_hydro'] +\n                     CZ_re_df['megawatts_electric_solar'] +\n                     CZ_re_df['megawatts_electric_biogas_and_biomass'] + \n                     CZ_re_df['megawatts_electric_wind'] + \n                     CZ_re_df['megawatts_electric_unspecified']) == 0\n\nCZ_re_df = CZ_re_df.loc[~conventional_mask]\nCZ_re_df.reset_index(inplace=True, drop=True)\n\n\"\"\"\nExplanation: As of April 2020, as we can see in the output above, there are only 4 sites which use more than one type of renewable energy, and there are 193 sites which do not use renewable energy at all.\nClean the data\nEnd of explanation\n\"\"\"\n\n\n# Define the function which will extract the data about the type of energy specified by the given column\n# and return it as a dataframe in the \"long format\"\ndef select_and_reformat(df, column):\n    # Use the mwe and mwt columns defined above\n    global mwe_columns\n    global mwt_columns\n    \n    # Declare the given column and its mwt counterpart as exceptions\n    mwt_exception = column.replace('electric', 'thermal')\n    exceptions = [column, mwt_exception]\n\n    # Exclude all the mwe and mwt columns which do not correspond to the given energy type\n    columns_to_skip = [col for col in mwe_columns + mwt_columns if col not in exceptions]\n    # Keep all the other columns\n    columns_to_keep = [col for col in df.columns if col not in columns_to_skip]\n    \n    # Find the stations which use the given type of energy\n    selection_mask = (df[column] > 0)\n    \n    # Keep them and select the columns we decided to keep\n    selection_df = df[selection_mask][columns_to_keep]\n    \n    # Create a new column which will indicate the energy type\n    selection_df['energy_type'] = \" \".join(column.split('_')[2:])\n    \n    # Remove the energy type name from the columns representing electrical capacity\n    # and megawatts thermal\n    selection_df.rename(columns = {column : 'electrical_capacity',\n                                   mwt_exception : 'megawatts_thermal'},\n                        inplace=True)\n    selection_df.drop(columns=['megawatts_electric_total', 'megawatts_thermal_total'],\n                     inplace=True)\n    \n    # Ensure the rows are properly indexed as 0,1,2,...\n    selection_df.reset_index(inplace=True, drop=True)\n    \n    return selection_df\n\n# Create a dataframe for each energy type\ndataframes = []\nfor column in mwe_columns:\n    selection = select_and_reformat(CZ_re_df, column)\n    energy_type = selection['energy_type'].unique()[0]\n    dataframes.append(selection)\n\n# Concatenate the dataframes\nCZ_re_df = pd.concat(dataframes, ignore_index=False)\nCZ_re_df.reset_index(inplace=True, drop=True)\n\n\"\"\"\nExplanation: Reformat the data\nThere are sites which use different types of renewable source to produce electric energy. Those are the sites where at least two of the following columns are not equal to zero: megawatts_electric_hydro, megawatts_electric_solar, megawatts_electric_biogas_and_biomass, megawatts_electric_wind, megawatts_electric_unspecified. The data that come in this shape are said to be in the so called wide format. For the purpose of our later processing, it would be more convenient to have the data where each row is associated to one and only one type of energy (the so called long format). Therefore, we must first restructure our data from the wide to long format.\nEnd of explanation\n\"\"\"\n\n\nCZ_re_df\n\n\"\"\"\nExplanation: Let us see what is this restructured dataframe like.\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for CZ and create the translation dictionary\nidx_CZ = columnnames[columnnames['country'] == 'CZ'].index\ncolumn_dict_CZ = columnnames.loc[idx_CZ].set_index('original_name')['opsd_name'].to_dict()\n\n# Show the dictionary\ncolumn_dict_CZ\n\n# Translate column names\nCZ_re_df.rename(columns=column_dict_CZ, inplace=True)\n\n\"\"\"\nExplanation: The number of columns has been reduced as we have transformed the data to the long format. The rows representning conventional power plants have been excluded. Since only few sites use multiple types of energy, the total number of rows has not increased.\nTranslate column names\nEnd of explanation\n\"\"\"\n\n\n# Choose the translation terms for Czech Republic\nidx_CZ = valuenames[valuenames['country'] == 'CZ'].index\n\n# Choose the translation terms for energy source level 3\nenergy3_dict_CZ = valuenames.loc[idx_CZ].set_index('original_name')['opsd_name'].to_dict()\nenergy3_dict_CZ\n\n# Add energy source level 3\nCZ_re_df['energy_source_level_3'] = CZ_re_df['technology'].replace(energy3_dict_CZ)\n\n# Choose the terms for energy source level 2\nenergy2_dict_CZ = valuenames.loc[idx_CZ].set_index('original_name')['energy_source_level_2'].to_dict()\nCZ_re_df['energy_source_level_2'] = CZ_re_df['technology'].replace(energy2_dict_CZ)\n\n# Standardize the values for technology\n# 1. np.nan means that technology should not be specified for the respective kind of sources\n#    according to the hierarchy (http://open-power-system-data.org/2016-10-25-opsd_tree.svg)\n# 2. 'Other or unspecified technology' means that technology should be specified\n#    but it was unclear or missing in the original dataset.\ntechnology_dict = {\n    'biogas and biomass' : np.nan,\n    'wind' : 'Onshore',\n    'solar' : 'Other or unspecified technology',\n    'hydro' : 'Run-of-river',\n    'unspecified' : np.nan\n}\nCZ_re_df['technology'] = CZ_re_df['technology'].replace(technology_dict)\n\n# Add energy_source_level_1\nCZ_re_df['energy_source_level_1'] = 'Renewable energy'\n\n# Show the hierarchy of sources present in the dataset\nCZ_re_df[['energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3', 'technology']].drop_duplicates()\n\n\"\"\"\nExplanation: Translate values and harmonize energy levels\nEnd of explanation\n\"\"\"\n\n\nCZ_re_df['data_source'] = 'ERU'\n\n\"\"\"\nExplanation: Add data source\nEnd of explanation\n\"\"\"\n\n\n# Get geo-information\nzip_CZ_geo = zipfile.ZipFile(CZ_geo_filepath)\n\n# Read generated postcode/location file\nCZ_geo = pd.read_csv(zip_CZ_geo.open('CZ.txt'), sep='\\t', header=None)\n\n# add column names as defined in associated readme file\nCZ_geo.columns = ['country_code', 'postcode', 'place_name', 'admin_name1',\n                  'admin_code1', 'admin_name2', 'admin_code2', 'admin_name3',\n                  'admin_code3', 'lat', 'lon', 'accuracy']\n\n# Drop rows of possible duplicate postal_code\nCZ_geo.drop_duplicates('postcode', keep='last', inplace=True)\n\n# Add longitude/latitude infomation assigned by postcode\nCZ_re_df = pd.merge(CZ_re_df,\n                    CZ_geo[['lat', 'lon', 'postcode']],\n                    left_on='postcode',\n                    right_on='postcode',\n                    how='left'\n                   )\n\n\"\"\"\nExplanation: Georeferencing\nEnd of explanation\n\"\"\"\n\n\nCZ_postcode2nuts_filepath = filepaths['Eurostat']\n\nCZ_re_df = nuts_converter.add_nuts_information(CZ_re_df, 'CZ', CZ_postcode2nuts_filepath, how=['postcode'])\n\n# Report the number of facilites whose NUTS codes were successfully determined\ndetermined = CZ_re_df['nuts_1_region'].notnull().sum()\nprint('NUTS successfully determined for', determined, 'out of', CZ_re_df.shape[0], 'facilities in CZ.')\n\n# Report the number of facilites whose NUTS codes could not be determined\nnot_determined = CZ_re_df['nuts_1_region'].isnull().sum()\nprint('NUTS could not be determined for', not_determined, 'out of', CZ_re_df.shape[0], 'facilities in CZ.')\n\n\"\"\"\nExplanation: Assign NUTS codes\nEnd of explanation\n\"\"\"\n\n\n# Define which columns should be kept\ncolumns_to_keep = ['site_name', 'region', 'municipality', 'locality', 'postcode',\n                   'nuts_1_region', 'nuts_2_region', 'nuts_3_region', 'lat', 'lon',\n                   'energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3', 'technology', \n                   'owner', 'electrical_capacity',  'data_source']\n\n# Keep only the selected columns\nCZ_re_df = CZ_re_df.loc[:, columns_to_keep]\n\n\"\"\"\nExplanation: Select the columns to keep\nEnd of explanation\n\"\"\"\n\n\nCZ_re_df.drop_duplicates(inplace=True)\nCZ_re_df.reset_index(drop=True, inplace=True)\n\n\"\"\"\nExplanation: Drop duplicates\nEnd of explanation\n\"\"\"\n\n\nvisualize_points(CZ_re_df['lat'],\n                 CZ_re_df['lon'],\n                 'Czechia',\n                 categories=CZ_re_df['energy_source_level_2']\n)\n\n\"\"\"\nExplanation: Visualuze\nEnd of explanation\n\"\"\"\n\n\nCZ_re_df.reset_index(inplace=True, drop=True)\nCZ_re_df.to_pickle('intermediate/CZ_renewables.pickle')\ndel CZ_re_df\n\n\"\"\"\nExplanation: Save\nEnd of explanation\n\"\"\"\n\n\nzip_archive = zipfile.ZipFile(input_directory_path + '.zip', 'w', zipfile.ZIP_DEFLATED)\nprint(\"Zipping the raw files...\")\nfor filename in os.listdir(input_directory_path):\n    print(\"Adding\", filename, \"to the zip.\")\n    filepath = os.path.join(input_directory_path, filename)\n    zip_archive.write(filepath)\nzip_archive.close()\nprint(\"Done!\")\n#shutil.rmtree(input_directory_path)\n\n\"\"\"\nExplanation: Zip the raw data\nEnd of explanation\n\"\"\"\n\nimport numpy as np\nimport tensorflow as tf\n\n\"\"\"\nExplanation: Fizz Buzz with Tensor Flow.\nThis notebook to explain the code from Fizz Buzz in Tensor Flow blog post written by Joel Grus\nYou should read his post first it is super funny!  \nHis code try to play the Fizz Buzz game by using machine learning. \nThis notebook is for real beginners who whant to understand the basis of TensorFlow by reading code.\nFeedback welcome @dh7net\nLet's start!\nThe code contain several part:\n* Create the training set\n  * Encode the input (a number)\n  * Encode the result (fizz or buzz, none or both?)\n  * create the training set\n* Build a model\n* Train the model\n  * Create a cost function\n  * Iterate\n* Make prediction\nEnd of explanation\n\"\"\"\n\n\nNUM_DIGITS = 10\n\ndef binary_encode(i, num_digits):\n    return np.array([i >> d & 1 for d in range(num_digits)])\n\n#Let's check if it works\nfor i in range(10):\n    print i, binary_encode(i, NUM_DIGITS)\n\n\"\"\"\nExplanation: Create the trainning set\nEncode the input (a number)\nThis example convert the number to a binary representation\nEnd of explanation\n\"\"\"\n\n\ndef fizz_buzz_encode(i):\n    if   i % 15 == 0: return np.array([0, 0, 0, 1])\n    elif i % 5  == 0: return np.array([0, 0, 1, 0])\n    elif i % 3  == 0: return np.array([0, 1, 0, 0])\n    else:             return np.array([1, 0, 0, 0])\n    \ndef fizz_buzz(i, prediction):\n    return [str(i), \"fizz\", \"buzz\", \"fizzbuzz\"][prediction]\n    \n# let'see how the encoding works\nfor i in range(1, 16):\n    print i, fizz_buzz_encode(i)\n\n# and the decoding\nfor i in range(1, 16):\n    fizz_or_buzz_number = np.argmax(fizz_buzz_encode(i))\n    print i, fizz_or_buzz_number, fizz_buzz(i, fizz_or_buzz_number)\n\n\"\"\"\nExplanation: Encode the result (fizz or buzz, none or both?)\n\nThe fizz_buzz function calculate what the output should be, an encoded it to a 4 dimention vector.  \nThe fizz_buzz function take a number and a prediction, and output a string\nEnd of explanation\n\"\"\"\n\n\ntraining_size = 2 ** NUM_DIGITS\nprint \"Size of the set:\", training_size\ntrX = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, training_size)])\ntrY = np.array([fizz_buzz_encode(i)          for i in range(101, training_size)])\n\nprint \"First 15 values:\"\nfor i in range(101, 116):\n    print i, trX[i], trY[i]\n\n\"\"\"\nExplanation: Create the training set\nEnd of explanation\n\"\"\"\n\n\ndef init_weights(shape):\n    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n\n\"\"\"\nExplanation: Creation of the model\nThe model is made of:\n* one hidden layer that contains 100 neurons\n* one output layer  \nThe input is fully connected to the hidden layer and a relu function is applyed\nThe relu function is a rectifier that just output zero if the input is negative.\nFirst we'll define an helper function to initialise parameters with randoms values\nEnd of explanation\n\"\"\"\n\n\nNUM_HIDDEN = 100 #Number of neuron in the hidden layer\n\nX = tf.placeholder(\"float\", [None, NUM_DIGITS])\nY = tf.placeholder(\"float\", [None, 4])\n\nw_h = init_weights([NUM_DIGITS, NUM_HIDDEN])\nw_o = init_weights([NUM_HIDDEN, 4])\n\n\"\"\"\nExplanation: X is the input\nY is the output\nw_h are the parameters between the input and the hidden layer\nw_o are the parameters between the hidden layer and the output\nEnd of explanation\n\"\"\"\n\n\ndef model(X, w_h, w_o):\n    h = tf.nn.relu(tf.matmul(X, w_h))\n    return tf.matmul(h, w_o)\n\npy_x = model(X, w_h, w_o)\n\n\"\"\"\nExplanation: To create the model we apply the w_h parameters to the input,\nand then we aply the relu function to calculate the value of the hidden layer.\nThe w_o coeefient are used to calculate the output layer. No rectification is applyed\npy_x is the predicted value for a given input represented as a vector (dimention 4)\nEnd of explanation\n\"\"\"\n\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(py_x, Y))\n\n\"\"\"\nExplanation: Training\nCreate the cost function\nThe cost function measure how bad the model is.\nIt is the distance between the prediction (py_x) and the reality (Y).\nEnd of explanation\n\"\"\"\n\n\ntrain_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost)\n\n\"\"\"\nExplanation: softmax_cross_entropy_with_logits(py_x, Y) measure the distance between py_x and Y. SoftMax is the classical way to measure the distance between a predicted result and the actual result in a cost function.  \nreduce_mean calculate the mean of a tensor. In this case the mean of the distance for the whole training set\n\nTrain the model\nTraining a model in TensorFlow is extremly simple, you just define a trainer operator!\nEnd of explanation\n\"\"\"\n\n\npredict_op = tf.argmax(py_x, 1)\n\n\"\"\"\nExplanation: This operator will minimize the cost using the Gradient Descent witch is the most common optimizer to find parameters than will minimise the cost.\nWe'll also define a prediction operator that will be able to output a prediction.\n* 0 means no fizz no buzz\n* 1 means fizz\n* 2 means buzz\n* 3 means fizzbuzz\nEnd of explanation\n\"\"\"\n\n\nBATCH_SIZE = 128\n\n\"\"\"\nExplanation: Iterate until the model is good enough\nOne epoch consists of one full training cycle on the training set.\nOnce every sample in the set is seen, you start again - marking the beginning of the 2nd epoch. source  \nThe training set is randomly permuted between each epoch.\nThe learning is not done on the full set at once.\nInstead the learning set is divided in small batch and the learning is done for each of them.\nEnd of explanation\n\"\"\"\n\n\n#random permutation of the index will be used during the training for each epoch\npermutation_index = np.random.permutation(range(len(trX)))\nfor start in range(0, len(trX), BATCH_SIZE):\n        end = start + BATCH_SIZE\n        print \"Batch starting at\", start\n        print permutation_index[start:end]\n\n\n# Launch the graph in a session\nsess = tf.Session()\ntf.initialize_all_variables().run(session=sess)\n\nfor epoch in range(5000):\n    # Shuffle the data before each training iteration.\n    p = np.random.permutation(range(len(trX)))\n    trX, trY = trX[p], trY[p]\n\n    # Train in batches of 128 inputs.\n    for start in range(0, len(trX), BATCH_SIZE):\n        end = start + BATCH_SIZE\n        sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n\n    # And print the current accuracy on the training data.\n    if (epoch%100==0):  # each 100 epoch, to not overflow the jupyter log\n        # np.mean(A==B) return a number between 0 and 1. (true_count/total_count)\n        print(epoch, np.mean(np.argmax(trY, axis=1) ==\n                         sess.run(predict_op, feed_dict={X: trX, Y: trY})))\n\n\n# And now for some fizz buzz\nnumbers = np.arange(1, 101)\nteX = np.transpose(binary_encode(numbers, NUM_DIGITS))\nteY = sess.run(predict_op, feed_dict={X: teX})\n\noutput = np.vectorize(fizz_buzz)(numbers, teY)\nprint output\n\nsess.close() # don't forget to close the session if you don't use it anymore. Or use the *with* statement.\n\n# Lets check the quality\nY = np.array([fizz_buzz_encode(i) for i in range(1,101)])\nprint \"accuracy\", np.mean(np.argmax(Y, axis=1) == teY)\n\nfor i in range(1,100):\n    actual = fizz_buzz(i, np.argmax(fizz_buzz_encode(i)))\n    predicted = output[i-1]\n    ok = True\n    if actual <> predicted: ok = False\n    print i, \"{:>8}\".format(actual), \"{:>8}\".format(predicted), ok\n\n\"\"\"\nExplanation: Here an example of index used for one epoch:\nEnd of explanation\n\"\"\"\n\nimport pkg_resources\nif pkg_resources.get_distribution('CGRtools').version.split('.')[:2] != ['4', '0']:\n    print('WARNING. Tutorial was tested on 4.0 version of CGRtools')\nelse:\n    print('Welcome!')\n\n# load data for tutorial\nfrom pickle import load\nfrom traceback import format_exc\n\nwith open('molecules.dat', 'rb') as f:\n    molecules = load(f) # list of MoleculeContainer objects\nwith open('reactions.dat', 'rb') as f:\n    reactions = load(f) # list of ReactionContainer objects\n\nm1, m2, m3 = molecules[:3] # molecule\nm7 = m3.copy()\nm11 = m3.copy()\nm11.standardize()\nm7.standardize()\nr1 = reactions[0] # reaction\nm1.delete_atom(3) \ncgr2 = ~r1\nbenzene = m3.substructure([4,5,6,7,8,9]) \nm3.delete_bond(4, 5)\n\n\"\"\"\nExplanation: 2. Signatures and duplicates selection\n\n(c) 2019, 2020 Dr. Ramil Nugmanov;\n(c) 2019 Dr. Timur Madzhidov; Ravil Mukhametgaleev\n\nInstallation instructions of CGRtools package information and tutorial's files see on https://github.com/stsouko/CGRtools\nNOTE: Tutorial should be performed sequentially from the start. Random cell running will lead to unexpected results.\nEnd of explanation\n\"\"\"\n\n\nms2 = str(m2)  # get and print signature\nprint(ms2)  \n# or \nprint(m2)\n\nhms2 = bytes(m2)  # get sha512 hash of signature as bytes-string\nm2\n\n\"\"\"\nExplanation: 2.1. Molecule Signatures\nMoleculeContainer has methods for unique molecule signature generation.\nSignature is SMILES string with canonical atoms ordering.\nFor signature generation one need to call str function on MoleculeContainer object.\nFixed length hash of signature could be retrieved by calling bytes function on molecule (correspond to SHA 512 bitstring).\nOrder of atoms calculated by Morgan-like algorithm. On initial state for each atoms it's integer code calculated based on its type. All bonds incident to atoms also coded as integers and stored in sorted tuple. Atom code and tuple of it's bonds used for ordering and similar atoms detecting. Ordered atoms rank is replaced with new integer code. Atoms of the same type with the same bonds types incident to it have equal numbers.\nNumbers codes found are used in Morgan algorithm cycle.\nLoop is repeated until all atoms will be unique or number of unique atoms will not change in 3 subsequent loops.\nEnd of explanation\n\"\"\"\n\n\nprint(f'f string {m2}')  # use signature in string formatting\nprint('C-style string %s' % m2)\nprint('format method {}'.format(m2))\n\n\"\"\"\nExplanation: String formatting is supported that is useful for reporting\nEnd of explanation\n\"\"\"\n\n\nmq = m2.substructure(m2, as_query=True)\nprint(f'{mq}')  # get signatures with hybridization and neighbors data\nprint('{:!n}'.format(mq))  # get signature with hybridization only data\n# h - hybridization marks, n- neighbors marks\nprint(format(mq, '!h')) # include only number of neighbors in signature\nprint(f'{mq:!n!h}')  # hide all data\nmq\n\n\"\"\"\nExplanation: For Queries number of neighbors and hybridization will be added to signature. Note that in this case they are not readable as SMILES.\nBut possible to hide this data.\nEnd of explanation\n\"\"\"\n\n\nm1\n\nm2\n\nm1 != m2 # different molecules\n\nm7\n\nm7 == m11 # copy of the same molecule\n\nm7 is m11  # this is not same objects!\n\n# Simplest way to exclude duplicated structures\nlen({m1, m2, m7, m11}) == 3 # create set of unique molecules. Only 3 of them were different.\n\n\"\"\"\nExplanation: Atoms in the QueryContainer and QueryCGRContainer signature are represented in the following way: [isotope;element_symbol;hn;charge;radical state]. h mean hybridization, n - number of neighbors. Notation for hybridization is the following:\ns - all bonds of atom are single\nd - atom has one double bond and others are single\nt - atom has one triple or two double bonds and other are single\na - atom is in aromatic ring\n\nExamples: s1 - atom has s hybridization and one neighbor\nd3 - atom has d hybridization and 3 neighbors\nSignatures for CGRContainer include only radical state marks additionally to common SMILES notation.\nMolecules comparable and hashable\nComparison of MoleculeContainer is based on its signatures. Moreover, since strings in Python are hashable, MoleculeContaier also hashable.\nNOTE: MoleculeContainer can be changed. This can lead to unobvious behavior of the sets and dictionaries in which these molecules were placed before the change. Avoid changing molecules (standardize, aromatize, hydrogens and atoms/bonds changes) placed inside sets and dictionaries.\nEnd of explanation\n\"\"\"\n\n\nstr(r1)\n\n\"\"\"\nExplanation: 2.2. Reaction signatures\nReactionContainer have its signature. Signature is SMIRKS string in which molecules of reactants, reagents, products presented in canonical order.\nAPI is the same as for molecules\nEnd of explanation\n\"\"\"\n\n\nstr(cgr2)\n\ncgr2.clean2d()\ncgr2\n\n\"\"\"\nExplanation: 2.3. CGR signature\nCGRContainer have its signature. Signatures is SMIRKS-like strings where dynamic bond labels and dynamic atoms are also specified within squared brackets, so not only atoms but bonds could be written in brackets if a bond has complex parameters. Dynamic bonds in CGR have special label representing changes in bond orders. Dynamic atom corresponds to a change of formal charge or radical state of atom in reaction. Their labels are also given in brackets, including the atom symbol and text keys for atomic property in reactant and product, separated by symbol >. For a neutral atom A gaining a positive charge +n in reaction dynamic atom will be encoded as [A0>+n]. In case of charges +1 and -1, the number 1 is omitted. Properties for charges and radicals may be combined consecutively within one pair of brackets, e.g. [A0>-^>*] stands for an atom which becomes an anion-radical.\nEnd of explanation\n\"\"\"\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo('GlcnxUlrtek')\n\n\"\"\"\nExplanation: <h1 align = 'center'> Neural Networks Demystified </h1>\n<h2 align = 'center'> Part 4: Backpropagation </h2>\n\n<h4 align = 'center' > @stephencwelch </h4>\nEnd of explanation\n\"\"\"\n\n\n%pylab inline\n#Import code from last time\nfrom partTwo import *\n\ndef sigmoid(z):\n     #Apply sigmoid activation function to scalar, vector, or matrix\n    return 1/(1+np.exp(-z))\n\ndef sigmoidPrime(z):\n    #Derivative of sigmoid function\n    return np.exp(-z)/((1+np.exp(-z))**2)\n\ntestValues = np.arange(-5,5,0.01)\nplot(testValues, sigmoid(testValues), linewidth=2)\nplot(testValues, sigmoidPrime(testValues), linewidth=2)\ngrid(1)\nlegend(['sigmoid', 'sigmoidPrime'])\n\n\"\"\"\nExplanation: <h3 align = 'center'> Variables </h3>\n\n|Code Symbol | Math Symbol | Definition | Dimensions\n| :-: | :-: | :-: | :-: |\n|X|$$X$$|Input Data, each row in an example| (numExamples, inputLayerSize)|\n|y |$$y$$|target data|(numExamples, outputLayerSize)|\n|W1 | $$W^{(1)}$$ | Layer 1 weights | (inputLayerSize, hiddenLayerSize) |\n|W2 | $$W^{(2)}$$ | Layer 2 weights | (hiddenLayerSize, outputLayerSize) |\n|z2 | $$z^{(2)}$$ | Layer 2 activation | (numExamples, hiddenLayerSize) |\n|a2 | $$a^{(2)}$$ | Layer 2 activity | (numExamples, hiddenLayerSize) |\n|z3 | $$z^{(3)}$$ | Layer 3 activation | (numExamples, outputLayerSize) |\n|J | $$J$$ | Cost | (1, outputLayerSize) |\n|dJdz3 | $$\\frac{\\partial J}{\\partial z^{(3)} } = \\delta^{(3)}$$ | Partial derivative of cost with respect to $z^{(3)}$ | (numExamples,outputLayerSize)|\n|dJdW2|$$\\frac{\\partial J}{\\partial W^{(2)}}$$|Partial derivative of cost with respect to $W^{(2)}$|(hiddenLayerSize, outputLayerSize)|\n|dz3dz2|$$\\frac{\\partial z^{(3)}}{\\partial z^{(2)}}$$|Partial derivative of $z^{(3)}$ with respect to $z^{(2)}$|(numExamples, hiddenLayerSize)|\n|dJdW1|$$\\frac{\\partial J}{\\partial W^{(1)}}$$|Partial derivative of cost with respect to $W^{(1)}$|(inputLayerSize, hiddenLayerSize)|\n|delta2|$$\\delta^{(2)}$$|Backpropagating Error 2|(numExamples,hiddenLayerSize)|\n|delta3|$$\\delta^{(3)}$$|Backpropagating Error 1|(numExamples,outputLayerSize)|\nLast time, we decided to use gradient descent to train our Neural Network, so it could make better predictions of your score on a test based on how many hours you slept, and how many hours you studied the night before. To perform gradient descent, we need an equation and some code for our gradient, dJ/dW.  \nOur weights, W, are spread across two matrices, W1 and W2. We’ll separate our dJ/dW computation in the same way, by computing dJdW1 and dJdW2 independently. We should have just as many gradient values as weight values, so when we’re done, our matrices dJdW1 and dJdW2 will be the same size as W1 and W2.\n$$\n\\frac{\\partial J}{\\partial W^{(2)}} = \\frac{\\partial \\sum \\frac{1}{2}(y-\\hat{y})^2}{\\partial W^{(2)}}\n$$\nLet’s work on dJdW2 first. The sum in our cost function adds the error from each example to create our overall cost. We’ll take advantage of the sum rule in differentiation, which says that the derivative of the sums equals the sum of the derivatives. We can move our sigma outside and just worry about the derivative of the inside expression first. \n$$\n\\frac{\\partial J}{\\partial W^{(2)}} = \\sum \\frac{\\partial \\frac{1}{2}(y-\\hat{y})^2}{\\partial W^{(2)}}\n$$\nTo keep things simple, we’ll temporarily forget about our summation. Once we’ve computed dJdW for a single example, we’ll add all our individual derivative terms together. \nWe can now evaluate our derivative. The power rule tells us to bring down our exponent, 2, and multiply. To finish our derivative, we’ll need to apply the chain rule. \nThe chain rule tells us how to take the derivative of a function inside of a function, and generally says we take the derivative of the outside function and then multiply it by the derivative of the inside function. \nOne way to express the chain rule is as the product of derivatives, this will come in very handy as we progress through backpropagation. In fact, a better name for backpropagation might be: don’t stop doing the chain rule. ever. \nWe’ve taken the derivative of the outside of our cost function - now we need to multiply it by the derivative of the inside.\nY is just our test scores, which won’t change, so the derivative of y, a constant, with respect to W two is 0! yHat, on the other hand, does change with respect to W two, so we’ll apply the chain rule and multiply our results by minus dYhat/dW2. \n$$\n\\frac{\\partial J}{\\partial W^{(2)}} = -(y-\\hat{y}) \\frac{\\partial \\hat{y}}{\\partial W^{(2)}}\n$$\nWe now need to think about the derivative of yHat with respect to W2. Equation 4 tells us that yHat is our activation function of z3, so we it will be helpful to apply the chain rule again to break dyHat/dW2 into dyHat/dz3 times dz3/dW2. \n$$\n\\frac{\\partial J}{\\partial W^{(2)}} = \n-(y-\\hat{y})\n\\frac{\\partial \\hat{y}}{\\partial z^{(3)}}\n\\frac{\\partial z^{(3)}}{\\partial W^{(2)}}\n$$\nTo find the rate of change of yHat with respect to z3, we need to differentiate our sigmoid activation function with respect to z. \n$$\nf(z) = \\frac{1}{1+e^{-z}}\n$$\n$$\nf^\\prime(z) = \\frac{e^{-z}}{(1+e^{-z})^2}\n$$\nNow is a good time to add a new python method for the derivative of our sigmoid function, sigmoid Prime. Our derivative should be the largest where our sigmoid function is the steepest, at the value z equals zero.\nEnd of explanation\n\"\"\"\n\n\n# Part of NN Class (won't work alone, needs to be included in class as \n# shown in below and in partFour.py):\n\ndef costFunctionPrime(self, X, y):\n    #Compute derivative with respect to W and W2 for a given X and y:\n    self.yHat = self.forward(X)\n\n    delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n    dJdW2 = np.dot(self.a2.T, delta3)\n\n\"\"\"\nExplanation: We can now replace dyHat/dz3 with f prime of z 3.\n$$\n\\frac{\\partial z^{(3)}}{\\partial W^{(2)}}= \n-(y-\\hat{y}) f^\\prime(z^{(3)}) \\frac{\\partial z^{(3)}}{\\partial W^{(2)}}\n$$\nOur final piece of the puzzle is dz3dW2, this term represents the change of z, our third layer activity, with respect to the weights in the second layer.\nZ three is the matrix product of our activities, a two, and our weights, w two. The activities from layer two are multiplied by their correspond weights and added together to yield z3. If we focus on a single synapse for a moment, we see a simple linear relationship between W and z, where a is the slope. So for each synapse, dz/dW(2) is just the activation, a on that synapse!\n$$\nz^{(3)} = a^{(2)}W^{(2)} \\tag{3}\\\n$$\nAnother way to think about what the calculus is doing here is that it is “backpropagating” the error to each weight, by multiplying by the activity on each synapses, the weights that contribute more to the error will have larger activations, and yield larger dJ/dW2 values, and those weights will be changed more when we perform gradient descent. \nWe need to be careful with our dimensionality here, and if we’re clever, we can take care of that summation we got rid of earlier. \nThe first part of our equation, y minus yHat is of the same dimension as our output data, 3 by 1. \nF prime of z three is of the same size, 3 by 1, and our first operation is scalar multiplication. Our resulting 3 by 1 matrix is referred to as the backpropagating error, delta 3.\nWe determined that dz3/dW2 is equal to the activity of each synapse. Each value in delta 3 needs to be multiplied by each activity. We can achieve this by transposing a2 and matrix multiplying by delta3. \n$$\n\\frac{\\partial J}{\\partial W^{(2)}} = \n(a^{(2)})^T\\delta^{(3)}\\tag{6}\n$$\n$$\n\\delta^{(3)} = -(y-\\hat{y}) f^\\prime(z^{(3)}) \n$$\nWhat’s cool here is that the matrix multiplication also takes care of our earlier omission – it adds up the dJ/dW terms across all our examples. \nAnother way to think about what’s happening here is that is that each example our algorithm sees has a certain cost and a certain gradient. The gradient with respect to each example pulls our gradient descent algorithm in a certain direction. It's like every example gets a vote on which way is downhill, and when we perform batch gradient descent we just add together everyone’s vote, call it downhill, and move in that direction.\nWe’ll code up our gradients in python in a new method, cost function prime. Numpy’s multiply method performs element-wise multiplication, and the dot method performs matrix multiplication.\nEnd of explanation\n\"\"\"\n\n\n# Whole Class with additions:\nclass Neural_Network(object):\n    def __init__(self):        \n        #Define Hyperparameters\n        self.inputLayerSize = 2\n        self.outputLayerSize = 1\n        self.hiddenLayerSize = 3\n        \n        #Weights (parameters)\n        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n        \n    def forward(self, X):\n        #Propogate inputs though network\n        self.z2 = np.dot(X, self.W1)\n        self.a2 = self.sigmoid(self.z2)\n        self.z3 = np.dot(self.a2, self.W2)\n        yHat = self.sigmoid(self.z3) \n        return yHat\n        \n    def sigmoid(self, z):\n        #Apply sigmoid activation function to scalar, vector, or matrix\n        return 1/(1+np.exp(-z))\n    \n    def sigmoidPrime(self,z):\n        #Gradient of sigmoid\n        return np.exp(-z)/((1+np.exp(-z))**2)\n    \n    def costFunction(self, X, y):\n        #Compute cost for given X,y, use weights already stored in class.\n        self.yHat = self.forward(X)\n        J = 0.5*sum((y-self.yHat)**2)\n        return J\n        \n    def costFunctionPrime(self, X, y):\n        #Compute derivative with respect to W and W2 for a given X and y:\n        self.yHat = self.forward(X)\n        \n        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n        dJdW2 = np.dot(self.a2.T, delta3)\n        \n        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n        dJdW1 = np.dot(X.T, delta2)  \n        \n        return dJdW1, dJdW2\n\n\"\"\"\nExplanation: We have one final term to compute: dJ/dW1. The derivation begins the same way, computing the derivative through our final layer: first dJ/dyHat, then dyHat/dz3, and we called these two taken together form our backpropagating error, delta3. We now take the derivative “across” our synapses, this is a little different from out job last time, computing the derivative with respect to the weights on our synapses. \n$$\n\\frac{\\partial J}{\\partial W^{(1)}} = (y-\\hat{y})\n\\frac{\\partial \\hat{y}}{\\partial W^{(1)}}\n$$\n$$\n\\frac{\\partial J}{\\partial W^{(1)}} = (y-\\hat{y})\n\\frac{\\partial \\hat{y}}{\\partial z^{(3)}}\n\\frac{\\partial z^{(3)}}{\\partial W^{(1)}}\n$$\n$$\n\\frac{\\partial J}{\\partial W^{(1)}} = -(y-\\hat{y}) f^\\prime(z^{(3)}) \\frac{\\partial z^{(3)}}{\\partial W^{(1)}}\n$$\n$$\n\\frac{\\partial z^{(3)}}{\\partial W^{(1)}} = \\frac{\\partial z^{(3)}}{\\partial a^{(2)}}\\frac{\\partial a^{(2)}}{\\partial W^{(1)}}\n$$\nThere’s still a nice linear relationship along each synapse, but now we’re interested in the rate of change of z(3) with respect to a(2). Now the slope is just equal to the weight value for that synapse. We can achieve this mathematically by multiplying by W(2) transpose. \n$$\n\\frac{\\partial J}{\\partial W^{(1)}} = \\delta^{(3)} \n(W^{(2)})^{T}\n\\frac{\\partial a^{(2)}}{\\partial W^{(1)}}\n$$\n$$\n\\frac{\\partial J}{\\partial W^{(1)}} = \\delta^{(3)} \n(W^{(2)})^{T}\n\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\n\\frac{\\partial z^{(2)}}{\\partial W^{(1)}}\n$$\nOur next term to work on is da(2)/dz(2) – this step is just like the derivative across our layer 3 neurons, so we can just multiply by f prime(z2). \n$$\n\\frac{\\partial J}{\\partial W^{(1)}} = \\delta^{(3)} \n(W^{(2)})^{T}\nf^\\prime(z^{(2)})\n\\frac{\\partial z^{(2)}}{\\partial W^{(1)}}\n$$\nOur final computation here is dz2/dW1. This is very similar to our dz3/dW2 computation, there is a simple linear relationship on the synapses between z2 and w1, in this case though, the slope is the input value, X. We can use the same technique as last time by multiplying by X transpose, effectively applying the derivative and adding our dJ/dW1’s together across all our examples. \n$$\n\\frac{\\partial J}{\\partial W^{(1)}} = \nX^{T}\n\\delta^{(3)} \n(W^{(2)})^{T}\nf^\\prime(z^{(2)})\n$$\nOr:\n$$\n\\frac{\\partial J}{\\partial W^{(1)}} = \nX^{T}\\delta^{(2)} \\tag{7}\n$$\nWhere:\n$$\n\\delta^{(2)} = \\delta^{(3)} \n(W^{(2)})^{T}\nf^\\prime(z^{(2)})\n$$\nAll that’s left is to code this equation up in python. What’s cool here is that if we want to make a deeper neural network, we could just stack a bunch of these operations together.\nEnd of explanation\n\"\"\"\n\n\nNN = Neural_Network()\n\ncost1 = NN.costFunction(X,y)\n\ndJdW1, dJdW2 = NN.costFunctionPrime(X,y)\n\ndJdW1\n\ndJdW2\n\n\"\"\"\nExplanation: So how should we change our W’s to decrease our cost? We can now compute dJ/dW, which tells us which way is uphill in our 9 dimensional optimization space.\nEnd of explanation\n\"\"\"\n\n\nscalar = 3\nNN.W1 = NN.W1 + scalar*dJdW1\nNN.W2 = NN.W2 + scalar*dJdW2\ncost2 = NN.costFunction(X,y)\n\nprint cost1, cost2\n\ndJdW1, dJdW2 = NN.costFunctionPrime(X,y)\nNN.W1 = NN.W1 - scalar*dJdW1\nNN.W2 = NN.W2 - scalar*dJdW2\ncost3 = NN.costFunction(X, y)\n\nprint cost2, cost3\n\n\"\"\"\nExplanation: If we move this way by adding a scalar times our derivative to our weights, our cost will increase, and if we do the opposite, subtract our gradient from our weights, we will move downhill and reduce our cost. This simple step downhill is the core of gradient descent and a key part of how even very sophisticated learning algorithms are trained.\nEnd of explanation\n\"\"\"\n\n#Importing basic libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nimport matplotlib.pyplot as plt # To plot graphs\nfrom sklearn.metrics import accuracy_score # To test accuracy\nfrom sklearn import tree\n\nchurn=pd.read_csv(\"../Datasets/Cell2Cell_data.csv\")\n\n#We set the training data size to 80% and the remaining to the test set. (trainsize)\ntrainsize=0.8\n#Setting seed to reproduce results if needed\nnp.random.seed(3462)\n\n#Using numpy's random number generator to generate numbers between 0 and 1. We select values less than the training size which is set to 80%\nindx=np.random.rand(len(churn))<trainsize\ntrain_churn=churn[indx]\ntest_churn=churn[~indx]\n\n\"\"\"\nExplanation: Introduction to Data Science\nHomework 3 Solutions\nStudent Name: Shaival Dalal\nStudent Netid: sd3462\n\nPart 1 - Preparing a Training Set and Training a Decision Tree (Total 10 Points)\nThis is a hands-on task where we build a predictive model using Decision Trees discussed in class. For this part, we will be using the data in cell2cell_data.csv (you can find this on NYU Classes).\nThese historical data consist of 39,859 customers: 19,901 customers that churned (i.e., left the company) and 19,958 that did not churn (see the \"churndep\" variable). Here are the data set's 11 possible predictor variables for churning behavior: \n```\nPos.  Var. Name  Var. Description\n\n1     revenue    Mean monthly revenue in dollars\n2     outcalls   Mean number of outbound voice calls\n3     incalls    Mean number of inbound voice calls\n4     months     Months in Service\n5     eqpdays    Number of days the customer has had his/her current equipment\n6     webcap     Handset is web capable\n7     marryyes   Married (1=Yes; 0=No)\n8     travel     Has traveled to non-US country (1=Yes; 0=No)\n9     pcown      Owns a personal computer (1=Yes; 0=No)\n10    creditcd   Possesses a credit card (1=Yes; 0=No)\n11    retcalls   Number of calls previously made to retention team\n```\nThe 12th column, the dependent variable \"churndep\", equals 1 if the customer churned, and 0 otherwise. \n1. Load the data and prepare it for modeling. Note that the features are already processed for you, so the only thing needed here is split the data into training and testing. Use pandas to create two data frames: train_df and test_df, where train_df has 80% of the data chosen uniformly at random without replacement (test_df should have the other 20%). Also, make sure to write your own code to do the splits. You may use any random() function numpy but do not use the data splitting functions from Sklearn.<br><br>\n(2 Points)\nEnd of explanation\n\"\"\"\n\n\nfeatures_train=train_churn.loc[:,'revenue':'retcalls']\ntarget_train=train_churn.loc[:,'churndep']\ndtree=DecisionTreeClassifier(criterion=\"entropy\")\ntrained=dtree.fit(features_train,target_train)\n\n\"\"\"\nExplanation: 2. If we had to, how would we prove to ourselves or a colleague that our data was indeed randomly sampled on X? And by prove, I mean empirically, not just showing this person our code. Don't actually do the work, just describe in your own words a test you could here. Hint: think about this in terms of selection bias and use notes from our 2nd lecture.\n(1 Point)\n<span style=\"color:blue\">Answer</span>\nWe use the random number generator provided by an external library called NumPy. By using this library we generate unique random numbers which are free from selection bias. We can safely assume that selection bias is non-existent\n3. Now build and train a decision tree classifier using DecisionTreeClassifier() (manual page) on train_df to predict the \"churndep\" target variable. Make sure to use criterion='entropy' when instantiating an instance of DecisionTreeClassifier(). For all other settings you should use all of the default options.\n(1 Point)\nEnd of explanation\n\"\"\"\n\n\nfeaturelength=np.arange(len(list(features_train)))\nnames=list(features_train)\nimportances=pd.DataFrame({\"Features\":list(features_train),\"Importance\":trained.feature_importances_})\nimportances.sort_values(by='Importance',ascending=False,inplace=True)\n\nplt.figure(figsize=(10,5))\nplt.title(\"Feature Importance\")\n\nplt.bar(featurelength,importances[\"Importance\"],align=\"center\",color=\"blue\")\nplt.xticks(featurelength,importances[\"Features\"],rotation=\"60\")\nplt.xlabel('Features')\nplt.ylabel('Importance')\n\n## We can alternatively use a vertical bar plot to represent information ##\n'''\nplt.barh(featurelength,importances[\"Importance\"],align=\"center\",color=\"blue\")\nplt.yticks(featurelength,importances[\"Features\"])\nplt.ylabel('Features')\nplt.xlabel('Importance')\n'''\n\nplt.show()\n\n\"\"\"\nExplanation: 4. Using the resulting model from 2.3, show a bar plot of feature names and their feature importance (hint: check the attributes of the DecisionTreeClassifier() object directly in IPython or check the manual!).\n(3 Points)\nEnd of explanation\n\"\"\"\n\n\ncolumn_names=list(churn[importances[:3][\"Features\"]])\ncolumn_names.extend([\"churndep\"])\nchurn[column_names].corr()\n\n\"\"\"\nExplanation: 5. Is the relationship between the top 3 most important features (as measured here) negative or positive? If your marketing director asked you to explain the top 3 drivers of churn, how would you interpret the relationship between these 3 features and the churn outcome?  What \"real-life\" connection can you draw between each variable and churn?\n(2 Points)\nEnd of explanation\n\"\"\"\n\n\n# Splitting test dataset into target and features\nfeatures_test=test_churn.loc[:,'revenue':'retcalls']\ntarget_test=test_churn.loc[:,'churndep']\n\n#  Predicting target for train and test dataset\nresults_test=trained.predict(features_test)\nresults_train=trained.predict(features_train)\n\ntest_accuracy=accuracy_score(target_test,results_test)\ntrain_accuracy=accuracy_score(target_train,results_train)\nprint(\"Accuracy for the test dataset is %.3f%% and accuracy for the training dataset is %.3f%%\" %(test_accuracy*100,train_accuracy*100))\n\n\"\"\"\nExplanation: The top 3 features are revenue, eqpdays and outcalls. \n\nRevenue and Eqpdays are negatively correlated. This means that as revenue slightly increases, eqpdays decrease. We can infer that once the customer stop using the current equipment and starts using the new, company provided equipment, the company's revenues will slightly increase. The relationship is weak in nature.\nRevenue and outcalls are positively correlated with moderate strength. This means that as the number of outgoing calls of the customer increase, the revenue of the company increases. \nEqpdays and outcalls are negatively correlated and the relationship is weak in nature. We can infer that as outgoing calls of customers increase, there is a slight decrease in the eqpdays i.e. the number of days the customer owns their current equipment. This means that customers who make more outgoing calls are somewhat likely to buy new equipment from the company than use their current equipment.\n\n\n\nThe top 3 drivers of churn are monthly revenue (revenue), the number of days the customer has had his/her current equipment (eqpdays) and the mean number of outbound voice calls (outcalls).\n\nAs our revenue per customer increases i.e. the more they pay, the less likely it is that they will churn. This can be attributed to customers finding satisfaction with services provided by us and voluntarily using more services provided by us. Although, the impact of revenue on churn is weak and almost insignificant.\nThe longer our customers use their current equipment, the more likely it is that they will churn. We need to ensure that the customer purchases equipment(s) from us in order to lock-in the customer leading to greater customer stickiness. \nThe more our customers make outgoing calls, the less likely it is that they will churn. More outgoing calls indicate successful and willing usage of our services. Satisfied customers are less likely to churn. Like revenue, this relation is also weak and almost insignificant.\n\n\n\n6. Using the classifier built in 2.3, try predicting \"churndep\" on both the train_df and test_df data sets. What is the accuracy on each?\n(1 Point)\nEnd of explanation\n\"\"\"\n\n\n# We can use graphviz to visualise the decision tree which may help us\n# tree.export_graphviz(trained,out_file=\"DecisionTree\")\nsplits=np.arange(10,1000,100)\nleafnodes=np.arange(10,1000,100)\n\n\"\"\"\nExplanation: Part 2 - Finding a Good Decision Tree (Total 10 Points)\nThe default options for your decision tree may not be optimal. We need to analyze whether tuning the parameters can improve the accuracy of the classifier.  For the following options min_samples_split and min_samples_leaf:\n1. Generate a list of 10 values of each for the parameters min_samples_split and min_samples_leaf. \n(1 Point)\nEnd of explanation\n\"\"\"\n\n\ndef DtreeIter(train_features,train_target,test_features,test_target,samplesplit,sampleleaf):\n    treeOpt=DecisionTreeClassifier(criterion=\"entropy\",min_samples_split=samplesplit,min_samples_leaf=sampleleaf)\n    treeOpt=treeOpt.fit(train_features,train_target)\n    result_Opt=treeOpt.predict(test_features)\n    return accuracy_score(test_target,result_Opt)\n\nresult_optimise=dict()\nfor values in splits:\n    result_optimise[values]=list()\n\nfor values in splits:\n    for nodes in leafnodes:\n        result_optimise[values].append([DtreeIter(features_train,target_train,features_test,target_test,values,nodes)])\n\n      \n        \n#To find out best parameters        \noptimal_split=max(result_optimise, key=lambda x: result_optimise[x][1])\noptimal_accuracy=max(result_optimise[optimal_split])\noptimal_leaf=leafnodes[list(result_optimise[optimal_split]).index(optimal_accuracy)]\nprint(\"Optimal 'Sample Split Size' is %d and 'Optimal Leaf Samples' are %d. Best accuracy is %.2f%%\" %(optimal_split,optimal_leaf,optimal_accuracy[0]*100))\n\n\nplt.figure(figsize=(10,5))\nplt.plot(splits,result_optimise[leafnodes[0]],'b',label='Leaf={}'.format(leafnodes[0]))\nplt.plot(splits,result_optimise[leafnodes[1]],'r',label='Leaf={}'.format(leafnodes[1]))\nplt.plot(splits,result_optimise[leafnodes[2]],'y',label='Leaf={}'.format(leafnodes[2]))\nplt.plot(splits,result_optimise[leafnodes[3]],'g',label='Leaf={}'.format(leafnodes[3]))\nplt.plot(splits,result_optimise[leafnodes[4]],'c',label='Leaf={}'.format(leafnodes[4]))\nplt.plot(splits,result_optimise[leafnodes[5]],'m',label='Leaf={}'.format(leafnodes[5]))\nplt.plot(splits,result_optimise[leafnodes[6]],'k',label='Leaf={}'.format(leafnodes[6]))\nplt.plot(splits,result_optimise[leafnodes[7]],'b',label='Leaf={}'.format(leafnodes[7]))\nplt.plot(splits,result_optimise[leafnodes[8]],'r',label='Leaf={}'.format(leafnodes[8]))\nplt.plot(splits,result_optimise[leafnodes[9]],'y',label='Leaf={}'.format(leafnodes[9]))\nplt.legend(loc=4)\nplt.xlabel('Min Sample Splits')\nplt.ylabel('Accuracy')\nplt.title('Classifier Accuracy')\nplt.show()\n\n\"\"\"\nExplanation: 2. Explain in words your reasoning for choosing the above ranges.\n<span style=\"color:Blue\">Answer </span>\nThe model we developed suffers from overfitting as demonstrated by the radical difference in accuracy when run on train and test data set. \n1. We choose a higher min_samples_split as the default value of 2 is too low and clearly causes our model to overfit on the training data. Since the size of the data is too large, we select larger values.\n2. We choose a higher min_samples_leaf as the default value of 1 is too low. However, we don't want the value to be too high as it can result in the model to demonstrate low variance and high bias.\n\n3. For each combination of values in 3.1 (there should be 100), build a new classifier and check the classifier's accuracy on the test data. Plot the test set accuracy for these options. Use the values of min_samples_split as the x-axis and generate a new series (line) for each of min_samples_leaf.\n(5 Points)\nEnd of explanation\n\"\"\"\n\n\nfrom scipy import stats\nfrom statsmodels.graphics.api import qqplot\n\nfever=pd.read_csv(\"../Datasets/cases.csv\")\n# We can directly read and convert using the read_csv function by using the below command:\n# fever=pd.read_csv(\"../Datasets/cases.csv\",parse_dates=[0])\nfever[\"YEAR\"]=pd.to_datetime(fever[\"YEAR\"],format=\"%Y\")\n\n\"\"\"\nExplanation: 4. Which configuration returns the best accuracy? What is this accuracy? (Note, if you don't see much variation in the test set accuracy across values of min_samples_split or min_samples_leaf, try redoing the above steps with a different range of values).\n(1 Point)\n<span style=\"color:Blue\">Answer </span>\nWhen we set the Sample Split size to 710 and the Optimal Leaf Samples to 110, we get the best accuracy of 60.31%\nThis accuracy represents the percentage of times our model predicts the correct output. Values predicted by the model are compared with actual value in the test data set to determine this metric.\n\n5. If you were working for a marketing department, how would you use your churn production model in a real business environment? Explain why churn prediction might be good for the business and how one might improve churn by using this model.\n(2 Points)\n<span style=\"color:Blue\">Answer </span>\nChurn prediction is an extremely important activity for any company. In the marketing department, churn can be of both high performing salesmen as well as customers. \n * By analysing customer churn, a company can take reasonable steps to retain the existing customer. Losing a customer will not only cost the company in lost revenue, but the company will also incur additional cost to bring in a new customer which often results in higher costs due to extra advertising or onboarding benefits. Customer acquisition is 5-40 times more expensive than customer retention and it makes economic sense for the company to keep existing customers happy. Doing so may also result in an increase in profits due to increased engagement with customers.\n * By analysing employee churn, a company can retain its high performing employees by rewarding them with perks and benefits as the salesperson is often the face of the company during negotiations. Loss of a high performing employee will cause a drop in sales and might also cause existing customers associated with that salesperson to shift loyalties.\nBy referring to the churn prediction model, the company can take decisive steps to pursue its employees and customers.\n * E.g.: A customer identified by the model may be rewarded with a free discount voucher or a free exclusive service for a trial period or more industry relevant and personally rewarding offers. By doing so, the customer may feel more attached to the company resulting in an enhanced utilisation of related products and services.\n * E.g.: A high performing salesperson identified by the model can be rewarded with a free holiday package or a free mobile phone or more personally rewarding products and services in order to increase the employee's association with the company.\n\nPart 3 - Working with Time Series (Total 10 Points)\nHere we will analyze a timeseries of number of Yellow Fever cases from a specific Asian country by year. The data is from 1700 – 2008. The file cases.csv is available on NYU Classes.\n1. Load the timeseries data set, and prepare the dataset by converting the variables to date-time format (hint: use date tools). (1 point)\nEnd of explanation\n\"\"\"\n\n\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.stattools import pacf,acf\n\nplt.figure(1)\nplt.figure(figsize=(10,5))\nplt.title(\"Autocorrelation Plot\")\nautocorrelation_plot(fever[\"YFCASES\"])\nplt.figure(2)\nplt.figure(figsize=(10,5))\nplt.title(\"Partial Autocorrelation Plot\")\nplt.plot(pacf(fever[\"YFCASES\"]))\nplt.show()\n\n\"\"\"\nExplanation: 2. Plot the autocorrelation function (ACF) and partial autocorrelation function (PCF) of the cases timeseries. (1 point)\nEnd of explanation\n\"\"\"\n\n\n# We run Durbin Watson test on the residuls that we obtain from the OLS.\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.stats.stattools import durbin_watson\n\nols_residuals=OLS(fever[\"YFCASES\"],np.ones(len(fever[\"YFCASES\"]))).fit()\ndurbin_watson(ols_residuals.resid)\n\n\"\"\"\nExplanation: 3. Describe what the plots indicate (in terms of autocorrelation and autoregressive parameter (p) and moving average (q)). 2 points.\nSome rules of thumb to recall:<br>\nRule 1: If the ACF shows exponential decay, the PACF has a spike at lag 1, and no correlation for other lags, then use one autoregressive (p)parameter <br>\nRule 2: If the ACF shows a sine-wave shape pattern or a set of exponential decays, the PACF has spikes at lags 1 and 2, and no correlation for other lags, the use two autoregressive (p) parameters. <br>\nRule 3: If the ACF has a spike at lag 1, no correlation for other lags, and the PACF damps out exponentially, then use one moving average (q) parameter. <br>\nRule 4: If the ACF has spikes at lags 1 and 2, no correlation for other lags, and the PACF has a sine-wave shape pattern or a set of exponential decays, then use two moving average (q) parameter.<br>\nRule 5: If the ACF shows exponential decay starting at lag 1, and the PACF shows exponential decay starting at lag 1, then use one autoregressive (p) and one moving average (q) parameter.<br>\n<span style=\"color:Blue\">Answer </span>\n1. The autocorrelation plot shows a sine-wave pattern meaning that the data exhibits strong seasonality.\n2. The partial autocorrelation plot shows sharp spikes at lag of 1 and 2.\nWe use \"Rule 2\" and select the autocorrelation parameter as 2 i.e. p=2 and q=0\n\n4. Another approach to assessing the presence of autocorrelation is by using the  Durbin-Waton (DW) statistic. The value of the DW statistic is close to 2 if the errors are uncorrelated. What is DW for our data, and does this match what you observed from the ACF and PCF plots? (1 point)\nEnd of explanation\n\"\"\"\n\n\nfrom statsmodels import tsa\nimport statsmodels.api as sm  \n\nindexedFever=fever.set_index(\"YEAR\")\ncases=indexedFever.astype(float)\narma_result = sm.tsa.ARMA(cases,(2,0)).fit()\ncases['forecast'] = arma_result.predict(start = 260 , end= 309, dynamic= True)  \ncases[['YFCASES', 'forecast']].plot(figsize=(10, 5))\nplt.show()\n\n\"\"\"\nExplanation: <span style=\"color:Blue\">Answer </span>\n* We observe that the Durbin-Watson statistic is approximately 0.35 which means that there is strong positive relation between the previous and the current value of reported cases of yellow fever. The DW statistic matches the output observed from the autocorrelation plot which indicated strong correlation with lagged values.\n* E.g.: If the number of yellow fever cases increase in the current year, they will increase further in the next year and vice versa.\n* We can use the ARMA model to remove this autocorrelation between lagged values.\n\n5. Removing serial dependency by modeling a simple ARMA process with p and q as derived above. Take a look at what the resulting process looks like (plot) (1 point)\nEnd of explanation\n\"\"\"\n\n\nprint(stats.normaltest(arma_result.resid))\nfigureP3 = plt.figure(figsize=(10,5))\nax = figureP3.add_subplot(1,1,1)\nfigP3 = qqplot(arma_result.resid, line='q', ax=ax, fit=True)\nplt.show()\n\n\"\"\"\nExplanation: 6. Calculate the residuals, and test the null hypothesis that the residuals come from a normal distribution, and construct a qq-plot. Do the results of the hypothesis test and qq-plot align? (1 point)\nEnd of explanation\n\"\"\"\n\n\nplt.figure(figsize=(15,5))\nplt.title(\"Autocorrelation plot\")\nautocorrelation_plot(arma_result.resid)\nplt.show()\nacfValue=acf(arma_result.resid,qstat=True)\nautocorrelation_value=acfValue[0]\nqstat_value=acfValue[1]\np_value=acfValue[2]\nacfValue\n\n\"\"\"\nExplanation: <span style=\"color:Blue\">Answer </span>\n\nWe performed the test of normality on our data and also plotted a qqplot. The results of our normality test and our qqplot do not match.\nOur null hypothesis is that the distribution is normal. However, the p-value is less than 0.05 giving us enough evidence to reject the null hypothesis. This means that the data is not from a normal distribution.\nOur qqplot indicates that our data follows the theoretical normal distribution line.\n\n\n7. Now investigate the autocorrelation of your ARMA(p,q) model. Did it improve?\nThese can be examined graphically, but a statistic will help.\nNext, we calculate the lag, autocorrelation (AC), Q statistic and Prob>Q. \nThe Ljung–Box Q test is a type of statistical test of whether any of a group of autocorrelations of a time series are different from zero. The null hypothesis is, H0: The data are independently distributed (i.e. the correlations in the population from which the sample is taken are 0, so that any observed correlations in the data result from randomness of the sampling process). (Hint: use qstat in tsa.acf).\nEnd of explanation\n\"\"\"\n\n\nfrom pandas import datetime\nbegin_year = datetime(2009,1,1)\nend_year = datetime(2012,1,1)\nforecasted = arma_result.predict(start=begin_year, end=end_year)\nforecasted\n\n\"\"\"\nExplanation: <span style=\"color:Blue\">Answer </span>\n\nThe autocorrelation plot has improved as we see an absence of sinusoidal pattern which was earlier evident. We have reduced the impact of seasonality using the ARMA model.\nThe Ljung–Box Q test is used to indicate whether our data is independently distributed or not. Based on the analysis of p values, can reject the null hypothesis that the data is independently distributed. We conclude that our data exhibits serial correlation. \n\n\n8. Compute prediction for years 2009-2012 and analyze their fit against actual values. (1 point)\nEnd of explanation\n\"\"\"\n\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\n\nferror_begin=datetime(1700,1,1)\nferror_end=datetime(2008,1,1)\npredictionARMA=arma_result.predict(start=ferror_begin,end=ferror_end)\nMAE=mean_absolute_error(fever[\"YFCASES\"],predictionARMA)\nMFE=mean_squared_error(fever[\"YFCASES\"],predictionARMA)\nprint(\"MAE is %f and MFE is %f\" %(MAE,MFE))\n\n\"\"\"\nExplanation: 9. Calculate the forecast error via MAE and MFE.  (2 points)\nReminders:\nMean absolute error: The mean absolute error (MAE) value is computed as the average absolute error value. If MAE is zero the forecast is perfect. As compared to the mean squared error (MSE), this measure of fit “de-emphasizes” outliers (unique or rare large error values will affect the MAE less than the MSE.\nMean Forecast Error (MFE, also known as Bias). The MFE is the average error in the observations. A large positive MFE means that the forecast is undershooting the actual observations. A large negative MFE means the forecast is overshooting the actual observations. A value near zero is ideal, and generally a small value means a pretty good fit. \nThe MAE is a better indicator of fit than the MFE.\nEnd of explanation\n\"\"\"\n\nimport os\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n#os.environ['THEANO_FLAGS'] = \"device=gpu2\"\nfrom keras.models import load_model\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout\nfrom keras.optimizers import SGD\n\nnb_classes = 10\n\n# FC@512+relu -> DropOut(0.2) -> FC@512+relu -> DropOut(0.2) -> FC@nb_classes+softmax\n# ... your Code Here\n\n# %load solutions/sol_221_1.py\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout\nfrom keras.optimizers import SGD\n\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(784,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=SGD(), \n              metrics=['accuracy'])\n\n\"\"\"\nExplanation: Fully Connected Feed-Forward Network\nIn this notebook we will play with Feed-Forward FC-NN (Fully Connected Neural Network) for a classification task: Image Classification on MNIST Dataset\nRECALL\nIn the FC-NN, the output of each layer is computed using the activations from the previous one, as follows:\n$$h_{i} = \\sigma(W_i h_{i-1} + b_i)$$\nwhere ${h}_i$ is the activation vector from the $i$-th layer (or the input data for $i=0$), ${W}_i$ and ${b}_i$ are  the weight matrix and the bias vector for the $i$-th layer, respectively. \n<br><rb>\n$\\sigma(\\cdot)$ is the activation function. In our example, we will use the ReLU activation function for the hidden layers and softmax for the last layer.\nTo regularize the model, we will also insert a Dropout layer between consecutive hidden layers. \nDropout works by “dropping out” some unit activations in a given layer, that is setting them to zero with a given probability.\nOur loss function will be the categorical crossentropy.\nModel definition\nKeras supports two different kind of models: the Sequential model and the Graph model. The former is used to build linear stacks of layer (so each layer has one input and one output), and the latter supports any kind of connection graph.\nIn our case we build a Sequential model with three Dense (aka fully connected) layers, with some Dropout. Notice that the output layer has the softmax activation function. \nThe resulting model is actually a function of its own inputs implemented using the Keras backend. \nWe apply the binary crossentropy loss and choose SGD as the optimizer. \nPlease remind that Keras supports a variety of different optimizers and loss functions, which you may want to check out.\nEnd of explanation\n\"\"\"\n\n\nfrom keras.datasets import mnist\nfrom keras.utils import np_utils\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = X_train.reshape(60000, 784)\nX_test = X_test.reshape(10000, 784)\nX_train = X_train.astype(\"float32\")\nX_test = X_test.astype(\"float32\")\nX_train /= 255\nX_test /= 255\n\n# convert class vectors to binary class matrices\nY_train = np_utils.to_categorical(y_train, 10)\nY_test = np_utils.to_categorical(y_test, 10)\n\n\"\"\"\nExplanation: Data preparation (keras.dataset)\nWe will train our model on the MNIST dataset, which consists of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. \n\nSince this dataset is provided with Keras, we just ask the keras.dataset model for training and test data.\nWe will:\n\ndownload the data\nreshape data to be in vectorial form (original data are images)\nnormalize between 0 and 1.\n\nThe binary_crossentropy loss expects a one-hot-vector as input, therefore we apply the to_categorical function from keras.utilis to convert integer labels to one-hot-vectors.\nEnd of explanation\n\"\"\"\n\n\n# You can train the network yourself or simply load a saved model :P, for now!!\n#network_history = model.fit(X_train, Y_train, batch_size=1000, \n#                            nb_epoch=100, verbose=1, validation_data=(X_test, Y_test))\n#model.save('example_MNIST_FC.h5')\n\nmodel=load_model('example_MNIST_FC.h5')\nmodel.summary()\n\n\"\"\"\nExplanation: Training\nHaving defined and compiled the model, it can be trained using the fit function. We also specify a validation dataset to monitor validation loss and accuracy.\nEnd of explanation\n\"\"\"\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure()\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.plot(network_history.history['loss'])\nplt.plot(network_history.history['val_loss'])\nplt.legend(['Training', 'Validation'])\n\nplt.figure()\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.plot(network_history.history['acc'])\nplt.plot(network_history.history['val_acc'])\nplt.legend(['Training', 'Validation'], loc='lower right')\n\nimport numpy as np\nprint(np.argmax(model.predict(X_test[5:10]),1))\nprint(y_test[5:10])\n\n# Can you write a snippet that finds a misclassified sample in X_train and\n# displays the image, correct classification and your prediction\n\n\n\"\"\"\nExplanation: Plotting Network Performance Trend\nThe return value of the fit function is a keras.callbacks.History object which contains the entire history of training/validation loss and accuracy, for each epoch. We can therefore plot the behaviour of loss and accuracy during the training phase.\nEnd of explanation\n\"\"\"\n\n# imports\nfrom ionize import Aqueous\nfrom math import sqrt, pi\nimport pint\nur = pint.UnitRegistry()\nQ = ur.Quantity\n\n# define values\ntemperature = Q(25, 'degC')\ne = ur.elementary_charge\nkb = ur.boltzmann_constant\ndielectric = Aqueous.dielectric(temperature.magnitude)\nviscosity = Aqueous.viscosity(temperature.magnitude) * ur.pascal * ur.second\nNa = ur.avogadro_number\neps = ur.vacuum_permittivity\n\n\"\"\"\nExplanation: Ion Interaction\nIon interaction effects are important for correcting the mobility of ions based on the ionic strength of solution. These effects are laid out in the Irreversible Processes in Electrohoresis, by Onsager and Fuoss. \nThese effects have prefactors that need to be converted to appropriate units for use. Here, I'm going to resolve the units on these prefactors, and resolve the differences between the paper and the STEEP implimentation.\nEnd of explanation\n\"\"\"\n\n\n# STEEP: F*0.2297*z.*omega+31.410e-9\ncoefficient_1 = 0.2297 * ((temperature.to('degK'))**.5*dielectric)**3.\ncoefficient_2 = 31.410e-9 * viscosity * (temperature.to('degK')**.5*dielectric)\n\nprint(coefficient_1, coefficient_2)\n\n\"\"\"\nExplanation: Robinson-Stokes\nEnd of explanation\n\"\"\"\n\n\n# Bhaga Paper\nA = e**3 / 12 / pi \nA *= (Na / (dielectric * eps * kb * temperature.to('degK'))**3)**.5\nprint('[A] =', A.dimensionality)\nprint('A =', A.to('liter^.5/mol^.5'))\n\nB = e**2 / 6 / pi / viscosity \nB *= (Na / (dielectric * eps * kb * temperature.to('degK')))**.5\nprint('[B] =', B.dimensionality)\nprint('B =', B.to('m^2/V/s* liter^.5/mol^.5'))\n\n\"\"\"\nExplanation: STEEP O-F\n mob_new= F * omega - (F*0.78420*z.*factor.*omega* / dielectric^1.5 / temperature^1.5 +\n\n          31.410e-9 *  viscosity / dielectric^.5 / temperature^.5 .*\n          sqrt(IonicStr/2000)./(1+1.5*sqrt((yy(5)*yy(1))/(yy(6)*yy(2)))\n          *sqrt(IonicStr/2000));\n\nSTEEP RS\n%Robinson-Stokes, just checking dont use this.\n%mob_new=F*omega-(F*0.2297*z.*omega+31.410e-9).*sqrt(IonicStr/2000)./(1+1.5*sqrt(IonicStr/2000));\n%Assemble matrix back\n\nOnsager-Fuoss\nThe Onsager-Fuoss constants A and B.\nEnd of explanation\n\"\"\"\n\n\nAp = e**3 / 12 / pi \nAp *= (Na / (eps * kb)**3)**.5\nprint(Ap.dimensionality)\nprint(Ap.to('liter^.5*degK^1.5/mol^.5'))\n\nBp = e**2 / 6 / pi \nBp *= (Na / (eps * kb))**.5\nprint(Bp.dimensionality)\nprint(Bp.to('m^2/V/s* liter^.5/mol^.5 * Pa * s * degK**.5'))\n\n# TODO: check steep and spresso implementation\n# also check onsager-fuoss paper\n\n\"\"\"\nExplanation: Temperature-dependant values\nBelow are values that only include temperature independant versions. \nA = Ap / dielectric^1.5 / temperature^1.5\nB = Bp x viscosity / dielectric^.5 / temperature^.5\nEnd of explanation\n\"\"\"\n\n\nD = (2 * e ** 2 * Na/ dielectric / eps / kb / temperature.to('degK'))**.5\nprint(D.dimensionality)\n(D).to('m^.5/mol^.5')\n(1/D / Q('(1e-7 mol/L)^.5')).to('um')\n\nDp = (2 * e ** 2 * Na / eps / kb)**.5\nprint(Dp.dimensionality)\nDp.to(())\n\n\"\"\"\nExplanation: Pitts\nEnd of explanation\n\"\"\"\n\n#encoding:UTF-8\nimport urllib.request\n \nurl = \"http://www.pku.edu.cn\"\ndata = urllib.request.urlopen(url).read()\ndata = data.decode('UTF-8')\nprint(data)\n\n\"\"\"\nExplanation: 用Python 3开发网络爬虫\nBy Terrill Yang (Github: https://github.com/yttty)\n由你需要这些：Python3.x爬虫学习资料整理 - 知乎专栏整理而来。\n用Python 3开发网络爬虫 - Chapter 01\n1. 一个简单的伪代码\n以下这个简单的伪代码用到了set和queue这两种经典的数据结构, 集与队列. 集的作用是记录那些已经访问过的页面, 队列的作用是进行广度优先搜索.\nqueue Q\nset S\nStartPoint = \"http://jecvay.com\"\nQ.push(StartPoint)  # 经典的BFS开头\nS.insert(StartPoint)  # 访问一个页面之前先标记他为已访问\nwhile (Q.empty() == false)  # BFS循环体\n  T = Q.top()  # 并且pop\n  for point in PageUrl(T)  # PageUrl(T)是指页面T中所有url的集合, point是这个集合中的一个元素.\n    if (point not in S)\n      Q.push(point)\n      S.insert(point)\n这个伪代码不能执行, 但是看懂是没问题的, 这就是个最简单的BFS结构. 我是看了知乎里面的那个伪代码之后, 自己用我的风格写了一遍. 你也需要用你的风格写一遍.\n这里用到的Set其内部原理是采用了Hash表, 传统的Hash对爬虫来说占用空间太大, 因此有一种叫做Bloom Filter的数据结构更适合用在这里替代Hash版本的set. 我打算以后再看这个数据结构怎么使用, 现在先跳过, 因为对于零基础的我来说, 这不是重点.\n2. 用Python抓取指定页面\nEnd of explanation\n\"\"\"\n\n\na = urllib.request.urlopen(url)\ntype(a)\n\na.geturl()\n\na.info()\n\na.getcode()\n\n\"\"\"\nExplanation: urllib.request是一个库, 隶属urllib. 点此打开官方相关文档. 官方文档应该怎么使用呢? 首先点刚刚提到的这个链接进去的页面有urllib的几个子库, 我们暂时用到了request, 所以我们先看urllib.request部分. 首先看到的是一句话介绍这个库是干什么用的:\n\nThe urllib.request module defines functions and classes which help in opening URLs (mostly HTTP) in a complex world — basic and digest authentication, redirections, cookies and more.\n\n然后把我们代码中用到的urlopen()函数部分阅读完.\n\nurllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False)\n\n重点部分是返回值, 这个函数返回一个 http.client.HTTPResponse 对象, 这个对象又有各种方法, 比如我们用到的read()方法, 这些方法都可以根据官方文档的链接链过去. 根据官方文档所写, 我用控制台运行完毕上面这个程序后, 又继续运行如下代码, 以更熟悉这些乱七八糟的方法是干什么的.\nEnd of explanation\n\"\"\"\n\n\nimport urllib\nimport urllib.request\n \ndata={}\ndata['word']='Jecvay Notes'\n \nurl_values=urllib.parse.urlencode(data)\nurl=\"http://www.baidu.com/s?\"\nfull_url=url+url_values\n \ndata=urllib.request.urlopen(full_url).read()\ndata=data.decode('UTF-8')\nprint(data)\n\n\"\"\"\nExplanation: 3. 用Python简单处理URL\n如果要抓取百度上面搜索关键词为Jecvay Notes的网页, 则代码如下\nEnd of explanation\n\"\"\"\n\nimport os\nimport sys\n\n# Google Cloud Notebook\nif os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n    USER_FLAG = \"--user\"\nelse:\n    USER_FLAG = \"\"\n\n! pip3 install -U google-cloud-aiplatform $USER_FLAG\n\n\"\"\"\nExplanation: Vertex client library: AutoML image classification model for export to edge\n<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/gapic/automl/showcase_automl_image_classification_export_edge.ipynb\">\n      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n    </a>\n  </td>\n  <td>\n    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/community/gapic/automl/showcase_automl_image_classification_export_edge.ipynb\">\n      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n      View on GitHub\n    </a>\n  </td>\n</table>\n<br/><br/><br/>\nOverview\nThis tutorial demonstrates how to use the Vertex client library for Python to create image classification models to export as an Edge model using Google Cloud's AutoML.\nDataset\nThe dataset used for this tutorial is the Flowers dataset from TensorFlow Datasets. The version of the dataset you will use in this tutorial is stored in a public Cloud Storage bucket. The trained model predicts the type of flower an image is from a class of five flowers: daisy, dandelion, rose, sunflower, or tulip.\nObjective\nIn this tutorial, you create a AutoML image classification model from a Python script using the Vertex client library, and then export the model as an Edge model in TFLite format. You can alternatively create models with AutoML using the gcloud command-line tool or online using the Google Cloud Console.\nThe steps performed include:\n\nCreate a Vertex Dataset resource.\nTrain the model.\nExport the Edge model from the Model resource to Cloud Storage.\nDownload the model locally.\nMake a local prediction.\n\nCosts\nThis tutorial uses billable components of Google Cloud (GCP):\n\nVertex AI\nCloud Storage\n\nLearn about Vertex AI\npricing and Cloud Storage\npricing, and use the Pricing\nCalculator\nto generate a cost estimate based on your projected usage.\nInstallation\nInstall the latest version of Vertex client library.\nEnd of explanation\n\"\"\"\n\n\n! pip3 install -U google-cloud-storage $USER_FLAG\n\n\"\"\"\nExplanation: Install the latest GA version of google-cloud-storage library as well.\nEnd of explanation\n\"\"\"\n\n\nif not os.getenv(\"IS_TESTING\"):\n    # Automatically restart kernel after installs\n    import IPython\n\n    app = IPython.Application.instance()\n    app.kernel.do_shutdown(True)\n\n\"\"\"\nExplanation: Restart the kernel\nOnce you've installed the Vertex client library and Google cloud-storage, you need to restart the notebook kernel so it can find the packages.\nEnd of explanation\n\"\"\"\n\n\nPROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n\nif PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n    # Get your GCP project id from gcloud\n    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n    PROJECT_ID = shell_output[0]\n    print(\"Project ID:\", PROJECT_ID)\n\n! gcloud config set project $PROJECT_ID\n\n\"\"\"\nExplanation: Before you begin\nGPU runtime\nMake sure you're running this notebook in a GPU runtime if you have that option. In Colab, select Runtime > Change Runtime Type > GPU\nSet up your Google Cloud project\nThe following steps are required, regardless of your notebook environment.\n\n\nSelect or create a Google Cloud project. When you first create an account, you get a $300 free credit towards your compute/storage costs.\n\n\nMake sure that billing is enabled for your project.\n\n\nEnable the Vertex APIs and Compute Engine APIs.\n\n\nThe Google Cloud SDK is already installed in Google Cloud Notebook.\n\n\nEnter your project ID in the cell below. Then run the  cell to make sure the\nCloud SDK uses the right project for all the commands in this notebook.\n\n\nNote: Jupyter runs lines prefixed with ! as shell commands, and it interpolates Python variables prefixed with $ into these commands.\nEnd of explanation\n\"\"\"\n\n\nREGION = \"us-central1\"  # @param {type: \"string\"}\n\n\"\"\"\nExplanation: Region\nYou can also change the REGION variable, which is used for operations\nthroughout the rest of this notebook.  Below are regions supported for Vertex. We recommend that you choose the region closest to you.\n\nAmericas: us-central1\nEurope: europe-west4\nAsia Pacific: asia-east1\n\nYou may not use a multi-regional bucket for training with Vertex. Not all regions provide support for all Vertex services. For the latest support per region, see the Vertex locations documentation\nEnd of explanation\n\"\"\"\n\n\nfrom datetime import datetime\n\nTIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n\"\"\"\nExplanation: Timestamp\nIf you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append onto the name of resources which will be created in this tutorial.\nEnd of explanation\n\"\"\"\n\n\n# If you are running this notebook in Colab, run this cell and follow the\n# instructions to authenticate your GCP account. This provides access to your\n# Cloud Storage bucket and lets you submit training jobs and prediction\n# requests.\n\n# If on Google Cloud Notebook, then don't execute this code\nif not os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n    if \"google.colab\" in sys.modules:\n        from google.colab import auth as google_auth\n\n        google_auth.authenticate_user()\n\n    # If you are running this notebook locally, replace the string below with the\n    # path to your service account key and run this cell to authenticate your GCP\n    # account.\n    elif not os.getenv(\"IS_TESTING\"):\n        %env GOOGLE_APPLICATION_CREDENTIALS ''\n\n\"\"\"\nExplanation: Authenticate your Google Cloud account\nIf you are using Google Cloud Notebook, your environment is already authenticated. Skip this step.\nIf you are using Colab, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\nOtherwise, follow these steps:\nIn the Cloud Console, go to the Create service account key page.\nClick Create service account.\nIn the Service account name field, enter a name, and click Create.\nIn the Grant this service account access to project section, click the Role drop-down list. Type \"Vertex\" into the filter box, and select Vertex Administrator. Type \"Storage Object Admin\" into the filter box, and select Storage Object Admin.\nClick Create. A JSON file that contains your key downloads to your local environment.\nEnter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell.\nEnd of explanation\n\"\"\"\n\n\nBUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n\nif BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP\n\n\"\"\"\nExplanation: Create a Cloud Storage bucket\nThe following steps are required, regardless of your notebook environment.\nThis tutorial is designed to use training data that is in a public Cloud Storage bucket and a local Cloud Storage bucket for exporting the trained model. You may alternatively use your own training data that you have stored in a local Cloud Storage bucket.\nSet the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\nEnd of explanation\n\"\"\"\n\n\n! gsutil mb -l $REGION $BUCKET_NAME\n\n\"\"\"\nExplanation: Only if your bucket doesn't already exist: Run the following cell to create your Cloud Storage bucket.\nEnd of explanation\n\"\"\"\n\n\n! gsutil ls -al $BUCKET_NAME\n\n\"\"\"\nExplanation: Finally, validate access to your Cloud Storage bucket by examining its contents:\nEnd of explanation\n\"\"\"\n\n\nimport time\n\nfrom google.cloud.aiplatform import gapic as aip\nfrom google.protobuf import json_format\nfrom google.protobuf.json_format import MessageToJson, ParseDict\nfrom google.protobuf.struct_pb2 import Struct, Value\n\n\"\"\"\nExplanation: Set up variables\nNext, set up some variables used throughout the tutorial.\nImport libraries and define constants\nImport Vertex client library\nImport the Vertex client library into our Python environment.\nEnd of explanation\n\"\"\"\n\n\n# API service endpoint\nAPI_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n\n# Vertex location root path for your dataset, model and endpoint resources\nPARENT = \"projects/\" + PROJECT_ID + \"/locations/\" + REGION\n\n\"\"\"\nExplanation: Vertex constants\nSetup up the following constants for Vertex:\n\nAPI_ENDPOINT: The Vertex API service endpoint for dataset, model, job, pipeline and endpoint services.\nPARENT: The Vertex location root path for dataset, model, job, pipeline and endpoint resources.\nEnd of explanation\n\"\"\"\n\n\n# Image Dataset type\nDATA_SCHEMA = \"gs://google-cloud-aiplatform/schema/dataset/metadata/image_1.0.0.yaml\"\n# Image Labeling type\nLABEL_SCHEMA = \"gs://google-cloud-aiplatform/schema/dataset/ioformat/image_classification_single_label_io_format_1.0.0.yaml\"\n# Image Training task\nTRAINING_SCHEMA = \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_image_classification_1.0.0.yaml\"\n\n\"\"\"\nExplanation: AutoML constants\nSet constants unique to AutoML datasets and training:\n\nDataset Schemas: Tells the Dataset resource service which type of dataset it is.\nData Labeling (Annotations) Schemas: Tells the Dataset resource service how the data is labeled (annotated).\nDataset Training Schemas: Tells the Pipeline resource service the task (e.g., classification) to train the model for.\nEnd of explanation\n\"\"\"\n\n\n# client options same for all services\nclient_options = {\"api_endpoint\": API_ENDPOINT}\n\n\ndef create_dataset_client():\n    client = aip.DatasetServiceClient(client_options=client_options)\n    return client\n\n\ndef create_model_client():\n    client = aip.ModelServiceClient(client_options=client_options)\n    return client\n\n\ndef create_pipeline_client():\n    client = aip.PipelineServiceClient(client_options=client_options)\n    return client\n\n\nclients = {}\nclients[\"dataset\"] = create_dataset_client()\nclients[\"model\"] = create_model_client()\nclients[\"pipeline\"] = create_pipeline_client()\n\nfor client in clients.items():\n    print(client)\n\n\"\"\"\nExplanation: Tutorial\nNow you are ready to start creating your own AutoML image classification model.\nSet up clients\nThe Vertex client library works as a client/server model. On your side (the Python script) you will create a client that sends requests and receives responses from the Vertex server.\nYou will use different clients in this tutorial for different steps in the workflow. So set them all up upfront.\n\nDataset Service for Dataset resources.\nModel Service for Model resources.\nPipeline Service for training.\nEnd of explanation\n\"\"\"\n\n\nTIMEOUT = 90\n\n\ndef create_dataset(name, schema, labels=None, timeout=TIMEOUT):\n    start_time = time.time()\n    try:\n        dataset = aip.Dataset(\n            display_name=name, metadata_schema_uri=schema, labels=labels\n        )\n\n        operation = clients[\"dataset\"].create_dataset(parent=PARENT, dataset=dataset)\n        print(\"Long running operation:\", operation.operation.name)\n        result = operation.result(timeout=TIMEOUT)\n        print(\"time:\", time.time() - start_time)\n        print(\"response\")\n        print(\" name:\", result.name)\n        print(\" display_name:\", result.display_name)\n        print(\" metadata_schema_uri:\", result.metadata_schema_uri)\n        print(\" metadata:\", dict(result.metadata))\n        print(\" create_time:\", result.create_time)\n        print(\" update_time:\", result.update_time)\n        print(\" etag:\", result.etag)\n        print(\" labels:\", dict(result.labels))\n        return result\n    except Exception as e:\n        print(\"exception:\", e)\n        return None\n\n\nresult = create_dataset(\"flowers-\" + TIMESTAMP, DATA_SCHEMA)\n\n\"\"\"\nExplanation: Dataset\nNow that your clients are ready, your first step in training a model is to create a managed dataset instance, and then upload your labeled data to it.\nCreate Dataset resource instance\nUse the helper function create_dataset to create the instance of a Dataset resource. This function does the following:\n\nUses the dataset client service.\nCreates an Vertex Dataset resource (aip.Dataset), with the following parameters:\ndisplay_name: The human-readable name you choose to give it.\nmetadata_schema_uri: The schema for the dataset type.\nCalls the client dataset service method create_dataset, with the following parameters:\nparent: The Vertex location root path for your Database, Model and Endpoint resources.\ndataset: The Vertex dataset object instance you created.\nThe method returns an operation object.\n\nAn operation object is how Vertex handles asynchronous calls for long running operations. While this step usually goes fast, when you first use it in your project, there is a longer delay due to provisioning.\nYou can use the operation object to get status on the operation (e.g., create Dataset resource) or to cancel the operation, by invoking an operation method:\n| Method      | Description |\n| ----------- | ----------- |\n| result()    | Waits for the operation to complete and returns a result object in JSON format.      |\n| running()   | Returns True/False on whether the operation is still running.        |\n| done()      | Returns True/False on whether the operation is completed. |\n| canceled()  | Returns True/False on whether the operation was canceled. |\n| cancel()    | Cancels the operation (this may take up to 30 seconds). |\nEnd of explanation\n\"\"\"\n\n\n# The full unique ID for the dataset\ndataset_id = result.name\n# The short numeric ID for the dataset\ndataset_short_id = dataset_id.split(\"/\")[-1]\n\nprint(dataset_id)\n\n\"\"\"\nExplanation: Now save the unique dataset identifier for the Dataset resource instance you created.\nEnd of explanation\n\"\"\"\n\n\nIMPORT_FILE = (\n    \"gs://cloud-samples-data/vision/automl_classification/flowers/all_data_v2.csv\"\n)\n\n\"\"\"\nExplanation: Data preparation\nThe Vertex Dataset resource for images has some requirements for your data:\n\nImages must be stored in a Cloud Storage bucket.\nEach image file must be in an image format (PNG, JPEG, BMP, ...).\nThere must be an index file stored in your Cloud Storage bucket that contains the path and label for each image.\nThe index file must be either CSV or JSONL.\n\nCSV\nFor image classification, the CSV index file has the requirements:\n\nNo heading.\nFirst column is the Cloud Storage path to the image.\nSecond column is the label.\n\nLocation of Cloud Storage training data.\nNow set the variable IMPORT_FILE to the location of the CSV index file in Cloud Storage.\nEnd of explanation\n\"\"\"\n\n\nif \"IMPORT_FILES\" in globals():\n    FILE = IMPORT_FILES[0]\nelse:\n    FILE = IMPORT_FILE\n\ncount = ! gsutil cat $FILE | wc -l\nprint(\"Number of Examples\", int(count[0]))\n\nprint(\"First 10 rows\")\n! gsutil cat $FILE | head\n\n\"\"\"\nExplanation: Quick peek at your data\nYou will use a version of the Flowers dataset that is stored in a public Cloud Storage bucket, using a CSV index file.\nStart by doing a quick peek at the data. You count the number of examples by counting the number of rows in the CSV index file  (wc -l) and then peek at the first few rows.\nEnd of explanation\n\"\"\"\n\n\ndef import_data(dataset, gcs_sources, schema):\n    config = [{\"gcs_source\": {\"uris\": gcs_sources}, \"import_schema_uri\": schema}]\n    print(\"dataset:\", dataset_id)\n    start_time = time.time()\n    try:\n        operation = clients[\"dataset\"].import_data(\n            name=dataset_id, import_configs=config\n        )\n        print(\"Long running operation:\", operation.operation.name)\n\n        result = operation.result()\n        print(\"result:\", result)\n        print(\"time:\", int(time.time() - start_time), \"secs\")\n        print(\"error:\", operation.exception())\n        print(\"meta :\", operation.metadata)\n        print(\n            \"after: running:\",\n            operation.running(),\n            \"done:\",\n            operation.done(),\n            \"cancelled:\",\n            operation.cancelled(),\n        )\n\n        return operation\n    except Exception as e:\n        print(\"exception:\", e)\n        return None\n\n\nimport_data(dataset_id, [IMPORT_FILE], LABEL_SCHEMA)\n\n\"\"\"\nExplanation: Import data\nNow, import the data into your Vertex Dataset resource. Use this helper function import_data to import the data. The function does the following:\n\nUses the Dataset client.\nCalls the client method import_data, with the following parameters:\nname: The human readable name you give to the Dataset resource (e.g., flowers).\n\nimport_configs: The import configuration.\n\n\nimport_configs: A Python list containing a dictionary, with the key/value entries:\n\ngcs_sources: A list of URIs to the paths of the one or more index files.\nimport_schema_uri: The schema identifying the labeling type.\n\nThe import_data() method returns a long running operation object. This will take a few minutes to complete. If you are in a live tutorial, this would be a good time to ask questions, or take a personal break.\nEnd of explanation\n\"\"\"\n\n\ndef create_pipeline(pipeline_name, model_name, dataset, schema, task):\n\n    dataset_id = dataset.split(\"/\")[-1]\n\n    input_config = {\n        \"dataset_id\": dataset_id,\n        \"fraction_split\": {\n            \"training_fraction\": 0.8,\n            \"validation_fraction\": 0.1,\n            \"test_fraction\": 0.1,\n        },\n    }\n\n    training_pipeline = {\n        \"display_name\": pipeline_name,\n        \"training_task_definition\": schema,\n        \"training_task_inputs\": task,\n        \"input_data_config\": input_config,\n        \"model_to_upload\": {\"display_name\": model_name},\n    }\n\n    try:\n        pipeline = clients[\"pipeline\"].create_training_pipeline(\n            parent=PARENT, training_pipeline=training_pipeline\n        )\n        print(pipeline)\n    except Exception as e:\n        print(\"exception:\", e)\n        return None\n    return pipeline\n\n\"\"\"\nExplanation: Train the model\nNow train an AutoML image classification model using your Vertex Dataset resource. To train the model, do the following steps:\n\nCreate an Vertex training pipeline for the Dataset resource.\nExecute the pipeline to start the training.\n\nCreate a training pipeline\nYou may ask, what do we use a pipeline for? You typically use pipelines when the job (such as training) has multiple steps, generally in sequential order: do step A, do step B, etc. By putting the steps into a pipeline, we gain the benefits of:\n\nBeing reusable for subsequent training jobs.\nCan be containerized and ran as a batch job.\nCan be distributed.\nAll the steps are associated with the same pipeline job for tracking progress.\n\nUse this helper function create_pipeline, which takes the following parameters:\n\npipeline_name: A human readable name for the pipeline job.\nmodel_name: A human readable name for the model.\ndataset: The Vertex fully qualified dataset identifier.\nschema: The dataset labeling (annotation) training schema.\ntask: A dictionary describing the requirements for the training job.\n\nThe helper function calls the Pipeline client service'smethod create_pipeline, which takes the following parameters:\n\nparent: The Vertex location root path for your Dataset, Model and Endpoint resources.\ntraining_pipeline: the full specification for the pipeline training job.\n\nLet's look now deeper into the minimal requirements for constructing a training_pipeline specification:\n\ndisplay_name: A human readable name for the pipeline job.\ntraining_task_definition: The dataset labeling (annotation) training schema.\ntraining_task_inputs: A dictionary describing the requirements for the training job.\nmodel_to_upload: A human readable name for the model.\ninput_data_config: The dataset specification.\ndataset_id: The Vertex dataset identifier only (non-fully qualified) -- this is the last part of the fully-qualified identifier.\nfraction_split: If specified, the percentages of the dataset to use for training, test and validation. Otherwise, the percentages are automatically selected by AutoML.\nEnd of explanation\n\"\"\"\n\n\nPIPE_NAME = \"flowers_pipe-\" + TIMESTAMP\nMODEL_NAME = \"flowers_model-\" + TIMESTAMP\n\ntask = json_format.ParseDict(\n    {\n        \"multi_label\": False,\n        \"budget_milli_node_hours\": 8000,\n        \"model_type\": \"MOBILE_TF_LOW_LATENCY_1\",\n        \"disable_early_stopping\": False,\n    },\n    Value(),\n)\n\nresponse = create_pipeline(PIPE_NAME, MODEL_NAME, dataset_id, TRAINING_SCHEMA, task)\n\n\"\"\"\nExplanation: Construct the task requirements\nNext, construct the task requirements. Unlike other parameters which take a Python (JSON-like) dictionary, the task field takes a Google protobuf Struct, which is very similar to a Python dictionary. Use the json_format.ParseDict method for the conversion.\nThe minimal fields we need to specify are:\n\nmulti_label: Whether True/False this is a multi-label (vs single) classification.\nbudget_milli_node_hours: The maximum time to budget (billed) for training the model, where 1000 = 1 hour. For image classification, the budget must be a minimum of 8 hours.\nmodel_type: The type of deployed model:\nCLOUD: For deploying to Google Cloud.\nMOBILE_TF_LOW_LATENCY_1: For deploying to the edge and optimizing for latency (response time).\nMOBILE_TF_HIGH_ACCURACY_1: For deploying to the edge and optimizing for accuracy.\nMOBILE_TF_VERSATILE_1: For deploying to the edge and optimizing for a trade off between latency and accuracy.\ndisable_early_stopping: Whether True/False to let AutoML use its judgement to stop training early or train for the entire budget.\n\nFinally, create the pipeline by calling the helper function create_pipeline, which returns an instance of a training pipeline object.\nEnd of explanation\n\"\"\"\n\n\n# The full unique ID for the pipeline\npipeline_id = response.name\n# The short numeric ID for the pipeline\npipeline_short_id = pipeline_id.split(\"/\")[-1]\n\nprint(pipeline_id)\n\n\"\"\"\nExplanation: Now save the unique identifier of the training pipeline you created.\nEnd of explanation\n\"\"\"\n\n\ndef get_training_pipeline(name, silent=False):\n    response = clients[\"pipeline\"].get_training_pipeline(name=name)\n    if silent:\n        return response\n\n    print(\"pipeline\")\n    print(\" name:\", response.name)\n    print(\" display_name:\", response.display_name)\n    print(\" state:\", response.state)\n    print(\" training_task_definition:\", response.training_task_definition)\n    print(\" training_task_inputs:\", dict(response.training_task_inputs))\n    print(\" create_time:\", response.create_time)\n    print(\" start_time:\", response.start_time)\n    print(\" end_time:\", response.end_time)\n    print(\" update_time:\", response.update_time)\n    print(\" labels:\", dict(response.labels))\n    return response\n\n\nresponse = get_training_pipeline(pipeline_id)\n\n\"\"\"\nExplanation: Get information on a training pipeline\nNow get pipeline information for just this training pipeline instance. The helper function  gets the job information for just this job by calling the the job client service's get_training_pipeline method, with the following parameter:\n\nname: The Vertex fully qualified pipeline identifier.\n\nWhen the model is done training, the pipeline state will be PIPELINE_STATE_SUCCEEDED.\nEnd of explanation\n\"\"\"\n\n\nwhile True:\n    response = get_training_pipeline(pipeline_id, True)\n    if response.state != aip.PipelineState.PIPELINE_STATE_SUCCEEDED:\n        print(\"Training job has not completed:\", response.state)\n        model_to_deploy_id = None\n        if response.state == aip.PipelineState.PIPELINE_STATE_FAILED:\n            raise Exception(\"Training Job Failed\")\n    else:\n        model_to_deploy = response.model_to_upload\n        model_to_deploy_id = model_to_deploy.name\n        print(\"Training Time:\", response.end_time - response.start_time)\n        break\n    time.sleep(60)\n\nprint(\"model to deploy:\", model_to_deploy_id)\n\n\"\"\"\nExplanation: Deployment\nTraining the above model may take upwards of 30 minutes time.\nOnce your model is done training, you can calculate the actual time it took to train the model by subtracting end_time from start_time. For your model, you will need to know the fully qualified Vertex Model resource identifier, which the pipeline service assigned to it. You can get this from the returned pipeline instance as the field model_to_deploy.name.\nEnd of explanation\n\"\"\"\n\n\ndef list_model_evaluations(name):\n    response = clients[\"model\"].list_model_evaluations(parent=name)\n    for evaluation in response:\n        print(\"model_evaluation\")\n        print(\" name:\", evaluation.name)\n        print(\" metrics_schema_uri:\", evaluation.metrics_schema_uri)\n        metrics = json_format.MessageToDict(evaluation._pb.metrics)\n        for metric in metrics.keys():\n            print(metric)\n        print(\"logloss\", metrics[\"logLoss\"])\n        print(\"auPrc\", metrics[\"auPrc\"])\n\n    return evaluation.name\n\n\nlast_evaluation = list_model_evaluations(model_to_deploy_id)\n\n\"\"\"\nExplanation: Model information\nNow that your model is trained, you can get some information on your model.\nEvaluate the Model resource\nNow find out how good the model service believes your model is. As part of training, some portion of the dataset was set aside as the test (holdout) data, which is used by the pipeline service to evaluate the model.\nList evaluations for all slices\nUse this helper function list_model_evaluations, which takes the following parameter:\n\nname: The Vertex fully qualified model identifier for the Model resource.\n\nThis helper function uses the model client service's list_model_evaluations method, which takes the same parameter. The response object from the call is a list, where each element is an evaluation metric.\nFor each evaluation (you probably only have one) we then print all the key names for each metric in the evaluation, and for a small set (logLoss and auPrc) you will print the result.\nEnd of explanation\n\"\"\"\n\n\nMODEL_DIR = BUCKET_NAME + \"/\" + \"flowers\"\n\n\ndef export_model(name, format, gcs_dest):\n    output_config = {\n        \"artifact_destination\": {\"output_uri_prefix\": gcs_dest},\n        \"export_format_id\": format,\n    }\n    response = clients[\"model\"].export_model(name=name, output_config=output_config)\n    print(\"Long running operation:\", response.operation.name)\n    result = response.result(timeout=1800)\n    metadata = response.operation.metadata\n    artifact_uri = str(metadata.value).split(\"\\\\\")[-1][4:-1]\n    print(\"Artifact Uri\", artifact_uri)\n    return artifact_uri\n\n\nmodel_package = export_model(model_to_deploy_id, \"tflite\", MODEL_DIR)\n\n\"\"\"\nExplanation: Export as Edge model\nYou can export an AutoML image classification model as an Edge model which you can then custom deploy to an edge device, such as a mobile phone or IoT device, or download locally. Use this helper function export_model to export the model to Google Cloud, which takes the following parameters:\n\nname: The Vertex fully qualified identifier for the Model resource.\nformat: The format to save the model format as.\ngcs_dest: The Cloud Storage location to store the SavedFormat model artifacts to.\n\nThis function calls the Model client service's method export_model, with the following parameters:\n\nname: The Vertex fully qualified identifier for the Model resource.\noutput_config: The destination information for the exported model.\nartifact_destination.output_uri_prefix: The Cloud Storage location to store the SavedFormat model artifacts to.\nexport_format_id: The format to save the model format as. For AutoML image classification:\ntf-saved-model: TensorFlow SavedFormat for deployment to a container.\ntflite: TensorFlow Lite for deployment to an edge or mobile device.\nedgetpu-tflite: TensorFlow Lite for TPU\ntf-js: TensorFlow for web client\ncoral-ml: for Coral devices\n\nThe method returns a long running operation response. We will wait sychronously for the operation to complete by calling the response.result(), which will block until the model is exported.\nEnd of explanation\n\"\"\"\n\n\n! gsutil ls $model_package\n# Download the model artifacts\n! gsutil cp -r $model_package tflite\n\ntflite_path = \"tflite/model.tflite\"\n\n\"\"\"\nExplanation: Download the TFLite model artifacts\nNow that you have an exported TFLite version of your model, you can test the exported model locally, but first downloading it from Cloud Storage.\nEnd of explanation\n\"\"\"\n\n\nimport tensorflow as tf\n\ninterpreter = tf.lite.Interpreter(model_path=tflite_path)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\ninput_shape = input_details[0][\"shape\"]\n\nprint(\"input tensor shape\", input_shape)\n\n\"\"\"\nExplanation: Instantiate a TFLite interpreter\nThe TFLite version of the model is not a TensorFlow SavedModel format. You cannot directly use methods like predict(). Instead, one uses the TFLite interpreter. You must first setup the interpreter for the TFLite model as follows:\n\nInstantiate an TFLite interpreter for the TFLite model.\nInstruct the interpreter to allocate input and output tensors for the model.\nGet detail information about the models input and output tensors that will need to be known for prediction.\nEnd of explanation\n\"\"\"\n\n\ntest_items = ! gsutil cat $IMPORT_FILE | head -n1\ntest_item = test_items[0].split(\",\")[0]\n\nwith tf.io.gfile.GFile(test_item, \"rb\") as f:\n    content = f.read()\ntest_image = tf.io.decode_jpeg(content)\nprint(\"test image shape\", test_image.shape)\n\ntest_image = tf.image.resize(test_image, (224, 224))\nprint(\"test image shape\", test_image.shape, test_image.dtype)\n\ntest_image = tf.cast(test_image, dtype=tf.uint8).numpy()\n\n\"\"\"\nExplanation: Get test item\nYou will use an arbitrary example out of the dataset as a test item. Don't be concerned that the example was likely used in training the model -- we just want to demonstrate how to make a prediction.\nEnd of explanation\n\"\"\"\n\n\nimport numpy as np\n\ndata = np.expand_dims(test_image, axis=0)\n\ninterpreter.set_tensor(input_details[0][\"index\"], data)\n\ninterpreter.invoke()\n\nsoftmax = interpreter.get_tensor(output_details[0][\"index\"])\n\nlabel = np.argmax(softmax)\n\nprint(label)\n\n\"\"\"\nExplanation: Make a prediction with TFLite model\nFinally, you do a prediction using your TFLite model, as follows:\n\nConvert the test image into a batch of a single image (np.expand_dims)\nSet the input tensor for the interpreter to your batch of a single image (data).\nInvoke the interpreter.\nRetrieve the softmax probabilities for the prediction (get_tensor).\nDetermine which label had the highest probability (np.argmax).\nEnd of explanation\n\"\"\"\n\n\ndelete_dataset = True\ndelete_pipeline = True\ndelete_model = True\ndelete_endpoint = True\ndelete_batchjob = True\ndelete_customjob = True\ndelete_hptjob = True\ndelete_bucket = True\n\n# Delete the dataset using the Vertex fully qualified identifier for the dataset\ntry:\n    if delete_dataset and \"dataset_id\" in globals():\n        clients[\"dataset\"].delete_dataset(name=dataset_id)\nexcept Exception as e:\n    print(e)\n\n# Delete the training pipeline using the Vertex fully qualified identifier for the pipeline\ntry:\n    if delete_pipeline and \"pipeline_id\" in globals():\n        clients[\"pipeline\"].delete_training_pipeline(name=pipeline_id)\nexcept Exception as e:\n    print(e)\n\n# Delete the model using the Vertex fully qualified identifier for the model\ntry:\n    if delete_model and \"model_to_deploy_id\" in globals():\n        clients[\"model\"].delete_model(name=model_to_deploy_id)\nexcept Exception as e:\n    print(e)\n\n# Delete the endpoint using the Vertex fully qualified identifier for the endpoint\ntry:\n    if delete_endpoint and \"endpoint_id\" in globals():\n        clients[\"endpoint\"].delete_endpoint(name=endpoint_id)\nexcept Exception as e:\n    print(e)\n\n# Delete the batch job using the Vertex fully qualified identifier for the batch job\ntry:\n    if delete_batchjob and \"batch_job_id\" in globals():\n        clients[\"job\"].delete_batch_prediction_job(name=batch_job_id)\nexcept Exception as e:\n    print(e)\n\n# Delete the custom job using the Vertex fully qualified identifier for the custom job\ntry:\n    if delete_customjob and \"job_id\" in globals():\n        clients[\"job\"].delete_custom_job(name=job_id)\nexcept Exception as e:\n    print(e)\n\n# Delete the hyperparameter tuning job using the Vertex fully qualified identifier for the hyperparameter tuning job\ntry:\n    if delete_hptjob and \"hpt_job_id\" in globals():\n        clients[\"job\"].delete_hyperparameter_tuning_job(name=hpt_job_id)\nexcept Exception as e:\n    print(e)\n\nif delete_bucket and \"BUCKET_NAME\" in globals():\n    ! gsutil rm -r $BUCKET_NAME\n\n\"\"\"\nExplanation: Cleaning up\nTo clean up all GCP resources used in this project, you can delete the GCP\nproject you used for the tutorial.\nOtherwise, you can delete the individual resources you created in this tutorial:\n\nDataset\nPipeline\nModel\nEndpoint\nBatch Job\nCustom Job\nHyperparameter Tuning Job\nCloud Storage Bucket\nEnd of explanation\n\"\"\"\n\n# DO NOT EDIT !  \nfrom pyesdoc.ipython.model_topic import NotebookOutput  \n\n# DO NOT EDIT !  \nDOC = NotebookOutput('cmip6', 'ncc', 'sandbox-1', 'ocean')\n\n\"\"\"\nExplanation: ES-DOC CMIP6 Model Properties - Ocean\nMIP Era: CMIP6\nInstitute: NCC\nSource ID: SANDBOX-1\nTopic: Ocean\nSub-Topics: Timestepping Framework, Advection, Lateral Physics, Vertical Physics, Uplow Boundaries, Boundary Forcing. \nProperties: 133 (101 required)\nModel descriptions: Model description details\nInitialized From: --  \nNotebook Help: Goto notebook help page\nNotebook Initialised: 2018-02-15 16:54:25\nDocument Setup\nIMPORTANT: to be executed each time you run the notebook\nEnd of explanation\n\"\"\"\n\n\n# Set as follows: DOC.set_author(\"name\", \"email\")  \n# TODO - please enter value(s)\n\n\"\"\"\nExplanation: Document Authors\nSet document authors\nEnd of explanation\n\"\"\"\n\n\n# Set as follows: DOC.set_contributor(\"name\", \"email\")  \n# TODO - please enter value(s)\n\n\"\"\"\nExplanation: Document Contributors\nSpecify document contributors\nEnd of explanation\n\"\"\"\n\n\n# Set publication status:  \n# 0=do not publish, 1=publish.   \nDOC.set_publication_status(0)\n\n\"\"\"\nExplanation: Document Publication\nSpecify document publication status\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.model_overview')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: Document Table of Contents\n1. Key Properties\n2. Key Properties --&gt; Seawater Properties\n3. Key Properties --&gt; Bathymetry\n4. Key Properties --&gt; Nonoceanic Waters\n5. Key Properties --&gt; Software Properties\n6. Key Properties --&gt; Resolution\n7. Key Properties --&gt; Tuning Applied\n8. Key Properties --&gt; Conservation\n9. Grid\n10. Grid --&gt; Discretisation --&gt; Vertical\n11. Grid --&gt; Discretisation --&gt; Horizontal\n12. Timestepping Framework\n13. Timestepping Framework --&gt; Tracers\n14. Timestepping Framework --&gt; Baroclinic Dynamics\n15. Timestepping Framework --&gt; Barotropic\n16. Timestepping Framework --&gt; Vertical Physics\n17. Advection\n18. Advection --&gt; Momentum\n19. Advection --&gt; Lateral Tracers\n20. Advection --&gt; Vertical Tracers\n21. Lateral Physics\n22. Lateral Physics --&gt; Momentum --&gt; Operator\n23. Lateral Physics --&gt; Momentum --&gt; Eddy Viscosity Coeff\n24. Lateral Physics --&gt; Tracers\n25. Lateral Physics --&gt; Tracers --&gt; Operator\n26. Lateral Physics --&gt; Tracers --&gt; Eddy Diffusity Coeff\n27. Lateral Physics --&gt; Tracers --&gt; Eddy Induced Velocity\n28. Vertical Physics\n29. Vertical Physics --&gt; Boundary Layer Mixing --&gt; Details\n30. Vertical Physics --&gt; Boundary Layer Mixing --&gt; Tracers\n31. Vertical Physics --&gt; Boundary Layer Mixing --&gt; Momentum\n32. Vertical Physics --&gt; Interior Mixing --&gt; Details\n33. Vertical Physics --&gt; Interior Mixing --&gt; Tracers\n34. Vertical Physics --&gt; Interior Mixing --&gt; Momentum\n35. Uplow Boundaries --&gt; Free Surface\n36. Uplow Boundaries --&gt; Bottom Boundary Layer\n37. Boundary Forcing\n38. Boundary Forcing --&gt; Momentum --&gt; Bottom Friction\n39. Boundary Forcing --&gt; Momentum --&gt; Lateral Friction\n40. Boundary Forcing --&gt; Tracers --&gt; Sunlight Penetration\n41. Boundary Forcing --&gt; Tracers --&gt; Fresh Water Forcing  \n1. Key Properties\nOcean key properties\n1.1. Model Overview\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nOverview of ocean model.\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.model_name')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 1.2. Model Name\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nName of ocean model code (NEMO 3.6, MOM 5.0,...)\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.model_family')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# Valid Choices: \n#      \"OGCM\"  \n#      \"slab ocean\"  \n#      \"mixed layer ocean\"  \n#      \"Other: [Please specify]\"  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 1.3. Model Family\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: ENUM&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nType of ocean model.\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.basic_approximations')  \n\n# PROPERTY VALUE(S): \n# Set as follows: DOC.set_value(\"value\")  \n# Valid Choices: \n#      \"Primitive equations\"  \n#      \"Non-hydrostatic\"  \n#      \"Boussinesq\"  \n#      \"Other: [Please specify]\"  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 1.4. Basic Approximations\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: ENUM&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.N\nBasic approximations made in the ocean.\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.prognostic_variables')  \n\n# PROPERTY VALUE(S): \n# Set as follows: DOC.set_value(\"value\")  \n# Valid Choices: \n#      \"Potential temperature\"  \n#      \"Conservative temperature\"  \n#      \"Salinity\"  \n#      \"U-velocity\"  \n#      \"V-velocity\"  \n#      \"W-velocity\"  \n#      \"SSH\"  \n#      \"Other: [Please specify]\"  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 1.5. Prognostic Variables\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: ENUM&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.N\nList of prognostic variables in the ocean component.\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.seawater_properties.eos_type')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# Valid Choices: \n#      \"Linear\"  \n#      \"Wright, 1997\"  \n#      \"Mc Dougall et al.\"  \n#      \"Jackett et al. 2006\"  \n#      \"TEOS 2010\"  \n#      \"Other: [Please specify]\"  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 2. Key Properties --&gt; Seawater Properties\nPhysical properties of seawater in ocean\n2.1. Eos Type\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: ENUM&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nType of EOS for sea water\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.seawater_properties.eos_functional_temp')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# Valid Choices: \n#      \"Potential temperature\"  \n#      \"Conservative temperature\"  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 2.2. Eos Functional Temp\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: ENUM&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nTemperature used in EOS for sea water\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.seawater_properties.eos_functional_salt')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# Valid Choices: \n#      \"Practical salinity Sp\"  \n#      \"Absolute salinity Sa\"  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 2.3. Eos Functional Salt\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: ENUM&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nSalinity used in EOS for sea water\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.seawater_properties.eos_functional_depth')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# Valid Choices: \n#      \"Pressure (dbars)\"  \n#      \"Depth (meters)\"  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 2.4. Eos Functional Depth\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: ENUM&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nDepth or pressure used in EOS for sea water ?\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.seawater_properties.ocean_freezing_point')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# Valid Choices: \n#      \"TEOS 2010\"  \n#      \"Other: [Please specify]\"  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 2.5. Ocean Freezing Point\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: ENUM&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nEquation used to compute the freezing point (in deg C) of seawater, as a function of salinity and pressure\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.seawater_properties.ocean_specific_heat')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(value)  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 2.6. Ocean Specific Heat\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: FLOAT&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nSpecific heat in ocean (cpocean) in J/(kg K)\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.seawater_properties.ocean_reference_density')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(value)  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 2.7. Ocean Reference Density\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: FLOAT&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nBoussinesq reference density (rhozero) in kg / m3\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.bathymetry.reference_dates')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# Valid Choices: \n#      \"Present day\"  \n#      \"21000 years BP\"  \n#      \"6000 years BP\"  \n#      \"LGM\"  \n#      \"Pliocene\"  \n#      \"Other: [Please specify]\"  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 3. Key Properties --&gt; Bathymetry\nProperties of bathymetry in ocean\n3.1. Reference Dates\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: ENUM&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nReference date of bathymetry\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.bathymetry.type')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(value)  \n# Valid Choices: \n#     True  \n#     False  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 3.2. Type\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: BOOLEAN&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nIs the bathymetry fixed in time in the ocean ?\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.bathymetry.ocean_smoothing')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 3.3. Ocean Smoothing\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nDescribe any smoothing or hand editing of bathymetry in ocean\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.bathymetry.source')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 3.4. Source\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nDescribe source of bathymetry in ocean\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.nonoceanic_waters.isolated_seas')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 4. Key Properties --&gt; Nonoceanic Waters\nNon oceanic waters treatement in ocean\n4.1. Isolated Seas\nIs Required: FALSE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 0.1\nDescribe if/how isolated seas is performed\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.nonoceanic_waters.river_mouth')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 4.2. River Mouth\nIs Required: FALSE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 0.1\nDescribe if/how river mouth mixing or estuaries specific treatment is performed\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.software_properties.repository')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 5. Key Properties --&gt; Software Properties\nSoftware properties of ocean code\n5.1. Repository\nIs Required: FALSE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 0.1\nLocation of code for this component.\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.software_properties.code_version')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 5.2. Code Version\nIs Required: FALSE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 0.1\nCode version identifier.\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.software_properties.code_languages')  \n\n# PROPERTY VALUE(S): \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 5.3. Code Languages\nIs Required: FALSE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 0.N\nCode language(s).\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.resolution.name')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 6. Key Properties --&gt; Resolution\nResolution in the ocean grid\n6.1. Name\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nThis is a string usually used by the modelling group to describe the resolution of this grid, e.g. ORCA025, N512L180, T512L70 etc.\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.resolution.canonical_horizontal_resolution')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 6.2. Canonical Horizontal Resolution\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nExpression quoted for gross comparisons of resolution, eg. 50km or 0.1 degrees etc.\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.resolution.range_horizontal_resolution')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 6.3. Range Horizontal Resolution\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nRange of horizontal resolution with spatial details, eg. 50(Equator)-100km or 0.1-0.5 degrees etc.\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.resolution.number_of_horizontal_gridpoints')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(value)  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 6.4. Number Of Horizontal Gridpoints\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: INTEGER&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nTotal number of horizontal (XY) points (or degrees of freedom) on computational grid.\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.resolution.number_of_vertical_levels')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(value)  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 6.5. Number Of Vertical Levels\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: INTEGER&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nNumber of vertical levels resolved on computational grid.\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.resolution.is_adaptive_grid')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(value)  \n# Valid Choices: \n#     True  \n#     False  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 6.6. Is Adaptive Grid\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: BOOLEAN&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nDefault is False. Set true if grid resolution changes during execution.\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.resolution.thickness_level_1')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(value)  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 6.7. Thickness Level 1\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: FLOAT&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nThickness of first surface ocean level (in meters)\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.tuning_applied.description')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 7. Key Properties --&gt; Tuning Applied\nTuning methodology for ocean component\n7.1. Description\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nGeneral overview description of tuning: explain and motivate the main targets and metrics retained. &amp;Document the relative weight given to climate performance metrics versus process oriented metrics, &amp;and on the possible conflicts with parameterization level tuning. In particular describe any struggle &amp;with a parameter value that required pushing it to its limits to solve a particular model deficiency.\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.tuning_applied.global_mean_metrics_used')  \n\n# PROPERTY VALUE(S): \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 7.2. Global Mean Metrics Used\nIs Required: FALSE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 0.N\nList set of metrics of the global mean state used in tuning model/component\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.tuning_applied.regional_metrics_used')  \n\n# PROPERTY VALUE(S): \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 7.3. Regional Metrics Used\nIs Required: FALSE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 0.N\nList of regional metrics of mean state (e.g THC, AABW, regional means etc) used in tuning model/component\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.tuning_applied.trend_metrics_used')  \n\n# PROPERTY VALUE(S): \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 7.4. Trend Metrics Used\nIs Required: FALSE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 0.N\nList observed trend metrics used in tuning model/component\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.conservation.description')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 8. Key Properties --&gt; Conservation\nConservation in the ocean component\n8.1. Description\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.1\nBrief description of conservation methodology\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.conservation.scheme')  \n\n# PROPERTY VALUE(S): \n# Set as follows: DOC.set_value(\"value\")  \n# Valid Choices: \n#      \"Energy\"  \n#      \"Enstrophy\"  \n#      \"Salt\"  \n#      \"Volume of ocean\"  \n#      \"Momentum\"  \n#      \"Other: [Please specify]\"  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 8.2. Scheme\nIs Required: TRUE&nbsp;&nbsp;&nbsp;&nbsp;Type: ENUM&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 1.N\nProperties conserved in the ocean by the numerical schemes\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.conservation.consistency_properties')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 8.3. Consistency Properties\nIs Required: FALSE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 0.1\nAny additional consistency properties (energy conversion, pressure gradient discretisation, ...)?\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.conservation.corrected_conserved_prognostic_variables')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_value(\"value\")  \n# TODO - please enter value(s)\n\n\n\"\"\"\nExplanation: 8.4. Corrected Conserved Prognostic Variables\nIs Required: FALSE&nbsp;&nbsp;&nbsp;&nbsp;Type: STRING&nbsp;&nbsp;&nbsp;&nbsp;Cardinality: 0.1\nSet of variables which are conserved by more than the numerical scheme alone.\nEnd of explanation\n\"\"\"\n\n\n# PROPERTY ID - DO NOT EDIT !  \nDOC.set_id('cmip6.ocean.key_properties.conservation.was_flux_correction_used')  \n\n# PROPERTY VALUE: \n# Set as follows: DOC.set_"
}